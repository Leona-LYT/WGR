{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f863c7-52e9-4f51-ae00-bc3c97f2ba53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nincase the above code does not work, you can use the absolute path instead\\nsys.path.append(r\".\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  #use to import the defined functions\n",
    "parent_dir = os.path.dirname(current_dir) \n",
    "sys.path.append(parent_dir)  \n",
    "\n",
    "\"\"\"\n",
    "incase the above code does not work, you can use the absolute path instead\n",
    "sys.path.append(r\".\\\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f578f4-d40c-4e9c-b234-ea17249bff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec819f9-bbc8-4b4f-bb26-cf48a2ce0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.basic_utils import setup_seed\n",
    "from data.SimulationData import DataGenerator \n",
    "from utils.training_utils import train_WGR_fnn\n",
    "from utils.evaluation_utils import L1L2_MSE_mean_sd_G, MSE_quantile_G_uniY\n",
    "from models.generator import generator_fnn\n",
    "from models.discriminator import discriminator_fnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82f422a-70a8-4191-b99d-16612d3dbef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Xdim=5, Ydim=1, model='M1', noise_dim=5, noise_dist='gaussian', train=5000, val=1000, test=1000, train_batch=128, val_batch=100, test_batch=100, epochs=100, reps=100)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if 'ipykernel_launcher.py' in sys.argv[0]:  #if not work in jupyter, you can delete this part\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] \n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Implementation of WGR for M1')\n",
    "\n",
    "parser.add_argument('--Xdim', default=5, type=int, help='dimensionality of X')\n",
    "parser.add_argument('--Ydim', default=1, type=int, help='dimensionality of Y')\n",
    "parser.add_argument('--model', default='M1', type=str, help='model')\n",
    "\n",
    "parser.add_argument('--noise_dim', default=5, type=int, help='dimensionality of noise vector')\n",
    "parser.add_argument('--noise_dist', default='gaussian', type=str, help='distribution of noise vector')\n",
    "\n",
    "parser.add_argument('--train', default=5000, type=int, help='size of train dataset')\n",
    "parser.add_argument('--val', default=1000, type=int, help='size of validation dataset')\n",
    "parser.add_argument('--test', default=1000, type=int, help='size of test dataset')\n",
    "\n",
    "parser.add_argument('--train_batch', default=128, type=int, metavar='BS', help='batch size while training')\n",
    "parser.add_argument('--val_batch', default=100, type=int, metavar='BS', help='batch size while validation')\n",
    "parser.add_argument('--test_batch', default=100, type=int, metavar='BS', help='batch size while testing')\n",
    "parser.add_argument('--epochs', default=100, type=int, help='number of epochs to train')\n",
    "parser.add_argument('--reps', default=100, type=int, help='number of replications')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c1b44a-0c6c-478a-b807-e5e1da20a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "setup_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177a8bd5-74a6-4ffe-ab47-e2e7cc821c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from M1\n",
    "data_gen = DataGenerator(args)\n",
    "DATA = data_gen.generate_data('M1')\n",
    "train_X, train_Y = DATA['train_X'], DATA['train_Y']\n",
    "val_X, val_Y = DATA['val_X'], DATA['val_Y']\n",
    "test_X, test_Y = DATA['test_X'], DATA['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de288fbd-c582-4ad8-b10d-59b3d145e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets and initialize a DataLoaders\n",
    "train_dataset = TensorDataset( train_X.float(), train_Y.float() )\n",
    "loader_train = DataLoader(train_dataset , batch_size=args.train_batch, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset( val_X.float(), val_Y.float() )\n",
    "loader_val = DataLoader(val_dataset , batch_size=args.val_batch, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset( test_X.float(), test_Y.float() )\n",
    "loader_test  = DataLoader(test_dataset , batch_size=args.test_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bb4d11-47ef-4e17-9852-27ea723f271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator network and discriminator network\n",
    "G_net = generator_fnn(Xdim=args.Xdim, Ydim=args.Ydim, noise_dim=args.noise_dim, hidden_dims = [64, 32])\n",
    "D_net = discriminator_fnn(input_dim=args.Xdim+args.Ydim, hidden_dims = [64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ccda7e-1d68-4ff1-9e4b-d5caa66dcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RMSprop optimizers\n",
    "D_solver = optim.RMSprop(D_net.parameters(),lr = 0.001)\n",
    "G_solver = optim.RMSprop(G_net.parameters(),lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c643e20e-1daf-420e-9145-fac2f3c39e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 Loss: 3.427226, Mean L2 Loss: 23.734234\n",
      "Epoch 0 - D Loss: 0.8336, G Loss: 1.1725\n",
      "Epoch 1 - D Loss: 0.1537, G Loss: 0.6534\n",
      "Epoch 2 - D Loss: 0.0946, G Loss: 0.9039\n",
      "Epoch 3 - D Loss: 0.0416, G Loss: 1.1471\n",
      "Epoch 4 - D Loss: 0.0354, G Loss: 1.1323\n",
      "Epoch 5 - D Loss: 0.0316, G Loss: 1.1433\n",
      "Epoch 6 - D Loss: 0.0282, G Loss: 1.0607\n",
      "Epoch 7 - D Loss: 0.0247, G Loss: 0.9919\n",
      "Epoch 8 - D Loss: 0.0272, G Loss: 0.9164\n",
      "Epoch 9 - D Loss: 0.0163, G Loss: 0.8657\n",
      "Epoch 10 - D Loss: 0.0110, G Loss: 0.8141\n",
      "Epoch 11 - D Loss: 0.0048, G Loss: 0.7245\n",
      "Epoch 12 - D Loss: 0.0002, G Loss: 0.6993\n",
      "Epoch 13 - D Loss: 0.0035, G Loss: 0.6172\n",
      "Epoch 14 - D Loss: -0.0007, G Loss: 0.5473\n",
      "Epoch 15 - D Loss: 0.0067, G Loss: 0.5043\n",
      "Epoch 16 - D Loss: 0.0002, G Loss: 0.4877\n",
      "Epoch 17 - D Loss: 0.0105, G Loss: 0.4386\n",
      "Epoch 18 - D Loss: 0.0099, G Loss: 0.3890\n",
      "Epoch 19 - D Loss: 0.0127, G Loss: 0.4128\n",
      "Epoch 20 - D Loss: 0.0143, G Loss: 0.3690\n",
      "Epoch 21 - D Loss: 0.0095, G Loss: 0.4072\n",
      "Epoch 22 - D Loss: 0.0102, G Loss: 0.4151\n",
      "Epoch 23 - D Loss: 0.0121, G Loss: 0.4468\n",
      "Epoch 24 - D Loss: 0.0057, G Loss: 0.4437\n",
      "Mean L1 Loss: 1.253012, Mean L2 Loss: 3.576280\n",
      "Epoch 25, Iter 1000, D Loss: 0.0010, G Loss: 0.5154, L1: 1.2530, L2: 3.5763\n",
      "Saved best model with L2: 3.5763\n",
      "Epoch 25 - D Loss: 0.0052, G Loss: 0.4775\n",
      "Mean L1 Loss: 1.252755, Mean L2 Loss: 3.452138\n",
      "Epoch 26, Iter 1050, D Loss: -0.0043, G Loss: 0.4470, L1: 1.2528, L2: 3.4521\n",
      "Saved best model with L2: 3.4521\n",
      "Epoch 26 - D Loss: -0.0047, G Loss: 0.4481\n",
      "Epoch 27 - D Loss: 0.0075, G Loss: 0.4746\n",
      "Mean L1 Loss: 1.257658, Mean L2 Loss: 3.543334\n",
      "Epoch 28, Iter 1100, D Loss: -0.0024, G Loss: 0.4532, L1: 1.2577, L2: 3.5433\n",
      "Epoch 28 - D Loss: -0.0021, G Loss: 0.5373\n",
      "Mean L1 Loss: 1.315302, Mean L2 Loss: 3.689363\n",
      "Epoch 29, Iter 1150, D Loss: -0.0150, G Loss: 0.5621, L1: 1.3153, L2: 3.6894\n",
      "Epoch 29 - D Loss: -0.0038, G Loss: 0.5412\n",
      "Mean L1 Loss: 1.278466, Mean L2 Loss: 3.548564\n",
      "Epoch 30, Iter 1200, D Loss: -0.0076, G Loss: 0.5924, L1: 1.2785, L2: 3.5486\n",
      "Epoch 30 - D Loss: -0.0020, G Loss: 0.5646\n",
      "Epoch 31 - D Loss: -0.0023, G Loss: 0.6017\n",
      "Mean L1 Loss: 1.258777, Mean L2 Loss: 3.483699\n",
      "Epoch 32, Iter 1250, D Loss: 0.0111, G Loss: 0.6960, L1: 1.2588, L2: 3.4837\n",
      "Epoch 32 - D Loss: -0.0062, G Loss: 0.5823\n",
      "Mean L1 Loss: 1.248204, Mean L2 Loss: 3.429281\n",
      "Epoch 33, Iter 1300, D Loss: -0.0168, G Loss: 0.6389, L1: 1.2482, L2: 3.4293\n",
      "Saved best model with L2: 3.4293\n",
      "Epoch 33 - D Loss: -0.0044, G Loss: 0.6518\n",
      "Mean L1 Loss: 1.247096, Mean L2 Loss: 3.409877\n",
      "Epoch 34, Iter 1350, D Loss: -0.0153, G Loss: 0.6675, L1: 1.2471, L2: 3.4099\n",
      "Saved best model with L2: 3.4099\n",
      "Epoch 34 - D Loss: -0.0143, G Loss: 0.6709\n",
      "Mean L1 Loss: 1.248936, Mean L2 Loss: 3.402925\n",
      "Epoch 35, Iter 1400, D Loss: -0.0027, G Loss: 0.6630, L1: 1.2489, L2: 3.4029\n",
      "Saved best model with L2: 3.4029\n",
      "Epoch 35 - D Loss: -0.0025, G Loss: 0.6538\n",
      "Epoch 36 - D Loss: -0.0088, G Loss: 0.6829\n",
      "Mean L1 Loss: 1.242971, Mean L2 Loss: 3.376170\n",
      "Epoch 37, Iter 1450, D Loss: -0.0053, G Loss: 0.6372, L1: 1.2430, L2: 3.3762\n",
      "Saved best model with L2: 3.3762\n",
      "Epoch 37 - D Loss: -0.0022, G Loss: 0.6526\n",
      "Mean L1 Loss: 1.264914, Mean L2 Loss: 3.461171\n",
      "Epoch 38, Iter 1500, D Loss: -0.0169, G Loss: 0.7089, L1: 1.2649, L2: 3.4612\n",
      "Epoch 38 - D Loss: -0.0086, G Loss: 0.7020\n",
      "Mean L1 Loss: 1.243640, Mean L2 Loss: 3.404674\n",
      "Epoch 39, Iter 1550, D Loss: -0.0094, G Loss: 0.6982, L1: 1.2436, L2: 3.4047\n",
      "Epoch 39 - D Loss: -0.0068, G Loss: 0.6951\n",
      "Epoch 40 - D Loss: -0.0026, G Loss: 0.7285\n",
      "Mean L1 Loss: 1.241691, Mean L2 Loss: 3.362872\n",
      "Epoch 41, Iter 1600, D Loss: 0.0107, G Loss: 0.6017, L1: 1.2417, L2: 3.3629\n",
      "Saved best model with L2: 3.3629\n",
      "Epoch 41 - D Loss: -0.0004, G Loss: 0.7132\n",
      "Mean L1 Loss: 1.240035, Mean L2 Loss: 3.323409\n",
      "Epoch 42, Iter 1650, D Loss: 0.0110, G Loss: 0.7285, L1: 1.2400, L2: 3.3234\n",
      "Saved best model with L2: 3.3234\n",
      "Epoch 42 - D Loss: 0.0011, G Loss: 0.7936\n",
      "Mean L1 Loss: 1.245935, Mean L2 Loss: 3.383976\n",
      "Epoch 43, Iter 1700, D Loss: 0.0035, G Loss: 0.7616, L1: 1.2459, L2: 3.3840\n",
      "Epoch 43 - D Loss: 0.0006, G Loss: 0.7750\n",
      "Mean L1 Loss: 1.238085, Mean L2 Loss: 3.300925\n",
      "Epoch 44, Iter 1750, D Loss: -0.0049, G Loss: 0.7779, L1: 1.2381, L2: 3.3009\n",
      "Saved best model with L2: 3.3009\n",
      "Epoch 44 - D Loss: -0.0032, G Loss: 0.7643\n",
      "Epoch 45 - D Loss: -0.0011, G Loss: 0.7786\n",
      "Mean L1 Loss: 1.252862, Mean L2 Loss: 3.320680\n",
      "Epoch 46, Iter 1800, D Loss: 0.0015, G Loss: 0.8248, L1: 1.2529, L2: 3.3207\n",
      "Epoch 46 - D Loss: -0.0008, G Loss: 0.8060\n",
      "Mean L1 Loss: 1.269375, Mean L2 Loss: 3.409549\n",
      "Epoch 47, Iter 1850, D Loss: 0.0022, G Loss: 0.8462, L1: 1.2694, L2: 3.4095\n",
      "Epoch 47 - D Loss: 0.0017, G Loss: 0.8304\n",
      "Mean L1 Loss: 1.277944, Mean L2 Loss: 3.403344\n",
      "Epoch 48, Iter 1900, D Loss: -0.0048, G Loss: 0.8185, L1: 1.2779, L2: 3.4033\n",
      "Epoch 48 - D Loss: -0.0014, G Loss: 0.8128\n",
      "Mean L1 Loss: 1.222224, Mean L2 Loss: 3.190583\n",
      "Epoch 49, Iter 1950, D Loss: 0.0018, G Loss: 0.8204, L1: 1.2222, L2: 3.1906\n",
      "Saved best model with L2: 3.1906\n",
      "Epoch 49 - D Loss: 0.0018, G Loss: 0.8204\n",
      "Epoch 50 - D Loss: 0.0011, G Loss: 0.8359\n",
      "Mean L1 Loss: 1.214581, Mean L2 Loss: 3.169894\n",
      "Epoch 51, Iter 2000, D Loss: -0.0002, G Loss: 0.8424, L1: 1.2146, L2: 3.1699\n",
      "Saved best model with L2: 3.1699\n",
      "Epoch 51 - D Loss: 0.0043, G Loss: 0.8282\n",
      "Mean L1 Loss: 1.227392, Mean L2 Loss: 3.273492\n",
      "Epoch 52, Iter 2050, D Loss: 0.0055, G Loss: 0.8361, L1: 1.2274, L2: 3.2735\n",
      "Epoch 52 - D Loss: 0.0048, G Loss: 0.8081\n",
      "Mean L1 Loss: 1.234426, Mean L2 Loss: 3.309672\n",
      "Epoch 53, Iter 2100, D Loss: 0.0022, G Loss: 0.8540, L1: 1.2344, L2: 3.3097\n",
      "Epoch 53 - D Loss: 0.0022, G Loss: 0.8614\n",
      "Epoch 54 - D Loss: 0.0032, G Loss: 0.8605\n",
      "Mean L1 Loss: 1.229930, Mean L2 Loss: 3.286544\n",
      "Epoch 55, Iter 2150, D Loss: 0.0198, G Loss: 0.8722, L1: 1.2299, L2: 3.2865\n",
      "Epoch 55 - D Loss: 0.0062, G Loss: 0.8576\n",
      "Mean L1 Loss: 1.222890, Mean L2 Loss: 3.205541\n",
      "Epoch 56, Iter 2200, D Loss: 0.0025, G Loss: 0.8323, L1: 1.2229, L2: 3.2055\n",
      "Epoch 56 - D Loss: 0.0073, G Loss: 0.8862\n",
      "Mean L1 Loss: 1.229429, Mean L2 Loss: 3.276014\n",
      "Epoch 57, Iter 2250, D Loss: 0.0061, G Loss: 0.9062, L1: 1.2294, L2: 3.2760\n",
      "Epoch 57 - D Loss: 0.0029, G Loss: 0.8903\n",
      "Mean L1 Loss: 1.223282, Mean L2 Loss: 3.204852\n",
      "Epoch 58, Iter 2300, D Loss: 0.0064, G Loss: 0.9498, L1: 1.2233, L2: 3.2049\n",
      "Epoch 58 - D Loss: 0.0063, G Loss: 0.9554\n",
      "Epoch 59 - D Loss: 0.0083, G Loss: 0.9508\n",
      "Mean L1 Loss: 1.208156, Mean L2 Loss: 3.178729\n",
      "Epoch 60, Iter 2350, D Loss: -0.0050, G Loss: 0.9291, L1: 1.2082, L2: 3.1787\n",
      "Epoch 60 - D Loss: 0.0028, G Loss: 0.9507\n",
      "Mean L1 Loss: 1.242103, Mean L2 Loss: 3.272338\n",
      "Epoch 61, Iter 2400, D Loss: 0.0101, G Loss: 0.9371, L1: 1.2421, L2: 3.2723\n",
      "Epoch 61 - D Loss: 0.0062, G Loss: 0.9659\n",
      "Mean L1 Loss: 1.233434, Mean L2 Loss: 3.274048\n",
      "Epoch 62, Iter 2450, D Loss: 0.0075, G Loss: 1.0000, L1: 1.2334, L2: 3.2740\n",
      "Epoch 62 - D Loss: 0.0077, G Loss: 1.0121\n",
      "Epoch 63 - D Loss: 0.0031, G Loss: 1.0098\n",
      "Mean L1 Loss: 1.230495, Mean L2 Loss: 3.230730\n",
      "Epoch 64, Iter 2500, D Loss: 0.0216, G Loss: 1.0220, L1: 1.2305, L2: 3.2307\n",
      "Epoch 64 - D Loss: 0.0061, G Loss: 1.0314\n",
      "Mean L1 Loss: 1.240360, Mean L2 Loss: 3.344927\n",
      "Epoch 65, Iter 2550, D Loss: 0.0005, G Loss: 0.9770, L1: 1.2404, L2: 3.3449\n",
      "Epoch 65 - D Loss: 0.0084, G Loss: 0.9981\n",
      "Mean L1 Loss: 1.335353, Mean L2 Loss: 3.611755\n",
      "Epoch 66, Iter 2600, D Loss: 0.0014, G Loss: 1.0473, L1: 1.3354, L2: 3.6118\n",
      "Epoch 66 - D Loss: 0.0021, G Loss: 1.0321\n",
      "Mean L1 Loss: 1.256094, Mean L2 Loss: 3.366967\n",
      "Epoch 67, Iter 2650, D Loss: 0.0047, G Loss: 1.0552, L1: 1.2561, L2: 3.3670\n",
      "Epoch 67 - D Loss: 0.0048, G Loss: 1.0506\n",
      "Epoch 68 - D Loss: 0.0052, G Loss: 1.0637\n",
      "Mean L1 Loss: 1.240511, Mean L2 Loss: 3.315235\n",
      "Epoch 69, Iter 2700, D Loss: 0.0004, G Loss: 1.0849, L1: 1.2405, L2: 3.3152\n",
      "Epoch 69 - D Loss: 0.0051, G Loss: 1.0598\n",
      "Mean L1 Loss: 1.231000, Mean L2 Loss: 3.276101\n",
      "Epoch 70, Iter 2750, D Loss: 0.0109, G Loss: 1.0318, L1: 1.2310, L2: 3.2761\n",
      "Epoch 70 - D Loss: 0.0069, G Loss: 1.0710\n",
      "Mean L1 Loss: 1.232073, Mean L2 Loss: 3.260199\n",
      "Epoch 71, Iter 2800, D Loss: 0.0034, G Loss: 1.0985, L1: 1.2321, L2: 3.2602\n",
      "Epoch 71 - D Loss: 0.0027, G Loss: 1.1049\n",
      "Epoch 72 - D Loss: 0.0036, G Loss: 1.0515\n",
      "Mean L1 Loss: 1.244856, Mean L2 Loss: 3.293420\n",
      "Epoch 73, Iter 2850, D Loss: -0.0058, G Loss: 1.1945, L1: 1.2449, L2: 3.2934\n",
      "Epoch 73 - D Loss: 0.0056, G Loss: 1.1564\n",
      "Mean L1 Loss: 1.218889, Mean L2 Loss: 3.225482\n",
      "Epoch 74, Iter 2900, D Loss: -0.0074, G Loss: 1.1460, L1: 1.2189, L2: 3.2255\n",
      "Epoch 74 - D Loss: 0.0017, G Loss: 1.1291\n",
      "Mean L1 Loss: 1.276490, Mean L2 Loss: 3.426544\n",
      "Epoch 75, Iter 2950, D Loss: 0.0070, G Loss: 1.1396, L1: 1.2765, L2: 3.4265\n",
      "Epoch 75 - D Loss: 0.0048, G Loss: 1.1487\n",
      "Mean L1 Loss: 1.267312, Mean L2 Loss: 3.437295\n",
      "Epoch 76, Iter 3000, D Loss: 0.0047, G Loss: 1.2139, L1: 1.2673, L2: 3.4373\n",
      "Epoch 76 - D Loss: 0.0064, G Loss: 1.2093\n",
      "Epoch 77 - D Loss: -0.0024, G Loss: 1.1549\n",
      "Mean L1 Loss: 1.227568, Mean L2 Loss: 3.251112\n",
      "Epoch 78, Iter 3050, D Loss: 0.0064, G Loss: 1.1982, L1: 1.2276, L2: 3.2511\n",
      "Epoch 78 - D Loss: 0.0011, G Loss: 1.1713\n",
      "Mean L1 Loss: 1.223003, Mean L2 Loss: 3.284921\n",
      "Epoch 79, Iter 3100, D Loss: 0.0042, G Loss: 1.1812, L1: 1.2230, L2: 3.2849\n",
      "Epoch 79 - D Loss: 0.0055, G Loss: 1.1903\n",
      "Mean L1 Loss: 1.219761, Mean L2 Loss: 3.262802\n",
      "Epoch 80, Iter 3150, D Loss: -0.0013, G Loss: 1.1743, L1: 1.2198, L2: 3.2628\n",
      "Epoch 80 - D Loss: -0.0009, G Loss: 1.1786\n",
      "Epoch 81 - D Loss: 0.0029, G Loss: 1.2415\n",
      "Mean L1 Loss: 1.217034, Mean L2 Loss: 3.215272\n",
      "Epoch 82, Iter 3200, D Loss: -0.0008, G Loss: 1.1929, L1: 1.2170, L2: 3.2153\n",
      "Epoch 82 - D Loss: 0.0017, G Loss: 1.2155\n",
      "Mean L1 Loss: 1.231993, Mean L2 Loss: 3.281773\n",
      "Epoch 83, Iter 3250, D Loss: 0.0028, G Loss: 1.2329, L1: 1.2320, L2: 3.2818\n",
      "Epoch 83 - D Loss: 0.0020, G Loss: 1.2177\n",
      "Mean L1 Loss: 1.228805, Mean L2 Loss: 3.306535\n",
      "Epoch 84, Iter 3300, D Loss: 0.0059, G Loss: 1.2505, L1: 1.2288, L2: 3.3065\n",
      "Epoch 84 - D Loss: 0.0071, G Loss: 1.2198\n",
      "Mean L1 Loss: 1.271161, Mean L2 Loss: 3.439026\n",
      "Epoch 85, Iter 3350, D Loss: -0.0017, G Loss: 1.2265, L1: 1.2712, L2: 3.4390\n",
      "Epoch 85 - D Loss: -0.0005, G Loss: 1.2103\n",
      "Epoch 86 - D Loss: 0.0027, G Loss: 1.2032\n",
      "Mean L1 Loss: 1.205119, Mean L2 Loss: 3.163197\n",
      "Epoch 87, Iter 3400, D Loss: 0.0061, G Loss: 1.1967, L1: 1.2051, L2: 3.1632\n",
      "Saved best model with L2: 3.1632\n",
      "Epoch 87 - D Loss: 0.0059, G Loss: 1.2655\n",
      "Mean L1 Loss: 1.265377, Mean L2 Loss: 3.324729\n",
      "Epoch 88, Iter 3450, D Loss: -0.0018, G Loss: 1.2261, L1: 1.2654, L2: 3.3247\n",
      "Epoch 88 - D Loss: 0.0020, G Loss: 1.2236\n",
      "Mean L1 Loss: 1.226603, Mean L2 Loss: 3.240107\n",
      "Epoch 89, Iter 3500, D Loss: 0.0048, G Loss: 1.3199, L1: 1.2266, L2: 3.2401\n",
      "Epoch 89 - D Loss: 0.0040, G Loss: 1.2909\n",
      "Epoch 90 - D Loss: 0.0033, G Loss: 1.2722\n",
      "Mean L1 Loss: 1.207224, Mean L2 Loss: 3.179541\n",
      "Epoch 91, Iter 3550, D Loss: -0.0073, G Loss: 1.0695, L1: 1.2072, L2: 3.1795\n",
      "Epoch 91 - D Loss: 0.0041, G Loss: 1.2558\n",
      "Mean L1 Loss: 1.224968, Mean L2 Loss: 3.248424\n",
      "Epoch 92, Iter 3600, D Loss: 0.0020, G Loss: 1.3125, L1: 1.2250, L2: 3.2484\n",
      "Epoch 92 - D Loss: 0.0045, G Loss: 1.3261\n",
      "Mean L1 Loss: 1.217192, Mean L2 Loss: 3.191975\n",
      "Epoch 93, Iter 3650, D Loss: -0.0051, G Loss: 1.1869, L1: 1.2172, L2: 3.1920\n",
      "Epoch 93 - D Loss: -0.0001, G Loss: 1.2119\n",
      "Mean L1 Loss: 1.245656, Mean L2 Loss: 3.345469\n",
      "Epoch 94, Iter 3700, D Loss: -0.0008, G Loss: 1.3488, L1: 1.2457, L2: 3.3455\n",
      "Epoch 94 - D Loss: -0.0005, G Loss: 1.3333\n",
      "Epoch 95 - D Loss: 0.0011, G Loss: 1.2809\n",
      "Mean L1 Loss: 1.263600, Mean L2 Loss: 3.324564\n",
      "Epoch 96, Iter 3750, D Loss: 0.0047, G Loss: 1.3173, L1: 1.2636, L2: 3.3246\n",
      "Epoch 96 - D Loss: 0.0021, G Loss: 1.3658\n",
      "Mean L1 Loss: 1.232422, Mean L2 Loss: 3.243969\n",
      "Epoch 97, Iter 3800, D Loss: 0.0031, G Loss: 1.3230, L1: 1.2324, L2: 3.2440\n",
      "Epoch 97 - D Loss: 0.0021, G Loss: 1.3607\n",
      "Mean L1 Loss: 1.250693, Mean L2 Loss: 3.311242\n",
      "Epoch 98, Iter 3850, D Loss: -0.0014, G Loss: 1.3221, L1: 1.2507, L2: 3.3112\n",
      "Epoch 98 - D Loss: 0.0011, G Loss: 1.3467\n",
      "Mean L1 Loss: 1.224458, Mean L2 Loss: 3.218544\n",
      "Epoch 99, Iter 3900, D Loss: 0.0042, G Loss: 1.3437, L1: 1.2245, L2: 3.2185\n",
      "Epoch 99 - D Loss: 0.0042, G Loss: 1.3437\n",
      "Epoch 100 - D Loss: 0.0054, G Loss: 1.3747\n",
      "Mean L1 Loss: 1.230176, Mean L2 Loss: 3.262272\n",
      "Epoch 101, Iter 3950, D Loss: 0.0073, G Loss: 1.3703, L1: 1.2302, L2: 3.2623\n",
      "Epoch 101 - D Loss: 0.0034, G Loss: 1.3629\n",
      "Mean L1 Loss: 1.207143, Mean L2 Loss: 3.148489\n",
      "Epoch 102, Iter 4000, D Loss: -0.0003, G Loss: 1.2915, L1: 1.2071, L2: 3.1485\n",
      "Saved best model with L2: 3.1485\n",
      "Epoch 102 - D Loss: 0.0018, G Loss: 1.3070\n",
      "Mean L1 Loss: 1.232559, Mean L2 Loss: 3.326939\n",
      "Epoch 103, Iter 4050, D Loss: -0.0016, G Loss: 1.3976, L1: 1.2326, L2: 3.3269\n",
      "Epoch 103 - D Loss: 0.0033, G Loss: 1.3861\n",
      "Epoch 104 - D Loss: 0.0055, G Loss: 1.4161\n",
      "Mean L1 Loss: 1.240485, Mean L2 Loss: 3.265327\n",
      "Epoch 105, Iter 4100, D Loss: 0.0019, G Loss: 1.3491, L1: 1.2405, L2: 3.2653\n",
      "Epoch 105 - D Loss: 0.0000, G Loss: 1.3992\n",
      "Mean L1 Loss: 1.224640, Mean L2 Loss: 3.223565\n",
      "Epoch 106, Iter 4150, D Loss: 0.0020, G Loss: 1.3946, L1: 1.2246, L2: 3.2236\n",
      "Epoch 106 - D Loss: 0.0004, G Loss: 1.4130\n",
      "Mean L1 Loss: 1.213084, Mean L2 Loss: 3.135978\n",
      "Epoch 107, Iter 4200, D Loss: 0.0029, G Loss: 1.4385, L1: 1.2131, L2: 3.1360\n",
      "Saved best model with L2: 3.1360\n",
      "Epoch 107 - D Loss: 0.0016, G Loss: 1.4181\n",
      "Mean L1 Loss: 1.220533, Mean L2 Loss: 3.200756\n",
      "Epoch 108, Iter 4250, D Loss: 0.0039, G Loss: 1.3459, L1: 1.2205, L2: 3.2008\n",
      "Epoch 108 - D Loss: 0.0037, G Loss: 1.3401\n",
      "Epoch 109 - D Loss: 0.0001, G Loss: 1.4177\n",
      "Mean L1 Loss: 1.286635, Mean L2 Loss: 3.378164\n",
      "Epoch 110, Iter 4300, D Loss: 0.0044, G Loss: 1.3742, L1: 1.2866, L2: 3.3782\n",
      "Epoch 110 - D Loss: -0.0028, G Loss: 1.4012\n",
      "Mean L1 Loss: 1.209406, Mean L2 Loss: 3.177913\n",
      "Epoch 111, Iter 4350, D Loss: 0.0039, G Loss: 1.3891, L1: 1.2094, L2: 3.1779\n",
      "Epoch 111 - D Loss: 0.0027, G Loss: 1.4069\n",
      "Mean L1 Loss: 1.217118, Mean L2 Loss: 3.177291\n",
      "Epoch 112, Iter 4400, D Loss: 0.0007, G Loss: 1.3970, L1: 1.2171, L2: 3.1773\n",
      "Epoch 112 - D Loss: -0.0031, G Loss: 1.3973\n",
      "Epoch 113 - D Loss: 0.0018, G Loss: 1.4159\n",
      "Mean L1 Loss: 1.221267, Mean L2 Loss: 3.204263\n",
      "Epoch 114, Iter 4450, D Loss: 0.0065, G Loss: 1.4744, L1: 1.2213, L2: 3.2043\n",
      "Epoch 114 - D Loss: 0.0042, G Loss: 1.4185\n",
      "Mean L1 Loss: 1.226912, Mean L2 Loss: 3.200208\n",
      "Epoch 115, Iter 4500, D Loss: 0.0006, G Loss: 1.3953, L1: 1.2269, L2: 3.2002\n",
      "Epoch 115 - D Loss: 0.0008, G Loss: 1.4000\n",
      "Mean L1 Loss: 1.211771, Mean L2 Loss: 3.136401\n",
      "Epoch 116, Iter 4550, D Loss: 0.0002, G Loss: 1.4727, L1: 1.2118, L2: 3.1364\n",
      "Epoch 116 - D Loss: -0.0010, G Loss: 1.4663\n",
      "Mean L1 Loss: 1.224671, Mean L2 Loss: 3.228284\n",
      "Epoch 117, Iter 4600, D Loss: 0.0016, G Loss: 1.4056, L1: 1.2247, L2: 3.2283\n",
      "Epoch 117 - D Loss: 0.0013, G Loss: 1.4095\n",
      "Epoch 118 - D Loss: -0.0020, G Loss: 1.4530\n",
      "Mean L1 Loss: 1.214545, Mean L2 Loss: 3.174338\n",
      "Epoch 119, Iter 4650, D Loss: -0.0066, G Loss: 1.4761, L1: 1.2145, L2: 3.1743\n",
      "Epoch 119 - D Loss: -0.0045, G Loss: 1.4791\n",
      "Mean L1 Loss: 1.225270, Mean L2 Loss: 3.174114\n",
      "Epoch 120, Iter 4700, D Loss: -0.0025, G Loss: 1.4339, L1: 1.2253, L2: 3.1741\n",
      "Epoch 120 - D Loss: -0.0003, G Loss: 1.4700\n",
      "Mean L1 Loss: 1.214709, Mean L2 Loss: 3.178956\n",
      "Epoch 121, Iter 4750, D Loss: 0.0010, G Loss: 1.5095, L1: 1.2147, L2: 3.1790\n",
      "Epoch 121 - D Loss: 0.0001, G Loss: 1.5192\n",
      "Epoch 122 - D Loss: -0.0017, G Loss: 1.5276\n",
      "Mean L1 Loss: 1.264033, Mean L2 Loss: 3.380152\n",
      "Epoch 123, Iter 4800, D Loss: 0.0128, G Loss: 1.6347, L1: 1.2640, L2: 3.3802\n",
      "Epoch 123 - D Loss: 0.0031, G Loss: 1.5295\n",
      "Mean L1 Loss: 1.250330, Mean L2 Loss: 3.298184\n",
      "Epoch 124, Iter 4850, D Loss: 0.0039, G Loss: 1.5832, L1: 1.2503, L2: 3.2982\n",
      "Epoch 124 - D Loss: 0.0006, G Loss: 1.5470\n",
      "Mean L1 Loss: 1.220526, Mean L2 Loss: 3.194873\n",
      "Epoch 125, Iter 4900, D Loss: -0.0031, G Loss: 1.5064, L1: 1.2205, L2: 3.1949\n",
      "Epoch 125 - D Loss: -0.0023, G Loss: 1.5092\n",
      "Mean L1 Loss: 1.222042, Mean L2 Loss: 3.213401\n",
      "Epoch 126, Iter 4950, D Loss: 0.0034, G Loss: 1.5725, L1: 1.2220, L2: 3.2134\n",
      "Epoch 126 - D Loss: 0.0025, G Loss: 1.5688\n",
      "Epoch 127 - D Loss: -0.0005, G Loss: 1.5828\n",
      "Mean L1 Loss: 1.207772, Mean L2 Loss: 3.118905\n",
      "Epoch 128, Iter 5000, D Loss: -0.0029, G Loss: 1.6003, L1: 1.2078, L2: 3.1189\n",
      "Saved best model with L2: 3.1189\n",
      "Epoch 128 - D Loss: 0.0004, G Loss: 1.5547\n",
      "Mean L1 Loss: 1.207008, Mean L2 Loss: 3.126873\n",
      "Epoch 129, Iter 5050, D Loss: 0.0046, G Loss: 1.5802, L1: 1.2070, L2: 3.1269\n",
      "Epoch 129 - D Loss: 0.0034, G Loss: 1.5783\n",
      "Mean L1 Loss: 1.266447, Mean L2 Loss: 3.289805\n",
      "Epoch 130, Iter 5100, D Loss: -0.0038, G Loss: 1.5672, L1: 1.2664, L2: 3.2898\n",
      "Epoch 130 - D Loss: -0.0025, G Loss: 1.5413\n",
      "Epoch 131 - D Loss: -0.0028, G Loss: 1.5899\n",
      "Mean L1 Loss: 1.220212, Mean L2 Loss: 3.227437\n",
      "Epoch 132, Iter 5150, D Loss: -0.0027, G Loss: 1.6161, L1: 1.2202, L2: 3.2274\n",
      "Epoch 132 - D Loss: -0.0019, G Loss: 1.6141\n",
      "Mean L1 Loss: 1.215155, Mean L2 Loss: 3.169528\n",
      "Epoch 133, Iter 5200, D Loss: -0.0060, G Loss: 1.6333, L1: 1.2152, L2: 3.1695\n",
      "Epoch 133 - D Loss: -0.0043, G Loss: 1.6194\n",
      "Mean L1 Loss: 1.234360, Mean L2 Loss: 3.264964\n",
      "Epoch 134, Iter 5250, D Loss: -0.0009, G Loss: 1.5146, L1: 1.2344, L2: 3.2650\n",
      "Epoch 134 - D Loss: -0.0002, G Loss: 1.5582\n",
      "Mean L1 Loss: 1.252610, Mean L2 Loss: 3.375742\n",
      "Epoch 135, Iter 5300, D Loss: -0.0033, G Loss: 1.5534, L1: 1.2526, L2: 3.3757\n",
      "Epoch 135 - D Loss: -0.0027, G Loss: 1.5698\n",
      "Epoch 136 - D Loss: -0.0017, G Loss: 1.6297\n",
      "Mean L1 Loss: 1.230038, Mean L2 Loss: 3.276557\n",
      "Epoch 137, Iter 5350, D Loss: -0.0115, G Loss: 1.6184, L1: 1.2300, L2: 3.2766\n",
      "Epoch 137 - D Loss: 0.0015, G Loss: 1.6366\n",
      "Mean L1 Loss: 1.212970, Mean L2 Loss: 3.206382\n",
      "Epoch 138, Iter 5400, D Loss: 0.0035, G Loss: 1.6530, L1: 1.2130, L2: 3.2064\n",
      "Epoch 138 - D Loss: 0.0020, G Loss: 1.6453\n",
      "Mean L1 Loss: 1.258483, Mean L2 Loss: 3.358502\n",
      "Epoch 139, Iter 5450, D Loss: -0.0014, G Loss: 1.6201, L1: 1.2585, L2: 3.3585\n",
      "Epoch 139 - D Loss: -0.0003, G Loss: 1.6149\n",
      "Epoch 140 - D Loss: -0.0009, G Loss: 1.6679\n",
      "Mean L1 Loss: 1.222008, Mean L2 Loss: 3.194583\n",
      "Epoch 141, Iter 5500, D Loss: 0.0259, G Loss: 1.5740, L1: 1.2220, L2: 3.1946\n",
      "Epoch 141 - D Loss: -0.0013, G Loss: 1.6111\n",
      "Mean L1 Loss: 1.229250, Mean L2 Loss: 3.167774\n",
      "Epoch 142, Iter 5550, D Loss: -0.0003, G Loss: 1.6384, L1: 1.2293, L2: 3.1678\n",
      "Epoch 142 - D Loss: 0.0005, G Loss: 1.6897\n",
      "Mean L1 Loss: 1.221711, Mean L2 Loss: 3.199609\n",
      "Epoch 143, Iter 5600, D Loss: -0.0011, G Loss: 1.7111, L1: 1.2217, L2: 3.1996\n",
      "Epoch 143 - D Loss: -0.0017, G Loss: 1.7113\n",
      "Mean L1 Loss: 1.230109, Mean L2 Loss: 3.144840\n",
      "Epoch 144, Iter 5650, D Loss: -0.0026, G Loss: 1.7113, L1: 1.2301, L2: 3.1448\n",
      "Epoch 144 - D Loss: -0.0013, G Loss: 1.7049\n",
      "Epoch 145 - D Loss: 0.0015, G Loss: 1.6827\n",
      "Mean L1 Loss: 1.244092, Mean L2 Loss: 3.298335\n",
      "Epoch 146, Iter 5700, D Loss: 0.0023, G Loss: 1.7626, L1: 1.2441, L2: 3.2983\n",
      "Epoch 146 - D Loss: -0.0024, G Loss: 1.7200\n",
      "Mean L1 Loss: 1.240496, Mean L2 Loss: 3.251146\n",
      "Epoch 147, Iter 5750, D Loss: 0.0012, G Loss: 1.7463, L1: 1.2405, L2: 3.2511\n",
      "Epoch 147 - D Loss: 0.0010, G Loss: 1.7570\n",
      "Mean L1 Loss: 1.258102, Mean L2 Loss: 3.341577\n",
      "Epoch 148, Iter 5800, D Loss: -0.0044, G Loss: 1.7338, L1: 1.2581, L2: 3.3416\n",
      "Epoch 148 - D Loss: -0.0029, G Loss: 1.7407\n",
      "Mean L1 Loss: 1.282500, Mean L2 Loss: 3.385970\n",
      "Epoch 149, Iter 5850, D Loss: -0.0041, G Loss: 1.7440, L1: 1.2825, L2: 3.3860\n",
      "Epoch 149 - D Loss: -0.0041, G Loss: 1.7440\n",
      "Epoch 150 - D Loss: -0.0025, G Loss: 1.7063\n",
      "Mean L1 Loss: 1.220720, Mean L2 Loss: 3.200705\n",
      "Epoch 151, Iter 5900, D Loss: 0.0031, G Loss: 1.7100, L1: 1.2207, L2: 3.2007\n",
      "Epoch 151 - D Loss: -0.0037, G Loss: 1.7399\n",
      "Mean L1 Loss: 1.237522, Mean L2 Loss: 3.262988\n",
      "Epoch 152, Iter 5950, D Loss: -0.0025, G Loss: 1.7542, L1: 1.2375, L2: 3.2630\n",
      "Epoch 152 - D Loss: -0.0014, G Loss: 1.7544\n",
      "Mean L1 Loss: 1.211341, Mean L2 Loss: 3.147797\n",
      "Epoch 153, Iter 6000, D Loss: -0.0040, G Loss: 1.7482, L1: 1.2113, L2: 3.1478\n",
      "Epoch 153 - D Loss: -0.0034, G Loss: 1.7485\n",
      "Epoch 154 - D Loss: -0.0049, G Loss: 1.7639\n",
      "Mean L1 Loss: 1.253526, Mean L2 Loss: 3.401761\n",
      "Epoch 155, Iter 6050, D Loss: -0.0104, G Loss: 1.6754, L1: 1.2535, L2: 3.4018\n",
      "Epoch 155 - D Loss: -0.0017, G Loss: 1.7860\n",
      "Mean L1 Loss: 1.231136, Mean L2 Loss: 3.234634\n",
      "Epoch 156, Iter 6100, D Loss: -0.0045, G Loss: 1.7968, L1: 1.2311, L2: 3.2346\n",
      "Epoch 156 - D Loss: -0.0043, G Loss: 1.7855\n",
      "Mean L1 Loss: 1.213335, Mean L2 Loss: 3.163721\n",
      "Epoch 157, Iter 6150, D Loss: -0.0006, G Loss: 1.7652, L1: 1.2133, L2: 3.1637\n",
      "Epoch 157 - D Loss: 0.0006, G Loss: 1.7813\n",
      "Mean L1 Loss: 1.240975, Mean L2 Loss: 3.227180\n",
      "Epoch 158, Iter 6200, D Loss: -0.0040, G Loss: 1.7245, L1: 1.2410, L2: 3.2272\n",
      "Epoch 158 - D Loss: -0.0043, G Loss: 1.7277\n",
      "Epoch 159 - D Loss: -0.0019, G Loss: 1.7691\n",
      "Mean L1 Loss: 1.235802, Mean L2 Loss: 3.255455\n",
      "Epoch 160, Iter 6250, D Loss: -0.0070, G Loss: 1.8586, L1: 1.2358, L2: 3.2555\n",
      "Epoch 160 - D Loss: -0.0013, G Loss: 1.7749\n",
      "Mean L1 Loss: 1.224587, Mean L2 Loss: 3.192200\n",
      "Epoch 161, Iter 6300, D Loss: -0.0023, G Loss: 1.7227, L1: 1.2246, L2: 3.1922\n",
      "Epoch 161 - D Loss: 0.0004, G Loss: 1.7438\n",
      "Mean L1 Loss: 1.217702, Mean L2 Loss: 3.181177\n",
      "Epoch 162, Iter 6350, D Loss: -0.0040, G Loss: 1.7840, L1: 1.2177, L2: 3.1812\n",
      "Epoch 162 - D Loss: -0.0021, G Loss: 1.7920\n",
      "Epoch 163 - D Loss: -0.0033, G Loss: 1.8154\n",
      "Mean L1 Loss: 1.240440, Mean L2 Loss: 3.279067\n",
      "Epoch 164, Iter 6400, D Loss: -0.0042, G Loss: 1.7658, L1: 1.2404, L2: 3.2791\n",
      "Epoch 164 - D Loss: -0.0034, G Loss: 1.7681\n",
      "Mean L1 Loss: 1.211884, Mean L2 Loss: 3.100575\n",
      "Epoch 165, Iter 6450, D Loss: -0.0090, G Loss: 1.7070, L1: 1.2119, L2: 3.1006\n",
      "Saved best model with L2: 3.1006\n",
      "Epoch 165 - D Loss: -0.0024, G Loss: 1.7658\n",
      "Mean L1 Loss: 1.265179, Mean L2 Loss: 3.339415\n",
      "Epoch 166, Iter 6500, D Loss: -0.0037, G Loss: 1.8453, L1: 1.2652, L2: 3.3394\n",
      "Epoch 166 - D Loss: -0.0018, G Loss: 1.8192\n",
      "Mean L1 Loss: 1.212163, Mean L2 Loss: 3.169796\n",
      "Epoch 167, Iter 6550, D Loss: -0.0049, G Loss: 1.7936, L1: 1.2122, L2: 3.1698\n",
      "Epoch 167 - D Loss: -0.0046, G Loss: 1.7885\n",
      "Epoch 168 - D Loss: -0.0023, G Loss: 1.7638\n",
      "Mean L1 Loss: 1.267882, Mean L2 Loss: 3.378831\n",
      "Epoch 169, Iter 6600, D Loss: 0.0010, G Loss: 1.7608, L1: 1.2679, L2: 3.3788\n",
      "Epoch 169 - D Loss: -0.0002, G Loss: 1.8147\n",
      "Mean L1 Loss: 1.227273, Mean L2 Loss: 3.238483\n",
      "Epoch 170, Iter 6650, D Loss: 0.0007, G Loss: 1.8354, L1: 1.2273, L2: 3.2385\n",
      "Epoch 170 - D Loss: -0.0027, G Loss: 1.8245\n",
      "Mean L1 Loss: 1.213515, Mean L2 Loss: 3.221832\n",
      "Epoch 171, Iter 6700, D Loss: 0.0029, G Loss: 1.8118, L1: 1.2135, L2: 3.2218\n",
      "Epoch 171 - D Loss: 0.0010, G Loss: 1.8214\n",
      "Epoch 172 - D Loss: 0.0004, G Loss: 1.8044\n",
      "Mean L1 Loss: 1.229114, Mean L2 Loss: 3.247681\n",
      "Epoch 173, Iter 6750, D Loss: -0.0165, G Loss: 1.7290, L1: 1.2291, L2: 3.2477\n",
      "Epoch 173 - D Loss: -0.0033, G Loss: 1.8067\n",
      "Mean L1 Loss: 1.224795, Mean L2 Loss: 3.238324\n",
      "Epoch 174, Iter 6800, D Loss: -0.0039, G Loss: 1.7755, L1: 1.2248, L2: 3.2383\n",
      "Epoch 174 - D Loss: 0.0012, G Loss: 1.8143\n",
      "Mean L1 Loss: 1.248973, Mean L2 Loss: 3.325027\n",
      "Epoch 175, Iter 6850, D Loss: -0.0057, G Loss: 1.7543, L1: 1.2490, L2: 3.3250\n",
      "Epoch 175 - D Loss: -0.0075, G Loss: 1.7744\n",
      "Mean L1 Loss: 1.223603, Mean L2 Loss: 3.225001\n",
      "Epoch 176, Iter 6900, D Loss: 0.0006, G Loss: 1.8432, L1: 1.2236, L2: 3.2250\n",
      "Epoch 176 - D Loss: 0.0002, G Loss: 1.8393\n",
      "Epoch 177 - D Loss: 0.0006, G Loss: 1.8080\n",
      "Mean L1 Loss: 1.217968, Mean L2 Loss: 3.216251\n",
      "Epoch 178, Iter 6950, D Loss: -0.0005, G Loss: 1.7644, L1: 1.2180, L2: 3.2163\n",
      "Epoch 178 - D Loss: -0.0008, G Loss: 1.8191\n",
      "Mean L1 Loss: 1.237275, Mean L2 Loss: 3.290508\n",
      "Epoch 179, Iter 7000, D Loss: -0.0108, G Loss: 1.8110, L1: 1.2373, L2: 3.2905\n",
      "Epoch 179 - D Loss: -0.0041, G Loss: 1.8306\n",
      "Mean L1 Loss: 1.262571, Mean L2 Loss: 3.362481\n",
      "Epoch 180, Iter 7050, D Loss: -0.0021, G Loss: 1.8716, L1: 1.2626, L2: 3.3625\n",
      "Epoch 180 - D Loss: -0.0003, G Loss: 1.8656\n",
      "Epoch 181 - D Loss: -0.0051, G Loss: 1.8700\n",
      "Mean L1 Loss: 1.228239, Mean L2 Loss: 3.320990\n",
      "Epoch 182, Iter 7100, D Loss: -0.0052, G Loss: 1.7138, L1: 1.2282, L2: 3.3210\n",
      "Epoch 182 - D Loss: -0.0018, G Loss: 1.7958\n",
      "Mean L1 Loss: 1.223822, Mean L2 Loss: 3.216166\n",
      "Epoch 183, Iter 7150, D Loss: 0.0000, G Loss: 1.7973, L1: 1.2238, L2: 3.2162\n",
      "Epoch 183 - D Loss: -0.0021, G Loss: 1.8473\n",
      "Mean L1 Loss: 1.223243, Mean L2 Loss: 3.253658\n",
      "Epoch 184, Iter 7200, D Loss: -0.0014, G Loss: 1.8509, L1: 1.2232, L2: 3.2537\n",
      "Epoch 184 - D Loss: -0.0006, G Loss: 1.8645\n",
      "Mean L1 Loss: 1.209206, Mean L2 Loss: 3.104351\n",
      "Epoch 185, Iter 7250, D Loss: -0.0032, G Loss: 1.8972, L1: 1.2092, L2: 3.1044\n",
      "Epoch 185 - D Loss: -0.0037, G Loss: 1.9016\n",
      "Epoch 186 - D Loss: -0.0035, G Loss: 1.9214\n",
      "Mean L1 Loss: 1.238532, Mean L2 Loss: 3.317402\n",
      "Epoch 187, Iter 7300, D Loss: 0.0069, G Loss: 1.9336, L1: 1.2385, L2: 3.3174\n",
      "Epoch 187 - D Loss: -0.0019, G Loss: 1.9032\n",
      "Mean L1 Loss: 1.218135, Mean L2 Loss: 3.188107\n",
      "Epoch 188, Iter 7350, D Loss: 0.0019, G Loss: 1.8745, L1: 1.2181, L2: 3.1881\n",
      "Epoch 188 - D Loss: 0.0007, G Loss: 1.8920\n",
      "Mean L1 Loss: 1.212955, Mean L2 Loss: 3.195707\n",
      "Epoch 189, Iter 7400, D Loss: -0.0082, G Loss: 1.8922, L1: 1.2130, L2: 3.1957\n",
      "Epoch 189 - D Loss: -0.0054, G Loss: 1.8970\n",
      "Epoch 190 - D Loss: -0.0029, G Loss: 1.9272\n",
      "Mean L1 Loss: 1.220658, Mean L2 Loss: 3.217115\n",
      "Epoch 191, Iter 7450, D Loss: -0.0178, G Loss: 1.7389, L1: 1.2207, L2: 3.2171\n",
      "Epoch 191 - D Loss: -0.0034, G Loss: 1.9232\n",
      "Mean L1 Loss: 1.236667, Mean L2 Loss: 3.260702\n",
      "Epoch 192, Iter 7500, D Loss: -0.0010, G Loss: 1.8946, L1: 1.2367, L2: 3.2607\n",
      "Epoch 192 - D Loss: -0.0034, G Loss: 1.8963\n",
      "Mean L1 Loss: 1.254812, Mean L2 Loss: 3.397313\n",
      "Epoch 193, Iter 7550, D Loss: -0.0059, G Loss: 1.9778, L1: 1.2548, L2: 3.3973\n",
      "Epoch 193 - D Loss: -0.0038, G Loss: 1.9761\n",
      "Mean L1 Loss: 1.249261, Mean L2 Loss: 3.311655\n",
      "Epoch 194, Iter 7600, D Loss: -0.0014, G Loss: 1.9343, L1: 1.2493, L2: 3.3117\n",
      "Epoch 194 - D Loss: -0.0008, G Loss: 1.9526\n",
      "Epoch 195 - D Loss: -0.0017, G Loss: 1.9497\n",
      "Mean L1 Loss: 1.223804, Mean L2 Loss: 3.270118\n",
      "Epoch 196, Iter 7650, D Loss: 0.0041, G Loss: 1.9369, L1: 1.2238, L2: 3.2701\n",
      "Epoch 196 - D Loss: 0.0005, G Loss: 1.9352\n",
      "Mean L1 Loss: 1.226257, Mean L2 Loss: 3.219783\n",
      "Epoch 197, Iter 7700, D Loss: 0.0027, G Loss: 1.8981, L1: 1.2263, L2: 3.2198\n",
      "Epoch 197 - D Loss: 0.0012, G Loss: 1.8951\n",
      "Mean L1 Loss: 1.210585, Mean L2 Loss: 3.173942\n",
      "Epoch 198, Iter 7750, D Loss: 0.0003, G Loss: 1.9371, L1: 1.2106, L2: 3.1739\n",
      "Epoch 198 - D Loss: 0.0003, G Loss: 1.9369\n",
      "Mean L1 Loss: 1.226389, Mean L2 Loss: 3.250054\n",
      "Epoch 199, Iter 7800, D Loss: -0.0019, G Loss: 1.9166, L1: 1.2264, L2: 3.2501\n",
      "Epoch 199 - D Loss: -0.0019, G Loss: 1.9166\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trained_G, trained_D = train_WGR_fnn(D=D_net, G=G_net, D_solver=D_solver, G_solver=G_solver, \n",
    "                                     loader_train = loader_train, loader_val=loader_val,\n",
    "                                     noise_dim=args.noise_dim, Xdim=args.Xdim, Ydim=args.Ydim, batch_size=args.train_batch,\n",
    "                                     save_path='./', device='cpu', num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf00ecd-57df-4b58-841e-932233afcf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M1, Univariate, Ydim: 1, J_t_size: 50\n",
      "L1 Loss: tensor([1.1950])\n",
      "L2 Loss: tensor([2.9834])\n",
      "MSE Mean: tensor([0.2318])\n",
      "MSE SD: tensor([0.2440])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the L1 and L2 error, MSE of conditional mean and conditional standard deviation on the test data  \n",
    "test_G_mean_sd = L1L2_MSE_mean_sd_G(G = trained_G,  test_size = args.test, noise_dim=args.noise_dim, \n",
    "                                    batch_size=args.test_batch, loader_dataset = loader_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53a40ac-b651-443f-8476-bbc9949dbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_5: 1.0164, Q_25: 0.3737, Q_50: 0.1901, Q_75: 0.3080, Q_95: 0.7316\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE of conditional quantiles at different levels.\n",
    "test_G_quantile = MSE_quantile_G_uniY(G = trained_G, loader_dataset = loader_test , noise_dim=args.noise_dim, \n",
    "                                      test_size = args.test,  batch_size=args.test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc16ad8-4d13-4b6f-ac05-8c45ab75c6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
