{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f863c7-52e9-4f51-ae00-bc3c97f2ba53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nincase the above code does not work, you can use the absolute path instead\\nsys.path.append(r\".\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  #use to import the defined functions\n",
    "parent_dir = os.path.dirname(current_dir) \n",
    "sys.path.append(parent_dir)  \n",
    "\n",
    "\"\"\"\n",
    "incase the above code does not work, you can use the absolute path instead\n",
    "sys.path.append(r\".\\\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f578f4-d40c-4e9c-b234-ea17249bff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec819f9-bbc8-4b4f-bb26-cf48a2ce0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.basic_utils import setup_seed\n",
    "from data.SimulationData import DataGenerator \n",
    "from utils.training_utils import train_WGR_fnn\n",
    "from utils.evaluation_utils import L1L2_MSE_mean_sd_G, MSE_quantile_G_uniY\n",
    "from models.generator import generator_fnn\n",
    "from models.discriminator import discriminator_fnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82f422a-70a8-4191-b99d-16612d3dbef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Xdim=5, Ydim=1, model='M2', noise_dim=5, noise_dist='gaussian', train=5000, val=1000, test=1000, train_batch=128, val_batch=100, test_batch=100, epochs=100, reps=100)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if 'ipykernel_launcher.py' in sys.argv[0]:  #if not work in jupyter, you can delete this part\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] \n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Implementation of WGR for M1')\n",
    "\n",
    "parser.add_argument('--Xdim', default=5, type=int, help='dimensionality of X')\n",
    "parser.add_argument('--Ydim', default=1, type=int, help='dimensionality of Y')\n",
    "parser.add_argument('--model', default='M2', type=str, help='model')\n",
    "\n",
    "parser.add_argument('--noise_dim', default=5, type=int, help='dimensionality of noise vector')\n",
    "parser.add_argument('--noise_dist', default='gaussian', type=str, help='distribution of noise vector')\n",
    "\n",
    "parser.add_argument('--train', default=5000, type=int, help='size of train dataset')\n",
    "parser.add_argument('--val', default=1000, type=int, help='size of validation dataset')\n",
    "parser.add_argument('--test', default=1000, type=int, help='size of test dataset')\n",
    "\n",
    "parser.add_argument('--train_batch', default=128, type=int, metavar='BS', help='batch size while training')\n",
    "parser.add_argument('--val_batch', default=100, type=int, metavar='BS', help='batch size while validation')\n",
    "parser.add_argument('--test_batch', default=100, type=int, metavar='BS', help='batch size while testing')\n",
    "parser.add_argument('--epochs', default=100, type=int, help='number of epochs to train')\n",
    "parser.add_argument('--reps', default=100, type=int, help='number of replications')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c1b44a-0c6c-478a-b807-e5e1da20a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "setup_seed(5678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177a8bd5-74a6-4ffe-ab47-e2e7cc821c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from M2\n",
    "data_gen = DataGenerator(args)\n",
    "DATA = data_gen.generate_data('M2')\n",
    "train_X, train_Y = DATA['train_X'], DATA['train_Y']\n",
    "val_X, val_Y = DATA['val_X'], DATA['val_Y']\n",
    "test_X, test_Y = DATA['test_X'], DATA['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de288fbd-c582-4ad8-b10d-59b3d145e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets and initialize a DataLoaders\n",
    "train_dataset = TensorDataset( train_X.float(), train_Y.float() )\n",
    "loader_train = DataLoader(train_dataset , batch_size=args.train_batch, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset( val_X.float(), val_Y.float() )\n",
    "loader_val = DataLoader(val_dataset , batch_size=args.val_batch, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset( test_X.float(), test_Y.float() )\n",
    "loader_test  = DataLoader(test_dataset , batch_size=args.test_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bb4d11-47ef-4e17-9852-27ea723f271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator network and discriminator network\n",
    "G_net = generator_fnn(Xdim=args.Xdim, Ydim=args.Ydim, noise_dim=args.noise_dim, hidden_dims = [64, 32])\n",
    "D_net = discriminator_fnn(input_dim=args.Xdim+args.Ydim, hidden_dims = [64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ccda7e-1d68-4ff1-9e4b-d5caa66dcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RMSprop optimizers\n",
    "D_solver = optim.Adam(D_net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "G_solver = optim.Adam(G_net.parameters(), lr=0.001, betas=(0.9, 0.999))                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c643e20e-1daf-420e-9145-fac2f3c39e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 Loss: 6.728624, Mean L2 Loss: 86.253746\n",
      "Epoch 0 - D Loss: 2.7911, G Loss: 7.5823\n",
      "Epoch 1 - D Loss: -2.2032, G Loss: 1.6467\n",
      "Epoch 2 - D Loss: 2.0292, G Loss: -1.9246\n",
      "Epoch 3 - D Loss: 0.3748, G Loss: 4.5907\n",
      "Epoch 4 - D Loss: 0.7547, G Loss: 6.9154\n",
      "Epoch 5 - D Loss: 0.2695, G Loss: 4.0228\n",
      "Epoch 6 - D Loss: 0.1154, G Loss: 1.6121\n",
      "Epoch 7 - D Loss: -0.1679, G Loss: 4.6640\n",
      "Epoch 8 - D Loss: 0.2876, G Loss: 4.2864\n",
      "Epoch 9 - D Loss: -0.0555, G Loss: 2.2260\n",
      "Epoch 10 - D Loss: -0.1342, G Loss: 2.6173\n",
      "Epoch 11 - D Loss: 0.1297, G Loss: 2.3706\n",
      "Epoch 12 - D Loss: 0.1163, G Loss: 0.4810\n",
      "Epoch 13 - D Loss: 0.1085, G Loss: -0.0949\n",
      "Epoch 14 - D Loss: 0.0586, G Loss: -0.0394\n",
      "Epoch 15 - D Loss: 0.0377, G Loss: -0.1144\n",
      "Epoch 16 - D Loss: 0.0252, G Loss: -0.0896\n",
      "Epoch 17 - D Loss: 0.0132, G Loss: -0.2361\n",
      "Epoch 18 - D Loss: 0.0025, G Loss: -0.3572\n",
      "Epoch 19 - D Loss: -0.0075, G Loss: -0.3748\n",
      "Epoch 20 - D Loss: -0.0210, G Loss: -0.4515\n",
      "Epoch 21 - D Loss: -0.0322, G Loss: -0.3666\n",
      "Epoch 22 - D Loss: -0.0309, G Loss: -0.4125\n",
      "Epoch 23 - D Loss: -0.0286, G Loss: -0.3586\n",
      "Epoch 24 - D Loss: -0.0206, G Loss: -0.5149\n",
      "Mean L1 Loss: 0.917243, Mean L2 Loss: 2.407272\n",
      "Epoch 25, Iter 1000, D Loss: -0.0151, G Loss: -0.6104, L1: 0.9172, L2: 2.4073\n",
      "Saved best model with L2: 2.4073\n",
      "Epoch 25 - D Loss: -0.0171, G Loss: -0.6178\n",
      "Mean L1 Loss: 0.895941, Mean L2 Loss: 2.204789\n",
      "Epoch 26, Iter 1050, D Loss: -0.0218, G Loss: -0.7021, L1: 0.8959, L2: 2.2048\n",
      "Saved best model with L2: 2.2048\n",
      "Epoch 26 - D Loss: -0.0210, G Loss: -0.7059\n",
      "Epoch 27 - D Loss: -0.0246, G Loss: -0.9018\n",
      "Mean L1 Loss: 0.879916, Mean L2 Loss: 2.030943\n",
      "Epoch 28, Iter 1100, D Loss: -0.0334, G Loss: -0.8096, L1: 0.8799, L2: 2.0309\n",
      "Saved best model with L2: 2.0309\n",
      "Epoch 28 - D Loss: -0.0249, G Loss: -1.0123\n",
      "Mean L1 Loss: 0.866706, Mean L2 Loss: 1.863287\n",
      "Epoch 29, Iter 1150, D Loss: -0.0265, G Loss: -1.0436, L1: 0.8667, L2: 1.8633\n",
      "Saved best model with L2: 1.8633\n",
      "Epoch 29 - D Loss: -0.0267, G Loss: -1.1180\n",
      "Mean L1 Loss: 0.842373, Mean L2 Loss: 1.760660\n",
      "Epoch 30, Iter 1200, D Loss: -0.0222, G Loss: -1.2044, L1: 0.8424, L2: 1.7607\n",
      "Saved best model with L2: 1.7607\n",
      "Epoch 30 - D Loss: -0.0221, G Loss: -1.2220\n",
      "Epoch 31 - D Loss: -0.0139, G Loss: -1.2821\n",
      "Mean L1 Loss: 1.015258, Mean L2 Loss: 2.119262\n",
      "Epoch 32, Iter 1250, D Loss: 0.0255, G Loss: -1.3617, L1: 1.0153, L2: 2.1193\n",
      "Epoch 32 - D Loss: 0.0174, G Loss: -1.3770\n",
      "Mean L1 Loss: 1.005440, Mean L2 Loss: 2.121227\n",
      "Epoch 33, Iter 1300, D Loss: 0.0218, G Loss: -2.1198, L1: 1.0054, L2: 2.1212\n",
      "Epoch 33 - D Loss: 0.0277, G Loss: -1.5092\n",
      "Mean L1 Loss: 1.070555, Mean L2 Loss: 2.161346\n",
      "Epoch 34, Iter 1350, D Loss: 0.0400, G Loss: -1.9302, L1: 1.0706, L2: 2.1613\n",
      "Epoch 34 - D Loss: 0.0534, G Loss: -1.7758\n",
      "Mean L1 Loss: 1.057795, Mean L2 Loss: 1.965237\n",
      "Epoch 35, Iter 1400, D Loss: 0.0902, G Loss: -1.4907, L1: 1.0578, L2: 1.9652\n",
      "Epoch 35 - D Loss: 0.0785, G Loss: -1.5488\n",
      "Epoch 36 - D Loss: 0.0460, G Loss: -1.7483\n",
      "Mean L1 Loss: 0.911200, Mean L2 Loss: 1.765633\n",
      "Epoch 37, Iter 1450, D Loss: 0.0191, G Loss: -2.6243, L1: 0.9112, L2: 1.7656\n",
      "Epoch 37 - D Loss: 0.0213, G Loss: -1.8204\n",
      "Mean L1 Loss: 0.791502, Mean L2 Loss: 1.362926\n",
      "Epoch 38, Iter 1500, D Loss: 0.0047, G Loss: -2.1580, L1: 0.7915, L2: 1.3629\n",
      "Saved best model with L2: 1.3629\n",
      "Epoch 38 - D Loss: 0.0077, G Loss: -1.8441\n",
      "Mean L1 Loss: 0.897748, Mean L2 Loss: 1.510892\n",
      "Epoch 39, Iter 1550, D Loss: -0.0058, G Loss: -2.0178, L1: 0.8977, L2: 1.5109\n",
      "Epoch 39 - D Loss: -0.0038, G Loss: -1.9807\n",
      "Epoch 40 - D Loss: 0.0002, G Loss: -1.9661\n",
      "Mean L1 Loss: 0.824778, Mean L2 Loss: 1.276622\n",
      "Epoch 41, Iter 1600, D Loss: 0.0028, G Loss: -2.3532, L1: 0.8248, L2: 1.2766\n",
      "Saved best model with L2: 1.2766\n",
      "Epoch 41 - D Loss: -0.0042, G Loss: -2.1225\n",
      "Mean L1 Loss: 0.783659, Mean L2 Loss: 1.215770\n",
      "Epoch 42, Iter 1650, D Loss: -0.0110, G Loss: -2.1839, L1: 0.7837, L2: 1.2158\n",
      "Saved best model with L2: 1.2158\n",
      "Epoch 42 - D Loss: -0.0030, G Loss: -2.2214\n",
      "Mean L1 Loss: 0.797649, Mean L2 Loss: 1.191498\n",
      "Epoch 43, Iter 1700, D Loss: 0.0145, G Loss: -2.0062, L1: 0.7976, L2: 1.1915\n",
      "Saved best model with L2: 1.1915\n",
      "Epoch 43 - D Loss: 0.0089, G Loss: -2.1768\n",
      "Mean L1 Loss: 0.752102, Mean L2 Loss: 1.163608\n",
      "Epoch 44, Iter 1750, D Loss: 0.0124, G Loss: -2.3679, L1: 0.7521, L2: 1.1636\n",
      "Saved best model with L2: 1.1636\n",
      "Epoch 44 - D Loss: 0.0094, G Loss: -2.3328\n",
      "Epoch 45 - D Loss: 0.0107, G Loss: -2.3102\n",
      "Mean L1 Loss: 0.846320, Mean L2 Loss: 1.211493\n",
      "Epoch 46, Iter 1800, D Loss: 0.0198, G Loss: -2.1195, L1: 0.8463, L2: 1.2115\n",
      "Epoch 46 - D Loss: 0.0008, G Loss: -2.4991\n",
      "Mean L1 Loss: 0.730857, Mean L2 Loss: 1.128708\n",
      "Epoch 47, Iter 1850, D Loss: 0.0088, G Loss: -2.7548, L1: 0.7309, L2: 1.1287\n",
      "Saved best model with L2: 1.1287\n",
      "Epoch 47 - D Loss: 0.0150, G Loss: -2.5106\n",
      "Mean L1 Loss: 1.050347, Mean L2 Loss: 1.628199\n",
      "Epoch 48, Iter 1900, D Loss: 0.0256, G Loss: -2.4431, L1: 1.0503, L2: 1.6282\n",
      "Epoch 48 - D Loss: 0.0211, G Loss: -2.5546\n",
      "Mean L1 Loss: 0.741628, Mean L2 Loss: 1.164705\n",
      "Epoch 49, Iter 1950, D Loss: 0.0186, G Loss: -2.6318, L1: 0.7416, L2: 1.1647\n",
      "Epoch 49 - D Loss: 0.0186, G Loss: -2.6318\n",
      "Epoch 50 - D Loss: -0.0018, G Loss: -2.5509\n",
      "Mean L1 Loss: 0.777453, Mean L2 Loss: 1.063834\n",
      "Epoch 51, Iter 2000, D Loss: 0.0049, G Loss: -2.5279, L1: 0.7775, L2: 1.0638\n",
      "Saved best model with L2: 1.0638\n",
      "Epoch 51 - D Loss: 0.0062, G Loss: -2.5772\n",
      "Mean L1 Loss: 0.748883, Mean L2 Loss: 1.033445\n",
      "Epoch 52, Iter 2050, D Loss: 0.0033, G Loss: -2.8706, L1: 0.7489, L2: 1.0334\n",
      "Saved best model with L2: 1.0334\n",
      "Epoch 52 - D Loss: 0.0056, G Loss: -2.8173\n",
      "Mean L1 Loss: 0.716485, Mean L2 Loss: 0.992308\n",
      "Epoch 53, Iter 2100, D Loss: 0.0219, G Loss: -2.7777, L1: 0.7165, L2: 0.9923\n",
      "Saved best model with L2: 0.9923\n",
      "Epoch 53 - D Loss: 0.0214, G Loss: -2.8723\n",
      "Epoch 54 - D Loss: 0.0324, G Loss: -2.8365\n",
      "Mean L1 Loss: 0.802424, Mean L2 Loss: 1.047391\n",
      "Epoch 55, Iter 2150, D Loss: -0.0041, G Loss: -2.7281, L1: 0.8024, L2: 1.0474\n",
      "Epoch 55 - D Loss: 0.0125, G Loss: -2.8130\n",
      "Mean L1 Loss: 0.730117, Mean L2 Loss: 0.991184\n",
      "Epoch 56, Iter 2200, D Loss: 0.0199, G Loss: -2.7190, L1: 0.7301, L2: 0.9912\n",
      "Saved best model with L2: 0.9912\n",
      "Epoch 56 - D Loss: 0.0111, G Loss: -2.8971\n",
      "Mean L1 Loss: 0.773769, Mean L2 Loss: 1.003950\n",
      "Epoch 57, Iter 2250, D Loss: 0.0076, G Loss: -2.9382, L1: 0.7738, L2: 1.0040\n",
      "Epoch 57 - D Loss: 0.0059, G Loss: -2.8884\n",
      "Mean L1 Loss: 0.731039, Mean L2 Loss: 0.950198\n",
      "Epoch 58, Iter 2300, D Loss: -0.0018, G Loss: -3.0231, L1: 0.7310, L2: 0.9502\n",
      "Saved best model with L2: 0.9502\n",
      "Epoch 58 - D Loss: -0.0021, G Loss: -3.0241\n",
      "Epoch 59 - D Loss: 0.0023, G Loss: -3.0471\n",
      "Mean L1 Loss: 0.809125, Mean L2 Loss: 1.011424\n",
      "Epoch 60, Iter 2350, D Loss: -0.0021, G Loss: -3.0261, L1: 0.8091, L2: 1.0114\n",
      "Epoch 60 - D Loss: 0.0010, G Loss: -3.0824\n",
      "Mean L1 Loss: 0.779206, Mean L2 Loss: 0.980297\n",
      "Epoch 61, Iter 2400, D Loss: -0.0019, G Loss: -3.1902, L1: 0.7792, L2: 0.9803\n",
      "Epoch 61 - D Loss: -0.0016, G Loss: -3.2268\n",
      "Mean L1 Loss: 0.798168, Mean L2 Loss: 1.004988\n",
      "Epoch 62, Iter 2450, D Loss: 0.0004, G Loss: -3.2439, L1: 0.7982, L2: 1.0050\n",
      "Epoch 62 - D Loss: -0.0002, G Loss: -3.2393\n",
      "Epoch 63 - D Loss: 0.0047, G Loss: -3.4055\n",
      "Mean L1 Loss: 0.738327, Mean L2 Loss: 0.923322\n",
      "Epoch 64, Iter 2500, D Loss: 0.0059, G Loss: -3.2560, L1: 0.7383, L2: 0.9233\n",
      "Saved best model with L2: 0.9233\n",
      "Epoch 64 - D Loss: 0.0100, G Loss: -3.2563\n",
      "Mean L1 Loss: 0.683658, Mean L2 Loss: 0.975033\n",
      "Epoch 65, Iter 2550, D Loss: 0.0028, G Loss: -3.4515, L1: 0.6837, L2: 0.9750\n",
      "Epoch 65 - D Loss: 0.0041, G Loss: -3.3834\n",
      "Mean L1 Loss: 0.796951, Mean L2 Loss: 0.984130\n",
      "Epoch 66, Iter 2600, D Loss: 0.0038, G Loss: -3.3149, L1: 0.7970, L2: 0.9841\n",
      "Epoch 66 - D Loss: 0.0036, G Loss: -3.3857\n",
      "Mean L1 Loss: 0.719178, Mean L2 Loss: 0.881896\n",
      "Epoch 67, Iter 2650, D Loss: 0.0009, G Loss: -3.4141, L1: 0.7192, L2: 0.8819\n",
      "Saved best model with L2: 0.8819\n",
      "Epoch 67 - D Loss: 0.0005, G Loss: -3.4191\n",
      "Epoch 68 - D Loss: 0.0013, G Loss: -3.4068\n",
      "Mean L1 Loss: 0.746526, Mean L2 Loss: 0.911110\n",
      "Epoch 69, Iter 2700, D Loss: -0.0022, G Loss: -3.3486, L1: 0.7465, L2: 0.9111\n",
      "Epoch 69 - D Loss: -0.0008, G Loss: -3.4579\n",
      "Mean L1 Loss: 0.738290, Mean L2 Loss: 0.915602\n",
      "Epoch 70, Iter 2750, D Loss: 0.0023, G Loss: -3.4113, L1: 0.7383, L2: 0.9156\n",
      "Epoch 70 - D Loss: -0.0000, G Loss: -3.4475\n",
      "Mean L1 Loss: 0.687234, Mean L2 Loss: 0.890144\n",
      "Epoch 71, Iter 2800, D Loss: 0.0008, G Loss: -3.5076, L1: 0.6872, L2: 0.8901\n",
      "Epoch 71 - D Loss: 0.0000, G Loss: -3.5112\n",
      "Epoch 72 - D Loss: 0.0042, G Loss: -3.5451\n",
      "Mean L1 Loss: 0.885648, Mean L2 Loss: 1.120922\n",
      "Epoch 73, Iter 2850, D Loss: 0.0243, G Loss: -3.1795, L1: 0.8856, L2: 1.1209\n",
      "Epoch 73 - D Loss: 0.0072, G Loss: -3.5375\n",
      "Mean L1 Loss: 0.805421, Mean L2 Loss: 0.970704\n",
      "Epoch 74, Iter 2900, D Loss: 0.0076, G Loss: -3.5785, L1: 0.8054, L2: 0.9707\n",
      "Epoch 74 - D Loss: 0.0066, G Loss: -3.6069\n",
      "Mean L1 Loss: 0.743373, Mean L2 Loss: 0.902865\n",
      "Epoch 75, Iter 2950, D Loss: 0.0013, G Loss: -3.5469, L1: 0.7434, L2: 0.9029\n",
      "Epoch 75 - D Loss: 0.0018, G Loss: -3.5847\n",
      "Mean L1 Loss: 0.718144, Mean L2 Loss: 0.925211\n",
      "Epoch 76, Iter 3000, D Loss: 0.0002, G Loss: -3.5716, L1: 0.7181, L2: 0.9252\n",
      "Epoch 76 - D Loss: 0.0004, G Loss: -3.5988\n",
      "Epoch 77 - D Loss: 0.0040, G Loss: -3.6382\n",
      "Mean L1 Loss: 0.767046, Mean L2 Loss: 0.915048\n",
      "Epoch 78, Iter 3050, D Loss: 0.0019, G Loss: -3.4629, L1: 0.7670, L2: 0.9150\n",
      "Epoch 78 - D Loss: 0.0099, G Loss: -3.6150\n",
      "Mean L1 Loss: 0.814944, Mean L2 Loss: 0.974643\n",
      "Epoch 79, Iter 3100, D Loss: 0.0079, G Loss: -3.6121, L1: 0.8149, L2: 0.9746\n",
      "Epoch 79 - D Loss: 0.0077, G Loss: -3.6738\n",
      "Mean L1 Loss: 0.803643, Mean L2 Loss: 0.952672\n",
      "Epoch 80, Iter 3150, D Loss: 0.0126, G Loss: -3.5433, L1: 0.8036, L2: 0.9527\n",
      "Epoch 80 - D Loss: 0.0141, G Loss: -3.6522\n",
      "Epoch 81 - D Loss: 0.0209, G Loss: -3.5326\n",
      "Mean L1 Loss: 0.851661, Mean L2 Loss: 1.027500\n",
      "Epoch 82, Iter 3200, D Loss: 0.0225, G Loss: -3.3164, L1: 0.8517, L2: 1.0275\n",
      "Epoch 82 - D Loss: 0.0079, G Loss: -3.6360\n",
      "Mean L1 Loss: 0.821050, Mean L2 Loss: 0.975764\n",
      "Epoch 83, Iter 3250, D Loss: 0.0073, G Loss: -3.4835, L1: 0.8211, L2: 0.9758\n",
      "Epoch 83 - D Loss: 0.0038, G Loss: -3.6085\n",
      "Mean L1 Loss: 0.774225, Mean L2 Loss: 0.901154\n",
      "Epoch 84, Iter 3300, D Loss: 0.0010, G Loss: -3.6084, L1: 0.7742, L2: 0.9012\n",
      "Epoch 84 - D Loss: 0.0018, G Loss: -3.6531\n",
      "Mean L1 Loss: 0.725994, Mean L2 Loss: 0.873847\n",
      "Epoch 85, Iter 3350, D Loss: 0.0025, G Loss: -3.6159, L1: 0.7260, L2: 0.8738\n",
      "Saved best model with L2: 0.8738\n",
      "Epoch 85 - D Loss: 0.0025, G Loss: -3.6277\n",
      "Epoch 86 - D Loss: 0.0058, G Loss: -3.6383\n",
      "Mean L1 Loss: 0.667748, Mean L2 Loss: 0.963179\n",
      "Epoch 87, Iter 3400, D Loss: 0.0037, G Loss: -3.8118, L1: 0.6677, L2: 0.9632\n",
      "Epoch 87 - D Loss: 0.0063, G Loss: -3.6225\n",
      "Mean L1 Loss: 0.736353, Mean L2 Loss: 1.235652\n",
      "Epoch 88, Iter 3450, D Loss: 0.0179, G Loss: -3.6478, L1: 0.7364, L2: 1.2357\n",
      "Epoch 88 - D Loss: 0.0169, G Loss: -3.6350\n",
      "Mean L1 Loss: 0.679685, Mean L2 Loss: 1.016801\n",
      "Epoch 89, Iter 3500, D Loss: 0.0148, G Loss: -3.6611, L1: 0.6797, L2: 1.0168\n",
      "Epoch 89 - D Loss: 0.0160, G Loss: -3.5872\n",
      "Epoch 90 - D Loss: 0.0078, G Loss: -3.6132\n",
      "Mean L1 Loss: 0.722411, Mean L2 Loss: 0.863571\n",
      "Epoch 91, Iter 3550, D Loss: -0.0070, G Loss: -3.6690, L1: 0.7224, L2: 0.8636\n",
      "Saved best model with L2: 0.8636\n",
      "Epoch 91 - D Loss: -0.0004, G Loss: -3.6014\n",
      "Mean L1 Loss: 0.698140, Mean L2 Loss: 0.841209\n",
      "Epoch 92, Iter 3600, D Loss: -0.0007, G Loss: -3.7472, L1: 0.6981, L2: 0.8412\n",
      "Saved best model with L2: 0.8412\n",
      "Epoch 92 - D Loss: 0.0003, G Loss: -3.5930\n",
      "Mean L1 Loss: 0.804473, Mean L2 Loss: 0.944842\n",
      "Epoch 93, Iter 3650, D Loss: 0.0054, G Loss: -3.6202, L1: 0.8045, L2: 0.9448\n",
      "Epoch 93 - D Loss: 0.0049, G Loss: -3.6958\n",
      "Mean L1 Loss: 0.799441, Mean L2 Loss: 0.932874\n",
      "Epoch 94, Iter 3700, D Loss: 0.0048, G Loss: -3.6279, L1: 0.7994, L2: 0.9329\n",
      "Epoch 94 - D Loss: 0.0046, G Loss: -3.6307\n",
      "Epoch 95 - D Loss: 0.0056, G Loss: -3.6803\n",
      "Mean L1 Loss: 0.723704, Mean L2 Loss: 0.851018\n",
      "Epoch 96, Iter 3750, D Loss: 0.0187, G Loss: -3.5044, L1: 0.7237, L2: 0.8510\n",
      "Epoch 96 - D Loss: 0.0116, G Loss: -3.7294\n",
      "Mean L1 Loss: 0.674379, Mean L2 Loss: 0.891402\n",
      "Epoch 97, Iter 3800, D Loss: 0.0031, G Loss: -3.5954, L1: 0.6744, L2: 0.8914\n",
      "Epoch 97 - D Loss: 0.0023, G Loss: -3.6550\n",
      "Mean L1 Loss: 0.666479, Mean L2 Loss: 0.962860\n",
      "Epoch 98, Iter 3850, D Loss: 0.0077, G Loss: -3.7133, L1: 0.6665, L2: 0.9629\n",
      "Epoch 98 - D Loss: 0.0064, G Loss: -3.6905\n",
      "Mean L1 Loss: 0.666814, Mean L2 Loss: 0.888201\n",
      "Epoch 99, Iter 3900, D Loss: 0.0120, G Loss: -3.6884, L1: 0.6668, L2: 0.8882\n",
      "Epoch 99 - D Loss: 0.0120, G Loss: -3.6884\n",
      "Epoch 100 - D Loss: 0.0050, G Loss: -3.6864\n",
      "Mean L1 Loss: 0.873285, Mean L2 Loss: 1.036168\n",
      "Epoch 101, Iter 3950, D Loss: 0.0075, G Loss: -3.6342, L1: 0.8733, L2: 1.0362\n",
      "Epoch 101 - D Loss: 0.0103, G Loss: -3.6681\n",
      "Mean L1 Loss: 0.743038, Mean L2 Loss: 0.869431\n",
      "Epoch 102, Iter 4000, D Loss: 0.0100, G Loss: -3.6638, L1: 0.7430, L2: 0.8694\n",
      "Epoch 102 - D Loss: 0.0067, G Loss: -3.6732\n",
      "Mean L1 Loss: 0.665820, Mean L2 Loss: 0.876727\n",
      "Epoch 103, Iter 4050, D Loss: 0.0056, G Loss: -3.6102, L1: 0.6658, L2: 0.8767\n",
      "Epoch 103 - D Loss: 0.0052, G Loss: -3.6167\n",
      "Epoch 104 - D Loss: -0.0013, G Loss: -3.6763\n",
      "Mean L1 Loss: 0.768729, Mean L2 Loss: 0.880792\n",
      "Epoch 105, Iter 4100, D Loss: 0.0020, G Loss: -3.5319, L1: 0.7687, L2: 0.8808\n",
      "Epoch 105 - D Loss: 0.0008, G Loss: -3.6672\n",
      "Mean L1 Loss: 0.736635, Mean L2 Loss: 0.857558\n",
      "Epoch 106, Iter 4150, D Loss: 0.0018, G Loss: -3.5992, L1: 0.7366, L2: 0.8576\n",
      "Epoch 106 - D Loss: 0.0019, G Loss: -3.6447\n",
      "Mean L1 Loss: 0.655361, Mean L2 Loss: 0.870747\n",
      "Epoch 107, Iter 4200, D Loss: -0.0010, G Loss: -3.6662, L1: 0.6554, L2: 0.8707\n",
      "Epoch 107 - D Loss: 0.0011, G Loss: -3.6546\n",
      "Mean L1 Loss: 0.660075, Mean L2 Loss: 0.814853\n",
      "Epoch 108, Iter 4250, D Loss: 0.0060, G Loss: -3.6633, L1: 0.6601, L2: 0.8149\n",
      "Saved best model with L2: 0.8149\n",
      "Epoch 108 - D Loss: 0.0058, G Loss: -3.6612\n",
      "Epoch 109 - D Loss: 0.0089, G Loss: -3.6464\n",
      "Mean L1 Loss: 0.986269, Mean L2 Loss: 1.252784\n",
      "Epoch 110, Iter 4300, D Loss: 0.0153, G Loss: -3.4310, L1: 0.9863, L2: 1.2528\n",
      "Epoch 110 - D Loss: 0.0153, G Loss: -3.5334\n",
      "Mean L1 Loss: 0.856314, Mean L2 Loss: 1.011732\n",
      "Epoch 111, Iter 4350, D Loss: 0.0136, G Loss: -3.6158, L1: 0.8563, L2: 1.0117\n",
      "Epoch 111 - D Loss: 0.0077, G Loss: -3.5944\n",
      "Mean L1 Loss: 0.667834, Mean L2 Loss: 0.878702\n",
      "Epoch 112, Iter 4400, D Loss: 0.0026, G Loss: -3.5792, L1: 0.6678, L2: 0.8787\n",
      "Epoch 112 - D Loss: 0.0025, G Loss: -3.6294\n",
      "Epoch 113 - D Loss: 0.0021, G Loss: -3.5592\n",
      "Mean L1 Loss: 0.795772, Mean L2 Loss: 0.902225\n",
      "Epoch 114, Iter 4450, D Loss: 0.0039, G Loss: -3.5762, L1: 0.7958, L2: 0.9022\n",
      "Epoch 114 - D Loss: 0.0030, G Loss: -3.6741\n",
      "Mean L1 Loss: 0.767071, Mean L2 Loss: 0.872076\n",
      "Epoch 115, Iter 4500, D Loss: -0.0004, G Loss: -3.6919, L1: 0.7671, L2: 0.8721\n",
      "Epoch 115 - D Loss: 0.0009, G Loss: -3.6606\n",
      "Mean L1 Loss: 0.781877, Mean L2 Loss: 0.884436\n",
      "Epoch 116, Iter 4550, D Loss: 0.0022, G Loss: -3.5918, L1: 0.7819, L2: 0.8844\n",
      "Epoch 116 - D Loss: 0.0011, G Loss: -3.6417\n",
      "Mean L1 Loss: 0.702136, Mean L2 Loss: 0.836844\n",
      "Epoch 117, Iter 4600, D Loss: -0.0008, G Loss: -3.6771, L1: 0.7021, L2: 0.8368\n",
      "Epoch 117 - D Loss: -0.0008, G Loss: -3.6790\n",
      "Epoch 118 - D Loss: 0.0011, G Loss: -3.6379\n",
      "Mean L1 Loss: 0.686269, Mean L2 Loss: 0.850075\n",
      "Epoch 119, Iter 4650, D Loss: 0.0106, G Loss: -3.5472, L1: 0.6863, L2: 0.8501\n",
      "Epoch 119 - D Loss: 0.0057, G Loss: -3.6920\n",
      "Mean L1 Loss: 0.685289, Mean L2 Loss: 1.033200\n",
      "Epoch 120, Iter 4700, D Loss: 0.0072, G Loss: -3.6194, L1: 0.6853, L2: 1.0332\n",
      "Epoch 120 - D Loss: 0.0059, G Loss: -3.6476\n",
      "Mean L1 Loss: 0.674943, Mean L2 Loss: 0.808428\n",
      "Epoch 121, Iter 4750, D Loss: 0.0003, G Loss: -3.6881, L1: 0.6749, L2: 0.8084\n",
      "Saved best model with L2: 0.8084\n",
      "Epoch 121 - D Loss: 0.0005, G Loss: -3.6662\n",
      "Epoch 122 - D Loss: 0.0001, G Loss: -3.6531\n",
      "Mean L1 Loss: 0.664921, Mean L2 Loss: 0.818068\n",
      "Epoch 123, Iter 4800, D Loss: 0.0008, G Loss: -3.7233, L1: 0.6649, L2: 0.8181\n",
      "Epoch 123 - D Loss: 0.0003, G Loss: -3.6268\n",
      "Mean L1 Loss: 0.716562, Mean L2 Loss: 0.834858\n",
      "Epoch 124, Iter 4850, D Loss: 0.0021, G Loss: -3.7372, L1: 0.7166, L2: 0.8349\n",
      "Epoch 124 - D Loss: -0.0004, G Loss: -3.6839\n",
      "Mean L1 Loss: 0.683847, Mean L2 Loss: 0.825942\n",
      "Epoch 125, Iter 4900, D Loss: -0.0002, G Loss: -3.6750, L1: 0.6838, L2: 0.8259\n",
      "Epoch 125 - D Loss: 0.0002, G Loss: -3.6984\n",
      "Mean L1 Loss: 0.717112, Mean L2 Loss: 0.829552\n",
      "Epoch 126, Iter 4950, D Loss: 0.0007, G Loss: -3.6701, L1: 0.7171, L2: 0.8296\n",
      "Epoch 126 - D Loss: 0.0011, G Loss: -3.6657\n",
      "Epoch 127 - D Loss: 0.0021, G Loss: -3.6977\n",
      "Mean L1 Loss: 0.748359, Mean L2 Loss: 0.853710\n",
      "Epoch 128, Iter 5000, D Loss: -0.0009, G Loss: -3.6558, L1: 0.7484, L2: 0.8537\n",
      "Epoch 128 - D Loss: 0.0004, G Loss: -3.6361\n",
      "Mean L1 Loss: 0.670928, Mean L2 Loss: 0.840408\n",
      "Epoch 129, Iter 5050, D Loss: -0.0000, G Loss: -3.5963, L1: 0.6709, L2: 0.8404\n",
      "Epoch 129 - D Loss: 0.0009, G Loss: -3.6135\n",
      "Mean L1 Loss: 0.755986, Mean L2 Loss: 0.860462\n",
      "Epoch 130, Iter 5100, D Loss: 0.0033, G Loss: -3.6312, L1: 0.7560, L2: 0.8605\n",
      "Epoch 130 - D Loss: 0.0019, G Loss: -3.6333\n",
      "Epoch 131 - D Loss: 0.0022, G Loss: -3.5615\n",
      "Mean L1 Loss: 0.698537, Mean L2 Loss: 0.861723\n",
      "Epoch 132, Iter 5150, D Loss: 0.0025, G Loss: -3.5599, L1: 0.6985, L2: 0.8617\n",
      "Epoch 132 - D Loss: -0.0010, G Loss: -3.5734\n",
      "Mean L1 Loss: 0.675886, Mean L2 Loss: 0.851739\n",
      "Epoch 133, Iter 5200, D Loss: 0.0018, G Loss: -3.5267, L1: 0.6759, L2: 0.8517\n",
      "Epoch 133 - D Loss: 0.0037, G Loss: -3.6615\n",
      "Mean L1 Loss: 0.728417, Mean L2 Loss: 0.824153\n",
      "Epoch 134, Iter 5250, D Loss: 0.0029, G Loss: -3.5670, L1: 0.7284, L2: 0.8242\n",
      "Epoch 134 - D Loss: 0.0009, G Loss: -3.5659\n",
      "Mean L1 Loss: 0.706444, Mean L2 Loss: 0.911903\n",
      "Epoch 135, Iter 5300, D Loss: 0.0012, G Loss: -3.5935, L1: 0.7064, L2: 0.9119\n",
      "Epoch 135 - D Loss: 0.0034, G Loss: -3.6240\n",
      "Epoch 136 - D Loss: 0.0118, G Loss: -3.5598\n",
      "Mean L1 Loss: 0.701978, Mean L2 Loss: 0.805015\n",
      "Epoch 137, Iter 5350, D Loss: 0.0041, G Loss: -3.7193, L1: 0.7020, L2: 0.8050\n",
      "Saved best model with L2: 0.8050\n",
      "Epoch 137 - D Loss: 0.0025, G Loss: -3.6400\n",
      "Mean L1 Loss: 0.740374, Mean L2 Loss: 0.844689\n",
      "Epoch 138, Iter 5400, D Loss: 0.0001, G Loss: -3.4936, L1: 0.7404, L2: 0.8447\n",
      "Epoch 138 - D Loss: -0.0009, G Loss: -3.5665\n",
      "Mean L1 Loss: 0.685513, Mean L2 Loss: 0.855768\n",
      "Epoch 139, Iter 5450, D Loss: 0.0024, G Loss: -3.5703, L1: 0.6855, L2: 0.8558\n",
      "Epoch 139 - D Loss: 0.0028, G Loss: -3.6193\n",
      "Epoch 140 - D Loss: 0.0007, G Loss: -3.6729\n",
      "Mean L1 Loss: 0.702515, Mean L2 Loss: 0.815046\n",
      "Epoch 141, Iter 5500, D Loss: 0.0020, G Loss: -3.6909, L1: 0.7025, L2: 0.8150\n",
      "Epoch 141 - D Loss: 0.0024, G Loss: -3.5661\n",
      "Mean L1 Loss: 0.715159, Mean L2 Loss: 0.826663\n",
      "Epoch 142, Iter 5550, D Loss: -0.0060, G Loss: -3.5033, L1: 0.7152, L2: 0.8267\n",
      "Epoch 142 - D Loss: 0.0031, G Loss: -3.4984\n",
      "Mean L1 Loss: 0.689188, Mean L2 Loss: 0.829015\n",
      "Epoch 143, Iter 5600, D Loss: 0.0127, G Loss: -3.5617, L1: 0.6892, L2: 0.8290\n",
      "Epoch 143 - D Loss: 0.0083, G Loss: -3.5534\n",
      "Mean L1 Loss: 0.714450, Mean L2 Loss: 0.837289\n",
      "Epoch 144, Iter 5650, D Loss: 0.0037, G Loss: -3.4779, L1: 0.7144, L2: 0.8373\n",
      "Epoch 144 - D Loss: 0.0041, G Loss: -3.4923\n",
      "Epoch 145 - D Loss: -0.0009, G Loss: -3.4227\n",
      "Mean L1 Loss: 0.836462, Mean L2 Loss: 0.957226\n",
      "Epoch 146, Iter 5700, D Loss: 0.0036, G Loss: -3.3552, L1: 0.8365, L2: 0.9572\n",
      "Epoch 146 - D Loss: 0.0070, G Loss: -3.4852\n",
      "Mean L1 Loss: 0.730567, Mean L2 Loss: 0.814251\n",
      "Epoch 147, Iter 5750, D Loss: 0.0029, G Loss: -3.5731, L1: 0.7306, L2: 0.8143\n",
      "Epoch 147 - D Loss: 0.0036, G Loss: -3.5999\n",
      "Mean L1 Loss: 0.660004, Mean L2 Loss: 0.872269\n",
      "Epoch 148, Iter 5800, D Loss: 0.0044, G Loss: -3.4942, L1: 0.6600, L2: 0.8723\n",
      "Epoch 148 - D Loss: 0.0034, G Loss: -3.4780\n",
      "Mean L1 Loss: 0.699162, Mean L2 Loss: 0.818603\n",
      "Epoch 149, Iter 5850, D Loss: 0.0010, G Loss: -3.5608, L1: 0.6992, L2: 0.8186\n",
      "Epoch 149 - D Loss: 0.0010, G Loss: -3.5608\n",
      "Epoch 150 - D Loss: 0.0063, G Loss: -3.5775\n",
      "Mean L1 Loss: 0.879450, Mean L2 Loss: 1.037294\n",
      "Epoch 151, Iter 5900, D Loss: 0.0057, G Loss: -3.5764, L1: 0.8795, L2: 1.0373\n",
      "Epoch 151 - D Loss: 0.0060, G Loss: -3.4889\n",
      "Mean L1 Loss: 0.681721, Mean L2 Loss: 0.839266\n",
      "Epoch 152, Iter 5950, D Loss: 0.0035, G Loss: -3.4556, L1: 0.6817, L2: 0.8393\n",
      "Epoch 152 - D Loss: 0.0022, G Loss: -3.4599\n",
      "Mean L1 Loss: 0.679219, Mean L2 Loss: 0.818802\n",
      "Epoch 153, Iter 6000, D Loss: 0.0034, G Loss: -3.4968, L1: 0.6792, L2: 0.8188\n",
      "Epoch 153 - D Loss: 0.0028, G Loss: -3.5086\n",
      "Epoch 154 - D Loss: 0.0006, G Loss: -3.5193\n",
      "Mean L1 Loss: 0.799630, Mean L2 Loss: 0.906491\n",
      "Epoch 155, Iter 6050, D Loss: -0.0001, G Loss: -3.4413, L1: 0.7996, L2: 0.9065\n",
      "Epoch 155 - D Loss: 0.0018, G Loss: -3.5537\n",
      "Mean L1 Loss: 0.713630, Mean L2 Loss: 0.828674\n",
      "Epoch 156, Iter 6100, D Loss: 0.0014, G Loss: -3.4637, L1: 0.7136, L2: 0.8287\n",
      "Epoch 156 - D Loss: 0.0009, G Loss: -3.4863\n",
      "Mean L1 Loss: 0.722898, Mean L2 Loss: 0.815745\n",
      "Epoch 157, Iter 6150, D Loss: 0.0004, G Loss: -3.4865, L1: 0.7229, L2: 0.8157\n",
      "Epoch 157 - D Loss: 0.0008, G Loss: -3.4888\n",
      "Mean L1 Loss: 0.684769, Mean L2 Loss: 0.828641\n",
      "Epoch 158, Iter 6200, D Loss: -0.0004, G Loss: -3.5734, L1: 0.6848, L2: 0.8286\n",
      "Epoch 158 - D Loss: -0.0003, G Loss: -3.5757\n",
      "Epoch 159 - D Loss: -0.0006, G Loss: -3.5443\n",
      "Mean L1 Loss: 0.689218, Mean L2 Loss: 0.818384\n",
      "Epoch 160, Iter 6250, D Loss: -0.0019, G Loss: -3.6019, L1: 0.6892, L2: 0.8184\n",
      "Epoch 160 - D Loss: -0.0007, G Loss: -3.5962\n",
      "Mean L1 Loss: 0.732899, Mean L2 Loss: 0.821825\n",
      "Epoch 161, Iter 6300, D Loss: -0.0015, G Loss: -3.5909, L1: 0.7329, L2: 0.8218\n",
      "Epoch 161 - D Loss: 0.0001, G Loss: -3.6077\n",
      "Mean L1 Loss: 0.759776, Mean L2 Loss: 0.860780\n",
      "Epoch 162, Iter 6350, D Loss: 0.0022, G Loss: -3.6297, L1: 0.7598, L2: 0.8608\n",
      "Epoch 162 - D Loss: 0.0019, G Loss: -3.6440\n",
      "Epoch 163 - D Loss: 0.0006, G Loss: -3.6055\n",
      "Mean L1 Loss: 0.735908, Mean L2 Loss: 0.829003\n",
      "Epoch 164, Iter 6400, D Loss: -0.0006, G Loss: -3.5088, L1: 0.7359, L2: 0.8290\n",
      "Epoch 164 - D Loss: 0.0016, G Loss: -3.6912\n",
      "Mean L1 Loss: 0.675264, Mean L2 Loss: 0.947157\n",
      "Epoch 165, Iter 6450, D Loss: 0.0071, G Loss: -3.6419, L1: 0.6753, L2: 0.9472\n",
      "Epoch 165 - D Loss: 0.0057, G Loss: -3.6477\n",
      "Mean L1 Loss: 0.861224, Mean L2 Loss: 0.996202\n",
      "Epoch 166, Iter 6500, D Loss: 0.0031, G Loss: -3.6478, L1: 0.8612, L2: 0.9962\n",
      "Epoch 166 - D Loss: 0.0036, G Loss: -3.6759\n",
      "Mean L1 Loss: 0.674991, Mean L2 Loss: 0.944569\n",
      "Epoch 167, Iter 6550, D Loss: 0.0015, G Loss: -3.6813, L1: 0.6750, L2: 0.9446\n",
      "Epoch 167 - D Loss: 0.0020, G Loss: -3.6850\n",
      "Epoch 168 - D Loss: 0.0045, G Loss: -3.5982\n",
      "Mean L1 Loss: 0.688675, Mean L2 Loss: 0.812030\n",
      "Epoch 169, Iter 6600, D Loss: 0.0048, G Loss: -3.5341, L1: 0.6887, L2: 0.8120\n",
      "Epoch 169 - D Loss: 0.0119, G Loss: -3.5908\n",
      "Mean L1 Loss: 0.901497, Mean L2 Loss: 1.071862\n",
      "Epoch 170, Iter 6650, D Loss: 0.0074, G Loss: -3.5335, L1: 0.9015, L2: 1.0719\n",
      "Epoch 170 - D Loss: 0.0067, G Loss: -3.5343\n",
      "Mean L1 Loss: 0.660865, Mean L2 Loss: 0.875383\n",
      "Epoch 171, Iter 6700, D Loss: 0.0053, G Loss: -3.6007, L1: 0.6609, L2: 0.8754\n",
      "Epoch 171 - D Loss: 0.0044, G Loss: -3.5963\n",
      "Epoch 172 - D Loss: 0.0012, G Loss: -3.5730\n",
      "Mean L1 Loss: 0.673151, Mean L2 Loss: 0.837862\n",
      "Epoch 173, Iter 6750, D Loss: -0.0000, G Loss: -3.7983, L1: 0.6732, L2: 0.8379\n",
      "Epoch 173 - D Loss: 0.0033, G Loss: -3.5730\n",
      "Mean L1 Loss: 0.722767, Mean L2 Loss: 0.831552\n",
      "Epoch 174, Iter 6800, D Loss: -0.0007, G Loss: -3.6605, L1: 0.7228, L2: 0.8316\n",
      "Epoch 174 - D Loss: 0.0004, G Loss: -3.5811\n",
      "Mean L1 Loss: 0.666393, Mean L2 Loss: 0.827921\n",
      "Epoch 175, Iter 6850, D Loss: 0.0012, G Loss: -3.6040, L1: 0.6664, L2: 0.8279\n",
      "Epoch 175 - D Loss: 0.0016, G Loss: -3.6010\n",
      "Mean L1 Loss: 0.803613, Mean L2 Loss: 0.906385\n",
      "Epoch 176, Iter 6900, D Loss: 0.0017, G Loss: -3.5816, L1: 0.8036, L2: 0.9064\n",
      "Epoch 176 - D Loss: 0.0017, G Loss: -3.5739\n",
      "Epoch 177 - D Loss: -0.0006, G Loss: -3.6017\n",
      "Mean L1 Loss: 0.713079, Mean L2 Loss: 0.871383\n",
      "Epoch 178, Iter 6950, D Loss: -0.0006, G Loss: -3.6187, L1: 0.7131, L2: 0.8714\n",
      "Epoch 178 - D Loss: 0.0008, G Loss: -3.5955\n",
      "Mean L1 Loss: 0.756763, Mean L2 Loss: 0.857225\n",
      "Epoch 179, Iter 7000, D Loss: 0.0001, G Loss: -3.5557, L1: 0.7568, L2: 0.8572\n",
      "Epoch 179 - D Loss: 0.0017, G Loss: -3.6060\n",
      "Mean L1 Loss: 0.730439, Mean L2 Loss: 0.835170\n",
      "Epoch 180, Iter 7050, D Loss: -0.0008, G Loss: -3.5496, L1: 0.7304, L2: 0.8352\n",
      "Epoch 180 - D Loss: -0.0006, G Loss: -3.5582\n",
      "Epoch 181 - D Loss: 0.0024, G Loss: -3.6546\n",
      "Mean L1 Loss: 0.753683, Mean L2 Loss: 0.860984\n",
      "Epoch 182, Iter 7100, D Loss: -0.0024, G Loss: -3.5456, L1: 0.7537, L2: 0.8610\n",
      "Epoch 182 - D Loss: -0.0003, G Loss: -3.5737\n",
      "Mean L1 Loss: 0.671467, Mean L2 Loss: 0.816169\n",
      "Epoch 183, Iter 7150, D Loss: -0.0016, G Loss: -3.5769, L1: 0.6715, L2: 0.8162\n",
      "Epoch 183 - D Loss: 0.0006, G Loss: -3.6330\n",
      "Mean L1 Loss: 0.760208, Mean L2 Loss: 0.842587\n",
      "Epoch 184, Iter 7200, D Loss: 0.0021, G Loss: -3.5717, L1: 0.7602, L2: 0.8426\n",
      "Epoch 184 - D Loss: 0.0015, G Loss: -3.5878\n",
      "Mean L1 Loss: 0.718530, Mean L2 Loss: 0.814903\n",
      "Epoch 185, Iter 7250, D Loss: 0.0021, G Loss: -3.5574, L1: 0.7185, L2: 0.8149\n",
      "Epoch 185 - D Loss: 0.0018, G Loss: -3.5418\n",
      "Epoch 186 - D Loss: 0.0005, G Loss: -3.5932\n",
      "Mean L1 Loss: 0.694564, Mean L2 Loss: 0.830190\n",
      "Epoch 187, Iter 7300, D Loss: -0.0007, G Loss: -3.5901, L1: 0.6946, L2: 0.8302\n",
      "Epoch 187 - D Loss: 0.0022, G Loss: -3.5440\n",
      "Mean L1 Loss: 0.790173, Mean L2 Loss: 0.884123\n",
      "Epoch 188, Iter 7350, D Loss: 0.0030, G Loss: -3.6323, L1: 0.7902, L2: 0.8841\n",
      "Epoch 188 - D Loss: 0.0023, G Loss: -3.6581\n",
      "Mean L1 Loss: 0.752901, Mean L2 Loss: 0.846080\n",
      "Epoch 189, Iter 7400, D Loss: -0.0006, G Loss: -3.6199, L1: 0.7529, L2: 0.8461\n",
      "Epoch 189 - D Loss: -0.0005, G Loss: -3.6301\n",
      "Epoch 190 - D Loss: 0.0041, G Loss: -3.5631\n",
      "Mean L1 Loss: 0.725627, Mean L2 Loss: 0.824058\n",
      "Epoch 191, Iter 7450, D Loss: -0.0038, G Loss: -3.4938, L1: 0.7256, L2: 0.8241\n",
      "Epoch 191 - D Loss: 0.0037, G Loss: -3.5658\n",
      "Mean L1 Loss: 0.663124, Mean L2 Loss: 0.876341\n",
      "Epoch 192, Iter 7500, D Loss: 0.0011, G Loss: -3.6100, L1: 0.6631, L2: 0.8763\n",
      "Epoch 192 - D Loss: 0.0018, G Loss: -3.5785\n",
      "Mean L1 Loss: 0.733678, Mean L2 Loss: 0.839408\n",
      "Epoch 193, Iter 7550, D Loss: 0.0038, G Loss: -3.5430, L1: 0.7337, L2: 0.8394\n",
      "Epoch 193 - D Loss: 0.0048, G Loss: -3.5524\n",
      "Mean L1 Loss: 0.701853, Mean L2 Loss: 0.813427\n",
      "Epoch 194, Iter 7600, D Loss: 0.0035, G Loss: -3.5799, L1: 0.7019, L2: 0.8134\n",
      "Epoch 194 - D Loss: 0.0037, G Loss: -3.5546\n",
      "Epoch 195 - D Loss: 0.0023, G Loss: -3.5153\n",
      "Mean L1 Loss: 0.683362, Mean L2 Loss: 0.889770\n",
      "Epoch 196, Iter 7650, D Loss: 0.0029, G Loss: -3.6672, L1: 0.6834, L2: 0.8898\n",
      "Epoch 196 - D Loss: 0.0029, G Loss: -3.4965\n",
      "Mean L1 Loss: 0.690199, Mean L2 Loss: 0.835642\n",
      "Epoch 197, Iter 7700, D Loss: 0.0008, G Loss: -3.4976, L1: 0.6902, L2: 0.8356\n",
      "Epoch 197 - D Loss: 0.0008, G Loss: -3.4779\n",
      "Mean L1 Loss: 0.788339, Mean L2 Loss: 0.900740\n",
      "Epoch 198, Iter 7750, D Loss: 0.0023, G Loss: -3.5250, L1: 0.7883, L2: 0.9007\n",
      "Epoch 198 - D Loss: 0.0015, G Loss: -3.5028\n",
      "Mean L1 Loss: 0.682143, Mean L2 Loss: 0.809152\n",
      "Epoch 199, Iter 7800, D Loss: 0.0082, G Loss: -3.4642, L1: 0.6821, L2: 0.8092\n",
      "Epoch 199 - D Loss: 0.0082, G Loss: -3.4642\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trained_G, trained_D = train_WGR_fnn(D=D_net, G=G_net, D_solver=D_solver, G_solver=G_solver, \n",
    "                                     loader_train = loader_train, loader_val=loader_val,\n",
    "                                     noise_dim=args.noise_dim, Xdim=args.Xdim, Ydim=args.Ydim, batch_size=args.train_batch,\n",
    "                                     save_path='./', model_type=args.model, device='cpu', num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf00ecd-57df-4b58-841e-932233afcf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M2, Univariate, Ydim: 1, J_t_size: 50\n",
      "L1 Loss: tensor([0.7289])\n",
      "L2 Loss: tensor([0.8540])\n",
      "MSE Mean: tensor([0.0745])\n",
      "MSE SD: tensor([0.0662])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the L1 and L2 error, MSE of conditional mean and conditional standard deviation on the test data  \n",
    "test_G_mean_sd = L1L2_MSE_mean_sd_G(G = trained_G,  test_size = args.test, noise_dim=args.noise_dim, Xdim=args.Xdim,\n",
    "                                    batch_size=args.test_batch,  model_type=args.model, loader_dataset = loader_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53a40ac-b651-443f-8476-bbc9949dbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_5: 0.6683, Q_25: 0.0968, Q_50: 0.1975, Q_75: 0.1257, Q_95: 0.4440\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE of conditional quantiles at different levels.\n",
    "test_G_quantile = MSE_quantile_G_uniY(G = trained_G, loader_dataset = loader_test , noise_dim=args.noise_dim, Xdim=args.Xdim,\n",
    "                                      test_size = args.test,  batch_size=args.test_batch, model_type=args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c8597-3088-42a8-9fee-59bc17a4b5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
