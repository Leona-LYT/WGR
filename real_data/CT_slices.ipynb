{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4bdced5-838f-41c1-8093-c842569754d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the code for real data analysis with one dimensional response\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  #use to import the defined functions\n",
    "parent_dir = os.path.dirname(current_dir) \n",
    "sys.path.append(parent_dir)  \n",
    "\n",
    "\"\"\"\n",
    "incase the above code does not work, you can use the absolute path instead\n",
    "sys.path.append(r\".\\\")\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2941b9-cea5-4d5b-bde4-e7a32f63528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.basic_utils import setup_seed, get_dimension\n",
    "from models.generator import generator_fnn\n",
    "from models.discriminator import discriminator_fnn\n",
    "from utils.training_utils import train_WGR_fnn\n",
    "from utils.evaluation_utils import eva_G_UniY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1355642a-73b2-452a-ba9e-d8d725cf9d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Xdim=100, Ydim=1, noise_dim=55, noise_dist='gaussian', train=40000, val=3500, test=10000, train_batch=64, val_batch=100, test_batch=100, epochs=50)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if 'ipykernel_launcher.py' in sys.argv[0]:  #if not work in jupyter, you can delete this part\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] \n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Implementation of WGR for dataset with one dimensional response Y')\n",
    "\n",
    "parser.add_argument('--Xdim', default=100, type=int, help='dimensionality of X')\n",
    "parser.add_argument('--Ydim', default=1, type=int, help='dimensionality of Y')\n",
    "\n",
    "parser.add_argument('--noise_dim', default=55, type=int, help='dimensionality of noise vector')\n",
    "parser.add_argument('--noise_dist', default='gaussian', type=str, help='distribution of noise vector')\n",
    "\n",
    "parser.add_argument('--train', default=40000, type=int, help='size of train dataset')\n",
    "parser.add_argument('--val', default=3500, type=int, help='size of validation dataset')\n",
    "parser.add_argument('--test', default=10000, type=int, help='size of test dataset')\n",
    "\n",
    "parser.add_argument('--train_batch', default=64, type=int, metavar='BS', help='batch size while training')\n",
    "parser.add_argument('--val_batch', default=100, type=int, metavar='BS', help='batch size while validation')\n",
    "parser.add_argument('--test_batch', default=100, type=int, metavar='BS', help='batch size while testing')\n",
    "parser.add_argument('--epochs', default=50, type=int, help='number of epochs to train')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5172f7a0-cc41-43d4-ba8c-18431c70316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "all_CT = pd.read_csv(\"../data/CT.csv\")\n",
    "all_CT = all_CT.iloc[:, 1:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cb1381-dd02-4653-91ca-9e295796649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(1234)  \n",
    "#split data into training dataset, testing dataset and validation dataset\n",
    "train_val_data, test_data = train_test_split(all_CT, test_size=args.test)#, random_state=5678)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=args.val)#, random_state=5678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e64fe7-a61c-4236-9025-55b173a899dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to PyTorch tensors\n",
    "X_train = torch.tensor(train_data.values[:, :-1], dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data.values[:, -1], dtype=torch.float32)\n",
    "\n",
    "X_val = torch.tensor(val_data.values[:, :-1], dtype=torch.float32)\n",
    "y_val = torch.tensor(val_data.values[:, -1], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(test_data.values[:, :-1], dtype=torch.float32)\n",
    "y_test = torch.tensor(test_data.values[:, -1], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4264e0a1-862b-47cd-a6e7-3a87dbf94209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 1\n"
     ]
    }
   ],
   "source": [
    "args.Xdim = get_dimension(X_train)\n",
    "args.Ydim = get_dimension(y_train)\n",
    "print(args.Xdim, args.Ydim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b089349-5acc-4c69-8769-85d2a51305d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca0461d-f16b-40dc-b081-e4f49c08b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "loader_train = DataLoader(train_dataset, batch_size=args.train_batch, shuffle=True)\n",
    "loader_val = DataLoader(val_dataset, batch_size=args.val_batch, shuffle=True)\n",
    "loader_test = DataLoader(test_dataset, batch_size=args.test_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473f7284-766c-4594-8945-acf0f3162bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define generator network and discriminator network\n",
    "G_net = generator_fnn(Xdim=args.Xdim, Ydim=args.Ydim, noise_dim=args.noise_dim, hidden_dims = [128, 64])\n",
    "D_net = discriminator_fnn(input_dim=args.Xdim+args.Ydim, hidden_dims = [128, 64])\n",
    "\n",
    "# Initialize RMSprop optimizers\n",
    "D_solver = optim.Adam(D_net.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "G_solver = optim.Adam(G_net.parameters(), lr=0.001, betas=(0.9, 0.999))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2500d4fc-d853-4aa7-a8f7-6ed56bb1e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 Loss: 47.040485, Mean L2 Loss: 2712.421387\n",
      "Epoch 0 - D Loss: -0.6777, G Loss: 224.9659\n",
      "Epoch 1 - D Loss: -0.0735, G Loss: 28.6452\n",
      "Epoch 2 - D Loss: -0.0640, G Loss: 14.3801\n",
      "Mean L1 Loss: 2.478719, Mean L2 Loss: 12.523778\n",
      "Epoch 3, Iter 2000, D Loss: -0.0607, G Loss: 8.4871, L1: 2.4787, L2: 12.5238\n",
      "Saved best model with L2: 12.5238\n",
      "Mean L1 Loss: 2.469465, Mean L2 Loss: 12.204628\n",
      "Epoch 3, Iter 2100, D Loss: -0.0558, G Loss: 8.3594, L1: 2.4695, L2: 12.2046\n",
      "Saved best model with L2: 12.2046\n",
      "Mean L1 Loss: 2.234936, Mean L2 Loss: 10.218590\n",
      "Epoch 3, Iter 2200, D Loss: -0.0554, G Loss: 8.0822, L1: 2.2349, L2: 10.2186\n",
      "Saved best model with L2: 10.2186\n",
      "Mean L1 Loss: 2.183106, Mean L2 Loss: 9.822016\n",
      "Epoch 3, Iter 2300, D Loss: -0.0519, G Loss: 7.9448, L1: 2.1831, L2: 9.8220\n",
      "Saved best model with L2: 9.8220\n",
      "Mean L1 Loss: 2.068280, Mean L2 Loss: 8.714594\n",
      "Epoch 3, Iter 2400, D Loss: -0.0519, G Loss: 7.6567, L1: 2.0683, L2: 8.7146\n",
      "Saved best model with L2: 8.7146\n",
      "Mean L1 Loss: 1.997804, Mean L2 Loss: 8.101177\n",
      "Epoch 3, Iter 2500, D Loss: -0.0498, G Loss: 7.5352, L1: 1.9978, L2: 8.1012\n",
      "Saved best model with L2: 8.1012\n",
      "Epoch 3 - D Loss: -0.0498, G Loss: 7.5352\n",
      "Mean L1 Loss: 1.959554, Mean L2 Loss: 7.640001\n",
      "Epoch 4, Iter 2600, D Loss: -0.0521, G Loss: 5.3945, L1: 1.9596, L2: 7.6400\n",
      "Saved best model with L2: 7.6400\n",
      "Mean L1 Loss: 1.976397, Mean L2 Loss: 7.379293\n",
      "Epoch 4, Iter 2700, D Loss: -0.0445, G Loss: 5.3533, L1: 1.9764, L2: 7.3793\n",
      "Saved best model with L2: 7.3793\n",
      "Mean L1 Loss: 1.890029, Mean L2 Loss: 6.924523\n",
      "Epoch 4, Iter 2800, D Loss: -0.0397, G Loss: 5.0125, L1: 1.8900, L2: 6.9245\n",
      "Saved best model with L2: 6.9245\n",
      "Mean L1 Loss: 1.769395, Mean L2 Loss: 6.267799\n",
      "Epoch 4, Iter 2900, D Loss: -0.0380, G Loss: 4.7361, L1: 1.7694, L2: 6.2678\n",
      "Saved best model with L2: 6.2678\n",
      "Mean L1 Loss: 1.737049, Mean L2 Loss: 5.860641\n",
      "Epoch 4, Iter 3000, D Loss: -0.0350, G Loss: 4.5760, L1: 1.7370, L2: 5.8606\n",
      "Saved best model with L2: 5.8606\n",
      "Mean L1 Loss: 1.674439, Mean L2 Loss: 5.461078\n",
      "Epoch 4, Iter 3100, D Loss: -0.0351, G Loss: 4.4403, L1: 1.6744, L2: 5.4611\n",
      "Saved best model with L2: 5.4611\n",
      "Epoch 4 - D Loss: -0.0348, G Loss: 4.3776\n",
      "Mean L1 Loss: 1.601042, Mean L2 Loss: 5.028723\n",
      "Epoch 5, Iter 3200, D Loss: -0.0103, G Loss: 2.7183, L1: 1.6010, L2: 5.0287\n",
      "Saved best model with L2: 5.0287\n",
      "Mean L1 Loss: 1.805565, Mean L2 Loss: 5.780188\n",
      "Epoch 5, Iter 3300, D Loss: -0.0167, G Loss: 3.1922, L1: 1.8056, L2: 5.7802\n",
      "Mean L1 Loss: 1.572550, Mean L2 Loss: 4.665974\n",
      "Epoch 5, Iter 3400, D Loss: -0.0215, G Loss: 2.9822, L1: 1.5726, L2: 4.6660\n",
      "Saved best model with L2: 4.6660\n",
      "Mean L1 Loss: 1.491754, Mean L2 Loss: 4.299899\n",
      "Epoch 5, Iter 3500, D Loss: -0.0192, G Loss: 2.9334, L1: 1.4918, L2: 4.2999\n",
      "Saved best model with L2: 4.2999\n",
      "Mean L1 Loss: 1.586476, Mean L2 Loss: 4.527199\n",
      "Epoch 5, Iter 3600, D Loss: -0.0210, G Loss: 2.8825, L1: 1.5865, L2: 4.5272\n",
      "Mean L1 Loss: 1.419762, Mean L2 Loss: 3.802899\n",
      "Epoch 5, Iter 3700, D Loss: -0.0198, G Loss: 2.7775, L1: 1.4198, L2: 3.8029\n",
      "Saved best model with L2: 3.8029\n",
      "Epoch 5 - D Loss: -0.0203, G Loss: 2.6988\n",
      "Mean L1 Loss: 1.556392, Mean L2 Loss: 4.288187\n",
      "Epoch 6, Iter 3800, D Loss: -0.0001, G Loss: 1.8522, L1: 1.5564, L2: 4.2882\n",
      "Mean L1 Loss: 1.406433, Mean L2 Loss: 3.654852\n",
      "Epoch 6, Iter 3900, D Loss: -0.0138, G Loss: 1.9570, L1: 1.4064, L2: 3.6549\n",
      "Saved best model with L2: 3.6549\n",
      "Mean L1 Loss: 1.348576, Mean L2 Loss: 3.380121\n",
      "Epoch 6, Iter 4000, D Loss: -0.0137, G Loss: 1.8488, L1: 1.3486, L2: 3.3801\n",
      "Saved best model with L2: 3.3801\n",
      "Mean L1 Loss: 1.451760, Mean L2 Loss: 3.845908\n",
      "Epoch 6, Iter 4100, D Loss: -0.0111, G Loss: 1.9119, L1: 1.4518, L2: 3.8459\n",
      "Mean L1 Loss: 1.340569, Mean L2 Loss: 3.425007\n",
      "Epoch 6, Iter 4200, D Loss: -0.0136, G Loss: 1.8697, L1: 1.3406, L2: 3.4250\n",
      "Mean L1 Loss: 1.264163, Mean L2 Loss: 2.953239\n",
      "Epoch 6, Iter 4300, D Loss: -0.0141, G Loss: 1.8967, L1: 1.2642, L2: 2.9532\n",
      "Saved best model with L2: 2.9532\n",
      "Epoch 6 - D Loss: -0.0149, G Loss: 1.9439\n",
      "Mean L1 Loss: 1.235120, Mean L2 Loss: 2.840053\n",
      "Epoch 7, Iter 4400, D Loss: -0.0057, G Loss: 1.6723, L1: 1.2351, L2: 2.8401\n",
      "Saved best model with L2: 2.8401\n",
      "Mean L1 Loss: 1.238624, Mean L2 Loss: 2.800382\n",
      "Epoch 7, Iter 4500, D Loss: -0.0194, G Loss: 1.3638, L1: 1.2386, L2: 2.8004\n",
      "Saved best model with L2: 2.8004\n",
      "Mean L1 Loss: 1.210446, Mean L2 Loss: 2.734354\n",
      "Epoch 7, Iter 4600, D Loss: -0.0092, G Loss: 1.4422, L1: 1.2104, L2: 2.7344\n",
      "Saved best model with L2: 2.7344\n",
      "Mean L1 Loss: 1.220636, Mean L2 Loss: 2.731435\n",
      "Epoch 7, Iter 4700, D Loss: -0.0091, G Loss: 1.5477, L1: 1.2206, L2: 2.7314\n",
      "Saved best model with L2: 2.7314\n",
      "Mean L1 Loss: 1.148736, Mean L2 Loss: 2.495378\n",
      "Epoch 7, Iter 4800, D Loss: -0.0104, G Loss: 1.5911, L1: 1.1487, L2: 2.4954\n",
      "Saved best model with L2: 2.4954\n",
      "Mean L1 Loss: 1.118485, Mean L2 Loss: 2.357965\n",
      "Epoch 7, Iter 4900, D Loss: -0.0089, G Loss: 1.5838, L1: 1.1185, L2: 2.3580\n",
      "Saved best model with L2: 2.3580\n",
      "Mean L1 Loss: 1.176965, Mean L2 Loss: 2.551730\n",
      "Epoch 7, Iter 5000, D Loss: -0.0081, G Loss: 1.6630, L1: 1.1770, L2: 2.5517\n",
      "Epoch 7 - D Loss: -0.0081, G Loss: 1.6630\n",
      "Mean L1 Loss: 1.071833, Mean L2 Loss: 2.186525\n",
      "Epoch 8, Iter 5100, D Loss: -0.0076, G Loss: 2.2845, L1: 1.0718, L2: 2.1865\n",
      "Saved best model with L2: 2.1865\n",
      "Mean L1 Loss: 1.163383, Mean L2 Loss: 2.421619\n",
      "Epoch 8, Iter 5200, D Loss: -0.0085, G Loss: 2.2848, L1: 1.1634, L2: 2.4216\n",
      "Mean L1 Loss: 1.135613, Mean L2 Loss: 2.368079\n",
      "Epoch 8, Iter 5300, D Loss: -0.0076, G Loss: 2.1090, L1: 1.1356, L2: 2.3681\n",
      "Mean L1 Loss: 1.181905, Mean L2 Loss: 2.606289\n",
      "Epoch 8, Iter 5400, D Loss: -0.0087, G Loss: 2.0780, L1: 1.1819, L2: 2.6063\n",
      "Mean L1 Loss: 1.279012, Mean L2 Loss: 2.828561\n",
      "Epoch 8, Iter 5500, D Loss: -0.0090, G Loss: 1.9207, L1: 1.2790, L2: 2.8286\n",
      "Mean L1 Loss: 1.017974, Mean L2 Loss: 1.978825\n",
      "Epoch 8, Iter 5600, D Loss: -0.0088, G Loss: 1.8223, L1: 1.0180, L2: 1.9788\n",
      "Saved best model with L2: 1.9788\n",
      "Epoch 8 - D Loss: -0.0090, G Loss: 1.7948\n",
      "Mean L1 Loss: 1.077360, Mean L2 Loss: 2.123633\n",
      "Epoch 9, Iter 5700, D Loss: -0.0020, G Loss: 0.9468, L1: 1.0774, L2: 2.1236\n",
      "Mean L1 Loss: 1.044053, Mean L2 Loss: 2.035943\n",
      "Epoch 9, Iter 5800, D Loss: -0.0041, G Loss: 1.0059, L1: 1.0441, L2: 2.0359\n",
      "Mean L1 Loss: 1.049602, Mean L2 Loss: 2.057714\n",
      "Epoch 9, Iter 5900, D Loss: -0.0067, G Loss: 0.8673, L1: 1.0496, L2: 2.0577\n",
      "Mean L1 Loss: 1.072477, Mean L2 Loss: 2.085987\n",
      "Epoch 9, Iter 6000, D Loss: -0.0056, G Loss: 0.8289, L1: 1.0725, L2: 2.0860\n",
      "Mean L1 Loss: 1.116668, Mean L2 Loss: 2.254136\n",
      "Epoch 9, Iter 6100, D Loss: -0.0048, G Loss: 0.8984, L1: 1.1167, L2: 2.2541\n",
      "Mean L1 Loss: 0.933501, Mean L2 Loss: 1.672481\n",
      "Epoch 9, Iter 6200, D Loss: -0.0044, G Loss: 0.9588, L1: 0.9335, L2: 1.6725\n",
      "Saved best model with L2: 1.6725\n",
      "Epoch 9 - D Loss: -0.0051, G Loss: 0.9520\n",
      "Mean L1 Loss: 1.092469, Mean L2 Loss: 2.217897\n",
      "Epoch 10, Iter 6300, D Loss: -0.0011, G Loss: 0.6132, L1: 1.0925, L2: 2.2179\n",
      "Mean L1 Loss: 1.023790, Mean L2 Loss: 1.922952\n",
      "Epoch 10, Iter 6400, D Loss: -0.0003, G Loss: 0.8266, L1: 1.0238, L2: 1.9230\n",
      "Mean L1 Loss: 0.912862, Mean L2 Loss: 1.596338\n",
      "Epoch 10, Iter 6500, D Loss: -0.0011, G Loss: 0.7277, L1: 0.9129, L2: 1.5963\n",
      "Saved best model with L2: 1.5963\n",
      "Mean L1 Loss: 1.067487, Mean L2 Loss: 2.084919\n",
      "Epoch 10, Iter 6600, D Loss: -0.0014, G Loss: 0.7441, L1: 1.0675, L2: 2.0849\n",
      "Mean L1 Loss: 0.911363, Mean L2 Loss: 1.617569\n",
      "Epoch 10, Iter 6700, D Loss: -0.0031, G Loss: 0.8068, L1: 0.9114, L2: 1.6176\n",
      "Mean L1 Loss: 1.118043, Mean L2 Loss: 2.278339\n",
      "Epoch 10, Iter 6800, D Loss: -0.0034, G Loss: 0.8163, L1: 1.1180, L2: 2.2783\n",
      "Epoch 10 - D Loss: -0.0033, G Loss: 0.7910\n",
      "Mean L1 Loss: 0.902740, Mean L2 Loss: 1.619811\n",
      "Epoch 11, Iter 6900, D Loss: 0.0119, G Loss: 0.7423, L1: 0.9027, L2: 1.6198\n",
      "Mean L1 Loss: 0.937878, Mean L2 Loss: 1.714152\n",
      "Epoch 11, Iter 7000, D Loss: 0.0039, G Loss: 0.8881, L1: 0.9379, L2: 1.7142\n",
      "Mean L1 Loss: 1.004809, Mean L2 Loss: 1.862313\n",
      "Epoch 11, Iter 7100, D Loss: 0.0014, G Loss: 1.0830, L1: 1.0048, L2: 1.8623\n",
      "Mean L1 Loss: 0.883592, Mean L2 Loss: 1.473755\n",
      "Epoch 11, Iter 7200, D Loss: -0.0002, G Loss: 1.0872, L1: 0.8836, L2: 1.4738\n",
      "Saved best model with L2: 1.4738\n",
      "Mean L1 Loss: 0.902509, Mean L2 Loss: 1.543465\n",
      "Epoch 11, Iter 7300, D Loss: -0.0010, G Loss: 1.0563, L1: 0.9025, L2: 1.5435\n",
      "Mean L1 Loss: 0.946619, Mean L2 Loss: 1.642667\n",
      "Epoch 11, Iter 7400, D Loss: -0.0030, G Loss: 1.0358, L1: 0.9466, L2: 1.6427\n",
      "Mean L1 Loss: 0.930964, Mean L2 Loss: 1.609451\n",
      "Epoch 11, Iter 7500, D Loss: -0.0031, G Loss: 0.9394, L1: 0.9310, L2: 1.6095\n",
      "Epoch 11 - D Loss: -0.0031, G Loss: 0.9394\n",
      "Mean L1 Loss: 0.901527, Mean L2 Loss: 1.525918\n",
      "Epoch 12, Iter 7600, D Loss: -0.0025, G Loss: 0.6195, L1: 0.9015, L2: 1.5259\n",
      "Mean L1 Loss: 0.987326, Mean L2 Loss: 1.729126\n",
      "Epoch 12, Iter 7700, D Loss: -0.0029, G Loss: 0.6737, L1: 0.9873, L2: 1.7291\n",
      "Mean L1 Loss: 0.889837, Mean L2 Loss: 1.508584\n",
      "Epoch 12, Iter 7800, D Loss: -0.0008, G Loss: 0.8227, L1: 0.8898, L2: 1.5086\n",
      "Mean L1 Loss: 0.870910, Mean L2 Loss: 1.435143\n",
      "Epoch 12, Iter 7900, D Loss: -0.0013, G Loss: 0.9405, L1: 0.8709, L2: 1.4351\n",
      "Saved best model with L2: 1.4351\n",
      "Mean L1 Loss: 1.092928, Mean L2 Loss: 2.051495\n",
      "Epoch 12, Iter 8000, D Loss: -0.0010, G Loss: 1.0545, L1: 1.0929, L2: 2.0515\n",
      "Mean L1 Loss: 0.943151, Mean L2 Loss: 1.681741\n",
      "Epoch 12, Iter 8100, D Loss: -0.0012, G Loss: 1.1189, L1: 0.9432, L2: 1.6817\n",
      "Epoch 12 - D Loss: -0.0013, G Loss: 1.1351\n",
      "Mean L1 Loss: 0.823056, Mean L2 Loss: 1.324978\n",
      "Epoch 13, Iter 8200, D Loss: -0.0052, G Loss: 1.1288, L1: 0.8231, L2: 1.3250\n",
      "Saved best model with L2: 1.3250\n",
      "Mean L1 Loss: 0.822133, Mean L2 Loss: 1.279333\n",
      "Epoch 13, Iter 8300, D Loss: -0.0051, G Loss: 1.1778, L1: 0.8221, L2: 1.2793\n",
      "Saved best model with L2: 1.2793\n",
      "Mean L1 Loss: 0.806766, Mean L2 Loss: 1.243364\n",
      "Epoch 13, Iter 8400, D Loss: -0.0045, G Loss: 1.0972, L1: 0.8068, L2: 1.2434\n",
      "Saved best model with L2: 1.2434\n",
      "Mean L1 Loss: 0.818717, Mean L2 Loss: 1.284910\n",
      "Epoch 13, Iter 8500, D Loss: -0.0043, G Loss: 0.9896, L1: 0.8187, L2: 1.2849\n",
      "Mean L1 Loss: 0.813498, Mean L2 Loss: 1.305667\n",
      "Epoch 13, Iter 8600, D Loss: -0.0039, G Loss: 0.9647, L1: 0.8135, L2: 1.3057\n",
      "Mean L1 Loss: 1.003134, Mean L2 Loss: 1.837098\n",
      "Epoch 13, Iter 8700, D Loss: -0.0031, G Loss: 0.9586, L1: 1.0031, L2: 1.8371\n",
      "Epoch 13 - D Loss: -0.0032, G Loss: 0.9896\n",
      "Mean L1 Loss: 0.903039, Mean L2 Loss: 1.519093\n",
      "Epoch 14, Iter 8800, D Loss: -0.0020, G Loss: 0.9275, L1: 0.9030, L2: 1.5191\n",
      "Mean L1 Loss: 0.804390, Mean L2 Loss: 1.270515\n",
      "Epoch 14, Iter 8900, D Loss: -0.0031, G Loss: 1.0206, L1: 0.8044, L2: 1.2705\n",
      "Mean L1 Loss: 0.962380, Mean L2 Loss: 1.643234\n",
      "Epoch 14, Iter 9000, D Loss: -0.0018, G Loss: 1.0636, L1: 0.9624, L2: 1.6432\n",
      "Mean L1 Loss: 0.984292, Mean L2 Loss: 1.694453\n",
      "Epoch 14, Iter 9100, D Loss: -0.0021, G Loss: 1.1092, L1: 0.9843, L2: 1.6945\n",
      "Mean L1 Loss: 0.823674, Mean L2 Loss: 1.267748\n",
      "Epoch 14, Iter 9200, D Loss: -0.0029, G Loss: 1.0905, L1: 0.8237, L2: 1.2677\n",
      "Mean L1 Loss: 0.737378, Mean L2 Loss: 1.093458\n",
      "Epoch 14, Iter 9300, D Loss: -0.0029, G Loss: 1.1013, L1: 0.7374, L2: 1.0935\n",
      "Saved best model with L2: 1.0935\n",
      "Epoch 14 - D Loss: -0.0028, G Loss: 1.1283\n",
      "Mean L1 Loss: 0.913792, Mean L2 Loss: 1.515189\n",
      "Epoch 15, Iter 9400, D Loss: -0.0005, G Loss: 1.4057, L1: 0.9138, L2: 1.5152\n",
      "Mean L1 Loss: 0.745486, Mean L2 Loss: 1.085727\n",
      "Epoch 15, Iter 9500, D Loss: -0.0009, G Loss: 1.4019, L1: 0.7455, L2: 1.0857\n",
      "Saved best model with L2: 1.0857\n",
      "Mean L1 Loss: 0.761175, Mean L2 Loss: 1.147457\n",
      "Epoch 15, Iter 9600, D Loss: -0.0021, G Loss: 1.3750, L1: 0.7612, L2: 1.1475\n",
      "Mean L1 Loss: 0.846423, Mean L2 Loss: 1.330383\n",
      "Epoch 15, Iter 9700, D Loss: -0.0026, G Loss: 1.3549, L1: 0.8464, L2: 1.3304\n",
      "Mean L1 Loss: 0.764705, Mean L2 Loss: 1.188588\n",
      "Epoch 15, Iter 9800, D Loss: -0.0030, G Loss: 1.3829, L1: 0.7647, L2: 1.1886\n",
      "Mean L1 Loss: 0.780903, Mean L2 Loss: 1.172088\n",
      "Epoch 15, Iter 9900, D Loss: -0.0032, G Loss: 1.4344, L1: 0.7809, L2: 1.1721\n",
      "Mean L1 Loss: 0.789348, Mean L2 Loss: 1.192881\n",
      "Epoch 15, Iter 10000, D Loss: -0.0028, G Loss: 1.4358, L1: 0.7893, L2: 1.1929\n",
      "Epoch 15 - D Loss: -0.0028, G Loss: 1.4358\n",
      "Mean L1 Loss: 0.755592, Mean L2 Loss: 1.104911\n",
      "Epoch 16, Iter 10100, D Loss: -0.0031, G Loss: 1.1517, L1: 0.7556, L2: 1.1049\n",
      "Mean L1 Loss: 0.711345, Mean L2 Loss: 1.010638\n",
      "Epoch 16, Iter 10200, D Loss: -0.0030, G Loss: 1.0654, L1: 0.7113, L2: 1.0106\n",
      "Saved best model with L2: 1.0106\n",
      "Mean L1 Loss: 0.753260, Mean L2 Loss: 1.070366\n",
      "Epoch 16, Iter 10300, D Loss: -0.0035, G Loss: 0.9425, L1: 0.7533, L2: 1.0704\n",
      "Mean L1 Loss: 0.731952, Mean L2 Loss: 1.025460\n",
      "Epoch 16, Iter 10400, D Loss: -0.0028, G Loss: 0.9248, L1: 0.7320, L2: 1.0255\n",
      "Mean L1 Loss: 0.788933, Mean L2 Loss: 1.145875\n",
      "Epoch 16, Iter 10500, D Loss: -0.0024, G Loss: 0.9293, L1: 0.7889, L2: 1.1459\n",
      "Mean L1 Loss: 0.692144, Mean L2 Loss: 0.923554\n",
      "Epoch 16, Iter 10600, D Loss: -0.0024, G Loss: 0.9904, L1: 0.6921, L2: 0.9236\n",
      "Saved best model with L2: 0.9236\n",
      "Epoch 16 - D Loss: -0.0024, G Loss: 1.0112\n",
      "Mean L1 Loss: 0.717535, Mean L2 Loss: 1.016194\n",
      "Epoch 17, Iter 10700, D Loss: -0.0000, G Loss: 1.4088, L1: 0.7175, L2: 1.0162\n",
      "Mean L1 Loss: 0.712941, Mean L2 Loss: 0.990644\n",
      "Epoch 17, Iter 10800, D Loss: -0.0012, G Loss: 1.5776, L1: 0.7129, L2: 0.9906\n",
      "Mean L1 Loss: 0.738804, Mean L2 Loss: 1.028276\n",
      "Epoch 17, Iter 10900, D Loss: -0.0005, G Loss: 1.4807, L1: 0.7388, L2: 1.0283\n",
      "Mean L1 Loss: 0.928589, Mean L2 Loss: 1.547802\n",
      "Epoch 17, Iter 11000, D Loss: -0.0012, G Loss: 1.4043, L1: 0.9286, L2: 1.5478\n",
      "Mean L1 Loss: 0.777765, Mean L2 Loss: 1.148869\n",
      "Epoch 17, Iter 11100, D Loss: -0.0011, G Loss: 1.3826, L1: 0.7778, L2: 1.1489\n",
      "Mean L1 Loss: 0.677381, Mean L2 Loss: 0.927110\n",
      "Epoch 17, Iter 11200, D Loss: -0.0012, G Loss: 1.3556, L1: 0.6774, L2: 0.9271\n",
      "Epoch 17 - D Loss: -0.0012, G Loss: 1.3297\n",
      "Mean L1 Loss: 0.693047, Mean L2 Loss: 0.949494\n",
      "Epoch 18, Iter 11300, D Loss: -0.0039, G Loss: 1.0433, L1: 0.6930, L2: 0.9495\n",
      "Mean L1 Loss: 0.690803, Mean L2 Loss: 0.937537\n",
      "Epoch 18, Iter 11400, D Loss: -0.0025, G Loss: 1.0091, L1: 0.6908, L2: 0.9375\n",
      "Mean L1 Loss: 0.762152, Mean L2 Loss: 1.147250\n",
      "Epoch 18, Iter 11500, D Loss: -0.0022, G Loss: 0.8647, L1: 0.7622, L2: 1.1473\n",
      "Mean L1 Loss: 0.705436, Mean L2 Loss: 0.969975\n",
      "Epoch 18, Iter 11600, D Loss: -0.0031, G Loss: 0.7159, L1: 0.7054, L2: 0.9700\n",
      "Mean L1 Loss: 0.689926, Mean L2 Loss: 0.943702\n",
      "Epoch 18, Iter 11700, D Loss: -0.0028, G Loss: 0.5816, L1: 0.6899, L2: 0.9437\n",
      "Mean L1 Loss: 0.702288, Mean L2 Loss: 0.960570\n",
      "Epoch 18, Iter 11800, D Loss: -0.0024, G Loss: 0.5953, L1: 0.7023, L2: 0.9606\n",
      "Epoch 18 - D Loss: -0.0023, G Loss: 0.5897\n",
      "Mean L1 Loss: 0.663336, Mean L2 Loss: 0.882786\n",
      "Epoch 19, Iter 11900, D Loss: 0.0281, G Loss: 0.4006, L1: 0.6633, L2: 0.8828\n",
      "Saved best model with L2: 0.8828\n",
      "Mean L1 Loss: 0.677374, Mean L2 Loss: 0.880237\n",
      "Epoch 19, Iter 12000, D Loss: 0.0075, G Loss: 0.5207, L1: 0.6774, L2: 0.8802\n",
      "Saved best model with L2: 0.8802\n",
      "Mean L1 Loss: 0.655636, Mean L2 Loss: 0.845835\n",
      "Epoch 19, Iter 12100, D Loss: 0.0040, G Loss: 0.4125, L1: 0.6556, L2: 0.8458\n",
      "Saved best model with L2: 0.8458\n",
      "Mean L1 Loss: 0.679507, Mean L2 Loss: 0.929544\n",
      "Epoch 19, Iter 12200, D Loss: 0.0029, G Loss: 0.4088, L1: 0.6795, L2: 0.9295\n",
      "Mean L1 Loss: 0.692107, Mean L2 Loss: 0.912718\n",
      "Epoch 19, Iter 12300, D Loss: 0.0021, G Loss: 0.4338, L1: 0.6921, L2: 0.9127\n",
      "Mean L1 Loss: 0.658328, Mean L2 Loss: 0.854077\n",
      "Epoch 19, Iter 12400, D Loss: 0.0012, G Loss: 0.4179, L1: 0.6583, L2: 0.8541\n",
      "Mean L1 Loss: 0.700499, Mean L2 Loss: 0.936670\n",
      "Epoch 19, Iter 12500, D Loss: 0.0010, G Loss: 0.4190, L1: 0.7005, L2: 0.9367\n",
      "Epoch 19 - D Loss: 0.0010, G Loss: 0.4190\n",
      "Mean L1 Loss: 0.665910, Mean L2 Loss: 0.897749\n",
      "Epoch 20, Iter 12600, D Loss: -0.0021, G Loss: 0.2591, L1: 0.6659, L2: 0.8977\n",
      "Mean L1 Loss: 0.645861, Mean L2 Loss: 0.844816\n",
      "Epoch 20, Iter 12700, D Loss: -0.0018, G Loss: 0.1649, L1: 0.6459, L2: 0.8448\n",
      "Saved best model with L2: 0.8448\n",
      "Mean L1 Loss: 0.665839, Mean L2 Loss: 0.856148\n",
      "Epoch 20, Iter 12800, D Loss: -0.0014, G Loss: 0.1805, L1: 0.6658, L2: 0.8561\n",
      "Mean L1 Loss: 0.649599, Mean L2 Loss: 0.836428\n",
      "Epoch 20, Iter 12900, D Loss: -0.0012, G Loss: 0.1826, L1: 0.6496, L2: 0.8364\n",
      "Saved best model with L2: 0.8364\n",
      "Mean L1 Loss: 0.655914, Mean L2 Loss: 0.835663\n",
      "Epoch 20, Iter 13000, D Loss: -0.0020, G Loss: 0.1991, L1: 0.6559, L2: 0.8357\n",
      "Saved best model with L2: 0.8357\n",
      "Mean L1 Loss: 0.666278, Mean L2 Loss: 0.867996\n",
      "Epoch 20, Iter 13100, D Loss: -0.0018, G Loss: 0.1902, L1: 0.6663, L2: 0.8680\n",
      "Epoch 20 - D Loss: -0.0016, G Loss: 0.1883\n",
      "Mean L1 Loss: 0.645708, Mean L2 Loss: 0.818153\n",
      "Epoch 21, Iter 13200, D Loss: 0.0016, G Loss: 0.1791, L1: 0.6457, L2: 0.8182\n",
      "Saved best model with L2: 0.8182\n",
      "Mean L1 Loss: 0.711263, Mean L2 Loss: 0.931857\n",
      "Epoch 21, Iter 13300, D Loss: 0.0010, G Loss: 0.2911, L1: 0.7113, L2: 0.9319\n",
      "Mean L1 Loss: 0.715863, Mean L2 Loss: 0.974668\n",
      "Epoch 21, Iter 13400, D Loss: 0.0002, G Loss: 0.3480, L1: 0.7159, L2: 0.9747\n",
      "Mean L1 Loss: 0.649962, Mean L2 Loss: 0.843298\n",
      "Epoch 21, Iter 13500, D Loss: -0.0002, G Loss: 0.4003, L1: 0.6500, L2: 0.8433\n",
      "Mean L1 Loss: 0.685031, Mean L2 Loss: 0.909449\n",
      "Epoch 21, Iter 13600, D Loss: -0.0008, G Loss: 0.3849, L1: 0.6850, L2: 0.9094\n",
      "Mean L1 Loss: 0.703386, Mean L2 Loss: 0.934918\n",
      "Epoch 21, Iter 13700, D Loss: -0.0009, G Loss: 0.3965, L1: 0.7034, L2: 0.9349\n",
      "Epoch 21 - D Loss: -0.0007, G Loss: 0.3972\n",
      "Mean L1 Loss: 0.657420, Mean L2 Loss: 0.838319\n",
      "Epoch 22, Iter 13800, D Loss: -0.0023, G Loss: 0.4883, L1: 0.6574, L2: 0.8383\n",
      "Mean L1 Loss: 0.665565, Mean L2 Loss: 0.876631\n",
      "Epoch 22, Iter 13900, D Loss: -0.0023, G Loss: 0.4130, L1: 0.6656, L2: 0.8766\n",
      "Mean L1 Loss: 0.643815, Mean L2 Loss: 0.817807\n",
      "Epoch 22, Iter 14000, D Loss: -0.0018, G Loss: 0.3780, L1: 0.6438, L2: 0.8178\n",
      "Saved best model with L2: 0.8178\n",
      "Mean L1 Loss: 0.683311, Mean L2 Loss: 0.897808\n",
      "Epoch 22, Iter 14100, D Loss: -0.0017, G Loss: 0.3406, L1: 0.6833, L2: 0.8978\n",
      "Mean L1 Loss: 0.614152, Mean L2 Loss: 0.768520\n",
      "Epoch 22, Iter 14200, D Loss: -0.0012, G Loss: 0.3554, L1: 0.6142, L2: 0.7685\n",
      "Saved best model with L2: 0.7685\n",
      "Mean L1 Loss: 0.670677, Mean L2 Loss: 0.891201\n",
      "Epoch 22, Iter 14300, D Loss: -0.0009, G Loss: 0.3717, L1: 0.6707, L2: 0.8912\n",
      "Epoch 22 - D Loss: -0.0009, G Loss: 0.3779\n",
      "Mean L1 Loss: 0.716337, Mean L2 Loss: 0.995374\n",
      "Epoch 23, Iter 14400, D Loss: -0.0005, G Loss: 0.4356, L1: 0.7163, L2: 0.9954\n",
      "Mean L1 Loss: 0.641363, Mean L2 Loss: 0.828760\n",
      "Epoch 23, Iter 14500, D Loss: -0.0020, G Loss: 0.3897, L1: 0.6414, L2: 0.8288\n",
      "Mean L1 Loss: 0.650282, Mean L2 Loss: 0.819567\n",
      "Epoch 23, Iter 14600, D Loss: -0.0014, G Loss: 0.4812, L1: 0.6503, L2: 0.8196\n",
      "Mean L1 Loss: 0.639574, Mean L2 Loss: 0.841361\n",
      "Epoch 23, Iter 14700, D Loss: -0.0017, G Loss: 0.5442, L1: 0.6396, L2: 0.8414\n",
      "Mean L1 Loss: 0.654627, Mean L2 Loss: 0.837863\n",
      "Epoch 23, Iter 14800, D Loss: -0.0015, G Loss: 0.6026, L1: 0.6546, L2: 0.8379\n",
      "Mean L1 Loss: 0.644689, Mean L2 Loss: 0.802995\n",
      "Epoch 23, Iter 14900, D Loss: -0.0012, G Loss: 0.6310, L1: 0.6447, L2: 0.8030\n",
      "Mean L1 Loss: 0.629407, Mean L2 Loss: 0.789361\n",
      "Epoch 23, Iter 15000, D Loss: -0.0010, G Loss: 0.6328, L1: 0.6294, L2: 0.7894\n",
      "Epoch 23 - D Loss: -0.0010, G Loss: 0.6328\n",
      "Mean L1 Loss: 0.629993, Mean L2 Loss: 0.760825\n",
      "Epoch 24, Iter 15100, D Loss: 0.0003, G Loss: 0.6802, L1: 0.6300, L2: 0.7608\n",
      "Saved best model with L2: 0.7608\n",
      "Mean L1 Loss: 0.687260, Mean L2 Loss: 0.939060\n",
      "Epoch 24, Iter 15200, D Loss: -0.0005, G Loss: 0.7102, L1: 0.6873, L2: 0.9391\n",
      "Mean L1 Loss: 0.720257, Mean L2 Loss: 0.968373\n",
      "Epoch 24, Iter 15300, D Loss: -0.0013, G Loss: 0.7783, L1: 0.7203, L2: 0.9684\n",
      "Mean L1 Loss: 0.654543, Mean L2 Loss: 0.874812\n",
      "Epoch 24, Iter 15400, D Loss: -0.0007, G Loss: 0.8080, L1: 0.6545, L2: 0.8748\n",
      "Mean L1 Loss: 0.693664, Mean L2 Loss: 0.915961\n",
      "Epoch 24, Iter 15500, D Loss: -0.0007, G Loss: 0.8102, L1: 0.6937, L2: 0.9160\n",
      "Mean L1 Loss: 0.640611, Mean L2 Loss: 0.780859\n",
      "Epoch 24, Iter 15600, D Loss: -0.0008, G Loss: 0.7848, L1: 0.6406, L2: 0.7809\n",
      "Epoch 24 - D Loss: -0.0008, G Loss: 0.7768\n",
      "Mean L1 Loss: 0.634388, Mean L2 Loss: 0.775326\n",
      "Epoch 25, Iter 15700, D Loss: -0.0009, G Loss: 0.5472, L1: 0.6344, L2: 0.7753\n",
      "Mean L1 Loss: 0.589900, Mean L2 Loss: 0.716790\n",
      "Epoch 25, Iter 15800, D Loss: -0.0009, G Loss: 0.5288, L1: 0.5899, L2: 0.7168\n",
      "Saved best model with L2: 0.7168\n",
      "Mean L1 Loss: 0.625765, Mean L2 Loss: 0.789690\n",
      "Epoch 25, Iter 15900, D Loss: -0.0010, G Loss: 0.5938, L1: 0.6258, L2: 0.7897\n",
      "Mean L1 Loss: 0.603565, Mean L2 Loss: 0.741169\n",
      "Epoch 25, Iter 16000, D Loss: -0.0015, G Loss: 0.6525, L1: 0.6036, L2: 0.7412\n",
      "Mean L1 Loss: 0.605917, Mean L2 Loss: 0.747566\n",
      "Epoch 25, Iter 16100, D Loss: -0.0012, G Loss: 0.6859, L1: 0.6059, L2: 0.7476\n",
      "Mean L1 Loss: 0.622998, Mean L2 Loss: 0.785306\n",
      "Epoch 25, Iter 16200, D Loss: -0.0011, G Loss: 0.6878, L1: 0.6230, L2: 0.7853\n",
      "Epoch 25 - D Loss: -0.0011, G Loss: 0.6915\n",
      "Mean L1 Loss: 0.609457, Mean L2 Loss: 0.750057\n",
      "Epoch 26, Iter 16300, D Loss: -0.0019, G Loss: 0.6718, L1: 0.6095, L2: 0.7501\n",
      "Mean L1 Loss: 0.663432, Mean L2 Loss: 0.822376\n",
      "Epoch 26, Iter 16400, D Loss: -0.0018, G Loss: 0.6696, L1: 0.6634, L2: 0.8224\n",
      "Mean L1 Loss: 0.585738, Mean L2 Loss: 0.702840\n",
      "Epoch 26, Iter 16500, D Loss: -0.0021, G Loss: 0.7385, L1: 0.5857, L2: 0.7028\n",
      "Saved best model with L2: 0.7028\n",
      "Mean L1 Loss: 0.625338, Mean L2 Loss: 0.776209\n",
      "Epoch 26, Iter 16600, D Loss: -0.0015, G Loss: 0.8041, L1: 0.6253, L2: 0.7762\n",
      "Mean L1 Loss: 0.586048, Mean L2 Loss: 0.701657\n",
      "Epoch 26, Iter 16700, D Loss: -0.0019, G Loss: 0.8101, L1: 0.5860, L2: 0.7017\n",
      "Saved best model with L2: 0.7017\n",
      "Mean L1 Loss: 0.623326, Mean L2 Loss: 0.775168\n",
      "Epoch 26, Iter 16800, D Loss: -0.0016, G Loss: 0.8241, L1: 0.6233, L2: 0.7752\n",
      "Epoch 26 - D Loss: -0.0014, G Loss: 0.8306\n",
      "Mean L1 Loss: 0.610171, Mean L2 Loss: 0.731819\n",
      "Epoch 27, Iter 16900, D Loss: -0.0029, G Loss: 0.8490, L1: 0.6102, L2: 0.7318\n",
      "Mean L1 Loss: 0.710288, Mean L2 Loss: 0.897391\n",
      "Epoch 27, Iter 17000, D Loss: -0.0017, G Loss: 0.8115, L1: 0.7103, L2: 0.8974\n",
      "Mean L1 Loss: 0.581318, Mean L2 Loss: 0.703232\n",
      "Epoch 27, Iter 17100, D Loss: -0.0013, G Loss: 0.7616, L1: 0.5813, L2: 0.7032\n",
      "Mean L1 Loss: 0.608777, Mean L2 Loss: 0.723997\n",
      "Epoch 27, Iter 17200, D Loss: -0.0015, G Loss: 0.6549, L1: 0.6088, L2: 0.7240\n",
      "Mean L1 Loss: 0.613436, Mean L2 Loss: 0.729202\n",
      "Epoch 27, Iter 17300, D Loss: -0.0008, G Loss: 0.5239, L1: 0.6134, L2: 0.7292\n",
      "Mean L1 Loss: 0.605006, Mean L2 Loss: 0.754293\n",
      "Epoch 27, Iter 17400, D Loss: -0.0011, G Loss: 0.4472, L1: 0.6050, L2: 0.7543\n",
      "Mean L1 Loss: 0.666951, Mean L2 Loss: 0.883090\n",
      "Epoch 27, Iter 17500, D Loss: -0.0008, G Loss: 0.3870, L1: 0.6670, L2: 0.8831\n",
      "Epoch 27 - D Loss: -0.0008, G Loss: 0.3870\n",
      "Mean L1 Loss: 0.549535, Mean L2 Loss: 0.640895\n",
      "Epoch 28, Iter 17600, D Loss: -0.0000, G Loss: 0.0817, L1: 0.5495, L2: 0.6409\n",
      "Saved best model with L2: 0.6409\n",
      "Mean L1 Loss: 0.563555, Mean L2 Loss: 0.651095\n",
      "Epoch 28, Iter 17700, D Loss: -0.0000, G Loss: 0.2143, L1: 0.5636, L2: 0.6511\n",
      "Mean L1 Loss: 0.589398, Mean L2 Loss: 0.710769\n",
      "Epoch 28, Iter 17800, D Loss: -0.0004, G Loss: 0.2551, L1: 0.5894, L2: 0.7108\n",
      "Mean L1 Loss: 0.697822, Mean L2 Loss: 0.929084\n",
      "Epoch 28, Iter 17900, D Loss: -0.0009, G Loss: 0.2412, L1: 0.6978, L2: 0.9291\n",
      "Mean L1 Loss: 0.548632, Mean L2 Loss: 0.631528\n",
      "Epoch 28, Iter 18000, D Loss: -0.0012, G Loss: 0.2041, L1: 0.5486, L2: 0.6315\n",
      "Saved best model with L2: 0.6315\n",
      "Mean L1 Loss: 0.565741, Mean L2 Loss: 0.645268\n",
      "Epoch 28, Iter 18100, D Loss: -0.0014, G Loss: 0.1501, L1: 0.5657, L2: 0.6453\n",
      "Epoch 28 - D Loss: -0.0013, G Loss: 0.1403\n",
      "Mean L1 Loss: 0.564116, Mean L2 Loss: 0.669350\n",
      "Epoch 29, Iter 18200, D Loss: 0.0002, G Loss: -0.1692, L1: 0.5641, L2: 0.6694\n",
      "Mean L1 Loss: 0.576859, Mean L2 Loss: 0.707894\n",
      "Epoch 29, Iter 18300, D Loss: -0.0001, G Loss: -0.0571, L1: 0.5769, L2: 0.7079\n",
      "Mean L1 Loss: 0.550671, Mean L2 Loss: 0.657460\n",
      "Epoch 29, Iter 18400, D Loss: 0.0003, G Loss: 0.0347, L1: 0.5507, L2: 0.6575\n",
      "Mean L1 Loss: 0.623189, Mean L2 Loss: 0.774322\n",
      "Epoch 29, Iter 18500, D Loss: -0.0005, G Loss: 0.1049, L1: 0.6232, L2: 0.7743\n",
      "Mean L1 Loss: 0.538823, Mean L2 Loss: 0.618323\n",
      "Epoch 29, Iter 18600, D Loss: -0.0010, G Loss: 0.1049, L1: 0.5388, L2: 0.6183\n",
      "Saved best model with L2: 0.6183\n",
      "Mean L1 Loss: 0.674752, Mean L2 Loss: 0.872234\n",
      "Epoch 29, Iter 18700, D Loss: -0.0012, G Loss: 0.0996, L1: 0.6748, L2: 0.8722\n",
      "Epoch 29 - D Loss: -0.0011, G Loss: 0.0899\n",
      "Mean L1 Loss: 0.602677, Mean L2 Loss: 0.721083\n",
      "Epoch 30, Iter 18800, D Loss: -0.0004, G Loss: -0.0056, L1: 0.6027, L2: 0.7211\n",
      "Mean L1 Loss: 0.584538, Mean L2 Loss: 0.744525\n",
      "Epoch 30, Iter 18900, D Loss: -0.0009, G Loss: -0.0038, L1: 0.5845, L2: 0.7445\n",
      "Mean L1 Loss: 0.536837, Mean L2 Loss: 0.627773\n",
      "Epoch 30, Iter 19000, D Loss: -0.0009, G Loss: 0.0442, L1: 0.5368, L2: 0.6278\n",
      "Mean L1 Loss: 0.549125, Mean L2 Loss: 0.634172\n",
      "Epoch 30, Iter 19100, D Loss: -0.0014, G Loss: 0.0163, L1: 0.5491, L2: 0.6342\n",
      "Mean L1 Loss: 0.544374, Mean L2 Loss: 0.622142\n",
      "Epoch 30, Iter 19200, D Loss: -0.0015, G Loss: -0.0120, L1: 0.5444, L2: 0.6221\n",
      "Mean L1 Loss: 0.667971, Mean L2 Loss: 0.891979\n",
      "Epoch 30, Iter 19300, D Loss: -0.0010, G Loss: -0.0312, L1: 0.6680, L2: 0.8920\n",
      "Epoch 30 - D Loss: -0.0012, G Loss: -0.0272\n",
      "Mean L1 Loss: 0.547558, Mean L2 Loss: 0.643688\n",
      "Epoch 31, Iter 19400, D Loss: -0.0029, G Loss: -0.0967, L1: 0.5476, L2: 0.6437\n",
      "Mean L1 Loss: 0.587529, Mean L2 Loss: 0.697713\n",
      "Epoch 31, Iter 19500, D Loss: -0.0014, G Loss: -0.0082, L1: 0.5875, L2: 0.6977\n",
      "Mean L1 Loss: 0.560474, Mean L2 Loss: 0.654262\n",
      "Epoch 31, Iter 19600, D Loss: -0.0012, G Loss: 0.0238, L1: 0.5605, L2: 0.6543\n",
      "Mean L1 Loss: 0.662270, Mean L2 Loss: 0.818693\n",
      "Epoch 31, Iter 19700, D Loss: -0.0015, G Loss: 0.0304, L1: 0.6623, L2: 0.8187\n",
      "Mean L1 Loss: 0.599068, Mean L2 Loss: 0.726811\n",
      "Epoch 31, Iter 19800, D Loss: -0.0016, G Loss: 0.0285, L1: 0.5991, L2: 0.7268\n",
      "Mean L1 Loss: 0.573851, Mean L2 Loss: 0.670410\n",
      "Epoch 31, Iter 19900, D Loss: -0.0015, G Loss: 0.0452, L1: 0.5739, L2: 0.6704\n",
      "Mean L1 Loss: 0.638519, Mean L2 Loss: 0.779189\n",
      "Epoch 31, Iter 20000, D Loss: -0.0016, G Loss: 0.0558, L1: 0.6385, L2: 0.7792\n",
      "Epoch 31 - D Loss: -0.0016, G Loss: 0.0558\n",
      "Mean L1 Loss: 0.620541, Mean L2 Loss: 0.796793\n",
      "Epoch 32, Iter 20100, D Loss: -0.0020, G Loss: -0.0343, L1: 0.6205, L2: 0.7968\n",
      "Mean L1 Loss: 0.593970, Mean L2 Loss: 0.726819\n",
      "Epoch 32, Iter 20200, D Loss: -0.0013, G Loss: 0.0251, L1: 0.5940, L2: 0.7268\n",
      "Mean L1 Loss: 0.573693, Mean L2 Loss: 0.704700\n",
      "Epoch 32, Iter 20300, D Loss: -0.0010, G Loss: 0.0619, L1: 0.5737, L2: 0.7047\n",
      "Mean L1 Loss: 0.527806, Mean L2 Loss: 0.597575\n",
      "Epoch 32, Iter 20400, D Loss: -0.0010, G Loss: 0.1029, L1: 0.5278, L2: 0.5976\n",
      "Saved best model with L2: 0.5976\n",
      "Mean L1 Loss: 0.573692, Mean L2 Loss: 0.674774\n",
      "Epoch 32, Iter 20500, D Loss: -0.0013, G Loss: 0.1113, L1: 0.5737, L2: 0.6748\n",
      "Mean L1 Loss: 0.561987, Mean L2 Loss: 0.639148\n",
      "Epoch 32, Iter 20600, D Loss: -0.0012, G Loss: 0.1075, L1: 0.5620, L2: 0.6391\n",
      "Epoch 32 - D Loss: -0.0013, G Loss: 0.1058\n",
      "Mean L1 Loss: 0.539519, Mean L2 Loss: 0.623437\n",
      "Epoch 33, Iter 20700, D Loss: -0.0014, G Loss: 0.0214, L1: 0.5395, L2: 0.6234\n",
      "Mean L1 Loss: 0.584435, Mean L2 Loss: 0.690801\n",
      "Epoch 33, Iter 20800, D Loss: -0.0007, G Loss: 0.0723, L1: 0.5844, L2: 0.6908\n",
      "Mean L1 Loss: 0.533492, Mean L2 Loss: 0.617936\n",
      "Epoch 33, Iter 20900, D Loss: -0.0006, G Loss: 0.0600, L1: 0.5335, L2: 0.6179\n",
      "Mean L1 Loss: 0.562253, Mean L2 Loss: 0.645289\n",
      "Epoch 33, Iter 21000, D Loss: -0.0007, G Loss: 0.0397, L1: 0.5623, L2: 0.6453\n",
      "Mean L1 Loss: 0.641602, Mean L2 Loss: 0.798081\n",
      "Epoch 33, Iter 21100, D Loss: -0.0008, G Loss: 0.0234, L1: 0.6416, L2: 0.7981\n",
      "Mean L1 Loss: 0.541842, Mean L2 Loss: 0.595835\n",
      "Epoch 33, Iter 21200, D Loss: -0.0008, G Loss: 0.0204, L1: 0.5418, L2: 0.5958\n",
      "Saved best model with L2: 0.5958\n",
      "Epoch 33 - D Loss: -0.0008, G Loss: 0.0227\n",
      "Mean L1 Loss: 0.609711, Mean L2 Loss: 0.709805\n",
      "Epoch 34, Iter 21300, D Loss: -0.0014, G Loss: 0.1193, L1: 0.6097, L2: 0.7098\n",
      "Mean L1 Loss: 0.555594, Mean L2 Loss: 0.624913\n",
      "Epoch 34, Iter 21400, D Loss: -0.0003, G Loss: 0.1913, L1: 0.5556, L2: 0.6249\n",
      "Mean L1 Loss: 0.534048, Mean L2 Loss: 0.595773\n",
      "Epoch 34, Iter 21500, D Loss: -0.0004, G Loss: 0.1756, L1: 0.5340, L2: 0.5958\n",
      "Saved best model with L2: 0.5958\n",
      "Mean L1 Loss: 0.557389, Mean L2 Loss: 0.631543\n",
      "Epoch 34, Iter 21600, D Loss: -0.0006, G Loss: 0.1522, L1: 0.5574, L2: 0.6315\n",
      "Mean L1 Loss: 0.534307, Mean L2 Loss: 0.610831\n",
      "Epoch 34, Iter 21700, D Loss: -0.0008, G Loss: 0.1339, L1: 0.5343, L2: 0.6108\n",
      "Mean L1 Loss: 0.547613, Mean L2 Loss: 0.615566\n",
      "Epoch 34, Iter 21800, D Loss: -0.0010, G Loss: 0.1271, L1: 0.5476, L2: 0.6156\n",
      "Epoch 34 - D Loss: -0.0010, G Loss: 0.1461\n",
      "Mean L1 Loss: 0.535248, Mean L2 Loss: 0.598905\n",
      "Epoch 35, Iter 21900, D Loss: -0.0009, G Loss: 0.3327, L1: 0.5352, L2: 0.5989\n",
      "Mean L1 Loss: 0.631646, Mean L2 Loss: 0.750063\n",
      "Epoch 35, Iter 22000, D Loss: -0.0015, G Loss: 0.3593, L1: 0.6316, L2: 0.7501\n",
      "Mean L1 Loss: 0.568166, Mean L2 Loss: 0.644374\n",
      "Epoch 35, Iter 22100, D Loss: -0.0012, G Loss: 0.3676, L1: 0.5682, L2: 0.6444\n",
      "Mean L1 Loss: 0.543096, Mean L2 Loss: 0.618166\n",
      "Epoch 35, Iter 22200, D Loss: -0.0013, G Loss: 0.3074, L1: 0.5431, L2: 0.6182\n",
      "Mean L1 Loss: 0.580387, Mean L2 Loss: 0.689973\n",
      "Epoch 35, Iter 22300, D Loss: -0.0012, G Loss: 0.2920, L1: 0.5804, L2: 0.6900\n",
      "Mean L1 Loss: 0.573927, Mean L2 Loss: 0.691554\n",
      "Epoch 35, Iter 22400, D Loss: -0.0011, G Loss: 0.3099, L1: 0.5739, L2: 0.6916\n",
      "Mean L1 Loss: 0.593255, Mean L2 Loss: 0.691235\n",
      "Epoch 35, Iter 22500, D Loss: -0.0010, G Loss: 0.3287, L1: 0.5933, L2: 0.6912\n",
      "Epoch 35 - D Loss: -0.0010, G Loss: 0.3287\n",
      "Mean L1 Loss: 0.563178, Mean L2 Loss: 0.667368\n",
      "Epoch 36, Iter 22600, D Loss: -0.0014, G Loss: 0.4024, L1: 0.5632, L2: 0.6674\n",
      "Mean L1 Loss: 0.566635, Mean L2 Loss: 0.669039\n",
      "Epoch 36, Iter 22700, D Loss: -0.0013, G Loss: 0.3576, L1: 0.5666, L2: 0.6690\n",
      "Mean L1 Loss: 0.595063, Mean L2 Loss: 0.723703\n",
      "Epoch 36, Iter 22800, D Loss: -0.0012, G Loss: 0.3078, L1: 0.5951, L2: 0.7237\n",
      "Mean L1 Loss: 0.506426, Mean L2 Loss: 0.560275\n",
      "Epoch 36, Iter 22900, D Loss: -0.0009, G Loss: 0.2399, L1: 0.5064, L2: 0.5603\n",
      "Saved best model with L2: 0.5603\n",
      "Mean L1 Loss: 0.554971, Mean L2 Loss: 0.650528\n",
      "Epoch 36, Iter 23000, D Loss: -0.0007, G Loss: 0.2106, L1: 0.5550, L2: 0.6505\n",
      "Mean L1 Loss: 0.519220, Mean L2 Loss: 0.587359\n",
      "Epoch 36, Iter 23100, D Loss: -0.0009, G Loss: 0.2041, L1: 0.5192, L2: 0.5874\n",
      "Epoch 36 - D Loss: -0.0010, G Loss: 0.1989\n",
      "Mean L1 Loss: 0.532553, Mean L2 Loss: 0.605354\n",
      "Epoch 37, Iter 23200, D Loss: -0.0005, G Loss: -0.0049, L1: 0.5326, L2: 0.6054\n",
      "Mean L1 Loss: 0.531279, Mean L2 Loss: 0.585870\n",
      "Epoch 37, Iter 23300, D Loss: -0.0008, G Loss: 0.0466, L1: 0.5313, L2: 0.5859\n",
      "Mean L1 Loss: 0.560553, Mean L2 Loss: 0.644068\n",
      "Epoch 37, Iter 23400, D Loss: -0.0008, G Loss: 0.0456, L1: 0.5606, L2: 0.6441\n",
      "Mean L1 Loss: 0.541554, Mean L2 Loss: 0.599134\n",
      "Epoch 37, Iter 23500, D Loss: -0.0007, G Loss: 0.0267, L1: 0.5416, L2: 0.5991\n",
      "Mean L1 Loss: 0.553843, Mean L2 Loss: 0.614253\n",
      "Epoch 37, Iter 23600, D Loss: -0.0006, G Loss: 0.0220, L1: 0.5538, L2: 0.6143\n",
      "Mean L1 Loss: 0.528743, Mean L2 Loss: 0.592289\n",
      "Epoch 37, Iter 23700, D Loss: -0.0007, G Loss: 0.0364, L1: 0.5287, L2: 0.5923\n",
      "Epoch 37 - D Loss: -0.0006, G Loss: 0.0524\n",
      "Mean L1 Loss: 0.532778, Mean L2 Loss: 0.607542\n",
      "Epoch 38, Iter 23800, D Loss: 0.0002, G Loss: 0.1387, L1: 0.5328, L2: 0.6075\n",
      "Mean L1 Loss: 0.519200, Mean L2 Loss: 0.585389\n",
      "Epoch 38, Iter 23900, D Loss: 0.0001, G Loss: 0.1503, L1: 0.5192, L2: 0.5854\n",
      "Mean L1 Loss: 0.560225, Mean L2 Loss: 0.642251\n",
      "Epoch 38, Iter 24000, D Loss: -0.0005, G Loss: 0.1359, L1: 0.5602, L2: 0.6423\n",
      "Mean L1 Loss: 0.640493, Mean L2 Loss: 0.810529\n",
      "Epoch 38, Iter 24100, D Loss: -0.0007, G Loss: 0.1398, L1: 0.6405, L2: 0.8105\n",
      "Mean L1 Loss: 0.566389, Mean L2 Loss: 0.678787\n",
      "Epoch 38, Iter 24200, D Loss: -0.0007, G Loss: 0.1395, L1: 0.5664, L2: 0.6788\n",
      "Mean L1 Loss: 0.582152, Mean L2 Loss: 0.681609\n",
      "Epoch 38, Iter 24300, D Loss: -0.0008, G Loss: 0.1635, L1: 0.5822, L2: 0.6816\n",
      "Epoch 38 - D Loss: -0.0011, G Loss: 0.1574\n",
      "Mean L1 Loss: 0.500164, Mean L2 Loss: 0.546428\n",
      "Epoch 39, Iter 24400, D Loss: -0.0025, G Loss: 0.0439, L1: 0.5002, L2: 0.5464\n",
      "Saved best model with L2: 0.5464\n",
      "Mean L1 Loss: 0.606861, Mean L2 Loss: 0.733213\n",
      "Epoch 39, Iter 24500, D Loss: -0.0004, G Loss: 0.0401, L1: 0.6069, L2: 0.7332\n",
      "Mean L1 Loss: 0.541472, Mean L2 Loss: 0.610283\n",
      "Epoch 39, Iter 24600, D Loss: -0.0006, G Loss: 0.0666, L1: 0.5415, L2: 0.6103\n",
      "Mean L1 Loss: 0.492479, Mean L2 Loss: 0.538817\n",
      "Epoch 39, Iter 24700, D Loss: -0.0007, G Loss: 0.0645, L1: 0.4925, L2: 0.5388\n",
      "Saved best model with L2: 0.5388\n",
      "Mean L1 Loss: 0.503189, Mean L2 Loss: 0.544235\n",
      "Epoch 39, Iter 24800, D Loss: -0.0010, G Loss: 0.0460, L1: 0.5032, L2: 0.5442\n",
      "Mean L1 Loss: 0.605053, Mean L2 Loss: 0.777134\n",
      "Epoch 39, Iter 24900, D Loss: -0.0011, G Loss: 0.0103, L1: 0.6051, L2: 0.7771\n",
      "Mean L1 Loss: 0.511116, Mean L2 Loss: 0.541627\n",
      "Epoch 39, Iter 25000, D Loss: -0.0009, G Loss: -0.0032, L1: 0.5111, L2: 0.5416\n",
      "Epoch 39 - D Loss: -0.0009, G Loss: -0.0032\n",
      "Mean L1 Loss: 0.513629, Mean L2 Loss: 0.558232\n",
      "Epoch 40, Iter 25100, D Loss: -0.0008, G Loss: -0.0460, L1: 0.5136, L2: 0.5582\n",
      "Mean L1 Loss: 0.494278, Mean L2 Loss: 0.524427\n",
      "Epoch 40, Iter 25200, D Loss: -0.0010, G Loss: -0.0868, L1: 0.4943, L2: 0.5244\n",
      "Saved best model with L2: 0.5244\n",
      "Mean L1 Loss: 0.514481, Mean L2 Loss: 0.561030\n",
      "Epoch 40, Iter 25300, D Loss: -0.0001, G Loss: -0.0585, L1: 0.5145, L2: 0.5610\n",
      "Mean L1 Loss: 0.521218, Mean L2 Loss: 0.574876\n",
      "Epoch 40, Iter 25400, D Loss: -0.0002, G Loss: -0.0239, L1: 0.5212, L2: 0.5749\n",
      "Mean L1 Loss: 0.519668, Mean L2 Loss: 0.568941\n",
      "Epoch 40, Iter 25500, D Loss: -0.0002, G Loss: 0.0286, L1: 0.5197, L2: 0.5689\n",
      "Mean L1 Loss: 0.541586, Mean L2 Loss: 0.621760\n",
      "Epoch 40, Iter 25600, D Loss: -0.0003, G Loss: 0.0789, L1: 0.5416, L2: 0.6218\n",
      "Epoch 40 - D Loss: -0.0003, G Loss: 0.0860\n",
      "Mean L1 Loss: 0.506491, Mean L2 Loss: 0.565634\n",
      "Epoch 41, Iter 25700, D Loss: -0.0004, G Loss: 0.3051, L1: 0.5065, L2: 0.5656\n",
      "Mean L1 Loss: 0.537388, Mean L2 Loss: 0.601383\n",
      "Epoch 41, Iter 25800, D Loss: 0.0007, G Loss: 0.3392, L1: 0.5374, L2: 0.6014\n",
      "Mean L1 Loss: 0.553416, Mean L2 Loss: 0.637070\n",
      "Epoch 41, Iter 25900, D Loss: 0.0006, G Loss: 0.2964, L1: 0.5534, L2: 0.6371\n",
      "Mean L1 Loss: 0.577954, Mean L2 Loss: 0.636239\n",
      "Epoch 41, Iter 26000, D Loss: 0.0002, G Loss: 0.2559, L1: 0.5780, L2: 0.6362\n",
      "Mean L1 Loss: 0.573944, Mean L2 Loss: 0.677203\n",
      "Epoch 41, Iter 26100, D Loss: 0.0000, G Loss: 0.2454, L1: 0.5739, L2: 0.6772\n",
      "Mean L1 Loss: 0.512388, Mean L2 Loss: 0.561570\n",
      "Epoch 41, Iter 26200, D Loss: -0.0001, G Loss: 0.2575, L1: 0.5124, L2: 0.5616\n",
      "Epoch 41 - D Loss: -0.0002, G Loss: 0.2630\n",
      "Mean L1 Loss: 0.477908, Mean L2 Loss: 0.533518\n",
      "Epoch 42, Iter 26300, D Loss: -0.0001, G Loss: 0.2512, L1: 0.4779, L2: 0.5335\n",
      "Mean L1 Loss: 0.520299, Mean L2 Loss: 0.575366\n",
      "Epoch 42, Iter 26400, D Loss: -0.0007, G Loss: 0.2165, L1: 0.5203, L2: 0.5754\n",
      "Mean L1 Loss: 0.558865, Mean L2 Loss: 0.611565\n",
      "Epoch 42, Iter 26500, D Loss: -0.0007, G Loss: 0.2199, L1: 0.5589, L2: 0.6116\n",
      "Mean L1 Loss: 0.569179, Mean L2 Loss: 0.658710\n",
      "Epoch 42, Iter 26600, D Loss: -0.0005, G Loss: 0.2359, L1: 0.5692, L2: 0.6587\n",
      "Mean L1 Loss: 0.602176, Mean L2 Loss: 0.712642\n",
      "Epoch 42, Iter 26700, D Loss: -0.0005, G Loss: 0.2541, L1: 0.6022, L2: 0.7126\n",
      "Mean L1 Loss: 0.495977, Mean L2 Loss: 0.557165\n",
      "Epoch 42, Iter 26800, D Loss: -0.0007, G Loss: 0.2523, L1: 0.4960, L2: 0.5572\n",
      "Epoch 42 - D Loss: -0.0007, G Loss: 0.2449\n",
      "Mean L1 Loss: 0.505850, Mean L2 Loss: 0.561535\n",
      "Epoch 43, Iter 26900, D Loss: -0.0002, G Loss: 0.1801, L1: 0.5058, L2: 0.5615\n",
      "Mean L1 Loss: 0.488244, Mean L2 Loss: 0.550242\n",
      "Epoch 43, Iter 27000, D Loss: -0.0001, G Loss: 0.1179, L1: 0.4882, L2: 0.5502\n",
      "Mean L1 Loss: 0.521876, Mean L2 Loss: 0.621850\n",
      "Epoch 43, Iter 27100, D Loss: -0.0003, G Loss: 0.1281, L1: 0.5219, L2: 0.6218\n",
      "Mean L1 Loss: 0.508200, Mean L2 Loss: 0.566212\n",
      "Epoch 43, Iter 27200, D Loss: -0.0004, G Loss: 0.1712, L1: 0.5082, L2: 0.5662\n",
      "Mean L1 Loss: 0.469678, Mean L2 Loss: 0.503421\n",
      "Epoch 43, Iter 27300, D Loss: -0.0006, G Loss: 0.1622, L1: 0.4697, L2: 0.5034\n",
      "Saved best model with L2: 0.5034\n",
      "Mean L1 Loss: 0.587976, Mean L2 Loss: 0.655857\n",
      "Epoch 43, Iter 27400, D Loss: -0.0006, G Loss: 0.1614, L1: 0.5880, L2: 0.6559\n",
      "Mean L1 Loss: 0.595857, Mean L2 Loss: 0.680944\n",
      "Epoch 43, Iter 27500, D Loss: -0.0004, G Loss: 0.1540, L1: 0.5959, L2: 0.6809\n",
      "Epoch 43 - D Loss: -0.0004, G Loss: 0.1540\n",
      "Mean L1 Loss: 0.507144, Mean L2 Loss: 0.550748\n",
      "Epoch 44, Iter 27600, D Loss: -0.0014, G Loss: 0.0509, L1: 0.5071, L2: 0.5507\n",
      "Mean L1 Loss: 0.497984, Mean L2 Loss: 0.530270\n",
      "Epoch 44, Iter 27700, D Loss: -0.0008, G Loss: 0.0327, L1: 0.4980, L2: 0.5303\n",
      "Mean L1 Loss: 0.579724, Mean L2 Loss: 0.703771\n",
      "Epoch 44, Iter 27800, D Loss: -0.0007, G Loss: 0.0506, L1: 0.5797, L2: 0.7038\n",
      "Mean L1 Loss: 0.704389, Mean L2 Loss: 0.851501\n",
      "Epoch 44, Iter 27900, D Loss: -0.0007, G Loss: 0.0751, L1: 0.7044, L2: 0.8515\n",
      "Mean L1 Loss: 0.497745, Mean L2 Loss: 0.526413\n",
      "Epoch 44, Iter 28000, D Loss: -0.0008, G Loss: 0.0743, L1: 0.4977, L2: 0.5264\n",
      "Mean L1 Loss: 0.500126, Mean L2 Loss: 0.544983\n",
      "Epoch 44, Iter 28100, D Loss: -0.0006, G Loss: 0.0841, L1: 0.5001, L2: 0.5450\n",
      "Epoch 44 - D Loss: -0.0006, G Loss: 0.0878\n",
      "Mean L1 Loss: 0.481319, Mean L2 Loss: 0.501279\n",
      "Epoch 45, Iter 28200, D Loss: 0.0001, G Loss: 0.1797, L1: 0.4813, L2: 0.5013\n",
      "Saved best model with L2: 0.5013\n",
      "Mean L1 Loss: 0.509040, Mean L2 Loss: 0.590938\n",
      "Epoch 45, Iter 28300, D Loss: -0.0002, G Loss: 0.1593, L1: 0.5090, L2: 0.5909\n",
      "Mean L1 Loss: 0.495462, Mean L2 Loss: 0.529269\n",
      "Epoch 45, Iter 28400, D Loss: -0.0005, G Loss: 0.1173, L1: 0.4955, L2: 0.5293\n",
      "Mean L1 Loss: 0.495672, Mean L2 Loss: 0.529667\n",
      "Epoch 45, Iter 28500, D Loss: -0.0001, G Loss: 0.1047, L1: 0.4957, L2: 0.5297\n",
      "Mean L1 Loss: 0.583632, Mean L2 Loss: 0.717614\n",
      "Epoch 45, Iter 28600, D Loss: 0.0002, G Loss: 0.1073, L1: 0.5836, L2: 0.7176\n",
      "Mean L1 Loss: 0.469437, Mean L2 Loss: 0.521869\n",
      "Epoch 45, Iter 28700, D Loss: 0.0001, G Loss: 0.0741, L1: 0.4694, L2: 0.5219\n",
      "Epoch 45 - D Loss: 0.0003, G Loss: 0.0570\n",
      "Mean L1 Loss: 0.473777, Mean L2 Loss: 0.492309\n",
      "Epoch 46, Iter 28800, D Loss: 0.0028, G Loss: -0.1226, L1: 0.4738, L2: 0.4923\n",
      "Saved best model with L2: 0.4923\n",
      "Mean L1 Loss: 0.553831, Mean L2 Loss: 0.650426\n",
      "Epoch 46, Iter 28900, D Loss: 0.0015, G Loss: -0.0563, L1: 0.5538, L2: 0.6504\n",
      "Mean L1 Loss: 0.459203, Mean L2 Loss: 0.490657\n",
      "Epoch 46, Iter 29000, D Loss: 0.0009, G Loss: -0.0801, L1: 0.4592, L2: 0.4907\n",
      "Saved best model with L2: 0.4907\n",
      "Mean L1 Loss: 0.558655, Mean L2 Loss: 0.591019\n",
      "Epoch 46, Iter 29100, D Loss: 0.0009, G Loss: -0.0923, L1: 0.5587, L2: 0.5910\n",
      "Mean L1 Loss: 0.465593, Mean L2 Loss: 0.493617\n",
      "Epoch 46, Iter 29200, D Loss: 0.0008, G Loss: -0.0882, L1: 0.4656, L2: 0.4936\n",
      "Mean L1 Loss: 0.456202, Mean L2 Loss: 0.489395\n",
      "Epoch 46, Iter 29300, D Loss: 0.0009, G Loss: -0.0857, L1: 0.4562, L2: 0.4894\n",
      "Saved best model with L2: 0.4894\n",
      "Epoch 46 - D Loss: 0.0011, G Loss: -0.0725\n",
      "Mean L1 Loss: 0.481091, Mean L2 Loss: 0.514811\n",
      "Epoch 47, Iter 29400, D Loss: -0.0016, G Loss: 0.0101, L1: 0.4811, L2: 0.5148\n",
      "Mean L1 Loss: 0.470132, Mean L2 Loss: 0.493282\n",
      "Epoch 47, Iter 29500, D Loss: -0.0001, G Loss: -0.0383, L1: 0.4701, L2: 0.4933\n",
      "Mean L1 Loss: 0.489963, Mean L2 Loss: 0.507801\n",
      "Epoch 47, Iter 29600, D Loss: 0.0003, G Loss: -0.0500, L1: 0.4900, L2: 0.5078\n",
      "Mean L1 Loss: 0.476958, Mean L2 Loss: 0.508404\n",
      "Epoch 47, Iter 29700, D Loss: 0.0005, G Loss: -0.0121, L1: 0.4770, L2: 0.5084\n",
      "Mean L1 Loss: 0.477324, Mean L2 Loss: 0.513382\n",
      "Epoch 47, Iter 29800, D Loss: 0.0005, G Loss: 0.0084, L1: 0.4773, L2: 0.5134\n",
      "Mean L1 Loss: 0.474518, Mean L2 Loss: 0.528351\n",
      "Epoch 47, Iter 29900, D Loss: 0.0005, G Loss: 0.0192, L1: 0.4745, L2: 0.5284\n",
      "Mean L1 Loss: 0.534528, Mean L2 Loss: 0.620037\n",
      "Epoch 47, Iter 30000, D Loss: 0.0005, G Loss: 0.0267, L1: 0.5345, L2: 0.6200\n",
      "Epoch 47 - D Loss: 0.0005, G Loss: 0.0267\n",
      "Mean L1 Loss: 0.531002, Mean L2 Loss: 0.584832\n",
      "Epoch 48, Iter 30100, D Loss: 0.0005, G Loss: 0.0016, L1: 0.5310, L2: 0.5848\n",
      "Mean L1 Loss: 0.453393, Mean L2 Loss: 0.477572\n",
      "Epoch 48, Iter 30200, D Loss: 0.0006, G Loss: 0.0299, L1: 0.4534, L2: 0.4776\n",
      "Saved best model with L2: 0.4776\n",
      "Mean L1 Loss: 0.559854, Mean L2 Loss: 0.618892\n",
      "Epoch 48, Iter 30300, D Loss: 0.0006, G Loss: 0.0650, L1: 0.5599, L2: 0.6189\n",
      "Mean L1 Loss: 0.485491, Mean L2 Loss: 0.530672\n",
      "Epoch 48, Iter 30400, D Loss: 0.0006, G Loss: 0.1071, L1: 0.4855, L2: 0.5307\n",
      "Mean L1 Loss: 0.476402, Mean L2 Loss: 0.514694\n",
      "Epoch 48, Iter 30500, D Loss: 0.0006, G Loss: 0.1174, L1: 0.4764, L2: 0.5147\n",
      "Mean L1 Loss: 0.452692, Mean L2 Loss: 0.471538\n",
      "Epoch 48, Iter 30600, D Loss: 0.0004, G Loss: 0.1319, L1: 0.4527, L2: 0.4715\n",
      "Saved best model with L2: 0.4715\n",
      "Epoch 48 - D Loss: 0.0005, G Loss: 0.1334\n",
      "Mean L1 Loss: 0.473897, Mean L2 Loss: 0.505297\n",
      "Epoch 49, Iter 30700, D Loss: -0.0001, G Loss: 0.2086, L1: 0.4739, L2: 0.5053\n",
      "Mean L1 Loss: 0.520987, Mean L2 Loss: 0.592459\n",
      "Epoch 49, Iter 30800, D Loss: 0.0008, G Loss: 0.2834, L1: 0.5210, L2: 0.5925\n",
      "Mean L1 Loss: 0.459730, Mean L2 Loss: 0.485602\n",
      "Epoch 49, Iter 30900, D Loss: 0.0004, G Loss: 0.3241, L1: 0.4597, L2: 0.4856\n",
      "Mean L1 Loss: 0.467240, Mean L2 Loss: 0.508293\n",
      "Epoch 49, Iter 31000, D Loss: 0.0007, G Loss: 0.3654, L1: 0.4672, L2: 0.5083\n",
      "Mean L1 Loss: 0.547819, Mean L2 Loss: 0.617081\n",
      "Epoch 49, Iter 31100, D Loss: 0.0007, G Loss: 0.3804, L1: 0.5478, L2: 0.6171\n",
      "Mean L1 Loss: 0.626568, Mean L2 Loss: 0.746455\n",
      "Epoch 49, Iter 31200, D Loss: 0.0005, G Loss: 0.4104, L1: 0.6266, L2: 0.7465\n",
      "Epoch 49 - D Loss: 0.0005, G Loss: 0.4266\n"
     ]
    }
   ],
   "source": [
    "trained_G, trained_D = train_WGR_fnn(D=D_net, G=G_net, D_solver=D_solver, G_solver=G_solver, loader_train = loader_train, \n",
    "                                     loader_val=loader_val, noise_dim=args.noise_dim, Xdim=args.Xdim, Ydim=args.Ydim, J_size=100, \n",
    "                                     lambda_w=0.1, lambda_l=0.9, batch_size=args.train_batch, save_path='./', start_eva=2000,  eva_iter = 100,\n",
    "                                     model_type='CT', device='cpu', num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21840938-791f-47b3-94cd-41726ee015b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: 0.4530401825904846\n",
      "L2 Loss: 0.4241640567779541\n",
      "CP: 0.9598000049591064\n",
      "PI length: 3.064796209335327\n",
      "std of LBE: 0.3187739849090576\n",
      "std of UBE: 0.3126869201660156\n"
     ]
    }
   ],
   "source": [
    "CT_numerical_Results = eva_G_UniY(G=G_net, x=X_test, y=y_test, noise_dim=args.noise_dim, test_size=args.test,  J_t_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87df8792-5fcd-4aa4-8a2a-76eee50994eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: 0.4902079105377197\n",
      "L2 Loss: 0.5055880546569824\n",
      "CP: 0.9435999989509583\n",
      "PI length: 3.4678492546081543\n",
      "std of LBE: 1.3118330240249634\n",
      "std of UBE: 1.1575133800506592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(0.4902079, dtype=float32),\n",
       " array(0.50558805, dtype=float32),\n",
       " array(0.9436, dtype=float32),\n",
       " array(3.4678493, dtype=float32),\n",
       " array(1.311833, dtype=float32),\n",
       " array(1.1575134, dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_G_UniY_new(G=G_net, x=X_test, y=y_test, noise_dim=args.noise_dim, test_size=args.test,  J_t_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7e4c88d-d8ef-4e16-b3c4-a8d900729186",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c70d200-00f4-4635-8834-ac0982950ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 384])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:5000,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "060049e9-10ac-4eea-a88b-ecdf34f07c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: 0.4587428569793701\n",
      "L2 Loss: 0.4520159661769867\n",
      "CP: 0.9595999717712402\n",
      "PI length: 3.078183889389038\n",
      "std of LBE: 0.9511776566505432\n",
      "std of UBE: 0.9317697286605835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(0.45874286, dtype=float32),\n",
       " array(0.45201597, dtype=float32),\n",
       " array(0.9596, dtype=float32),\n",
       " array(3.078184, dtype=float32),\n",
       " array(0.95117766, dtype=float32),\n",
       " array(0.9317697, dtype=float32))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_G_UniY_new(G=G_net, x=X_test[0:5000,], y=y_test[0:5000], noise_dim=args.noise_dim, test_size=5000,  J_t_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41153d22-010d-4abe-b6fc-b7951da7acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: 0.4333565831184387\n",
      "L2 Loss: 0.4567296504974365\n",
      "CP: 0.9323999881744385\n",
      "PI length: 2.452061414718628\n",
      "std of LBE: 0.7886634469032288\n",
      "std of UBE: 0.8261491060256958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(0.43335658, dtype=float32),\n",
       " array(0.45672965, dtype=float32),\n",
       " array(0.9324, dtype=float32),\n",
       " array(2.4520614, dtype=float32),\n",
       " array(0.78866345, dtype=float32),\n",
       " array(0.8261491, dtype=float32))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva_G_UniY_new(G=trained_G, noise_dim=args.noise_dim,  J_t_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c4559140-7292-4430-b41f-93165cf44e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_output = torch.zeros([500, args.test])\n",
    "for i in range(500):\n",
    "    eta = sample_noise(args.test, args.noise_dim)\n",
    "    g_input = torch.cat([X_test,eta],dim=1).float()\n",
    "    g_output[i] = G_net(g_input).view(args.test).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2d41902-c2c6-415c-93fa-8e6027b1a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_LB = g_output.quantile(0.025, axis=0)\n",
    "g_UB = g_output.quantile(0.975, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd3fc576-6238-482c-addc-c569abec7f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4179, 2.3069, 1.9637,  ..., 2.0179, 0.5045, 3.4088])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test-g_LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e384fac2-91de-42b8-9312-74f084ae6fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7665, 2.0949, 1.6445,  ..., 4.5945, 1.3378, 3.9293])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_UB-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "240b35c9-385b-4e8e-8097-4591b06abcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38.6398, 64.2331, 53.0171,  ..., 93.5578, 33.6407, 93.9192])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa77efdd-b90f-4e2a-8df3-9fcac2f76e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.linspace(0, 1, steps=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d34e0d7f-f948-4332-8a0d-511174c27f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7211, 12.8678, 18.4893, 24.2614, 27.7181, 29.9940, 32.5229, 34.8442,\n",
       "        37.6388, 40.6592, 44.0237, 47.4046, 51.1483, 54.6254, 58.6954, 64.1456,\n",
       "        69.6098, 75.1451, 81.1023, 86.6044, 96.8132])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile(y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d03b5a7c-28d5-4a14-ad88-45e650038d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.5154,  0.3057,  0.4973,  0.6242,  0.7241,  0.8207,  0.9084,  1.0035,\n",
       "         1.0966,  1.1997,  1.3075,  1.4310,  1.6070,  1.7803,  1.9994,  2.2255,\n",
       "         2.4490,  2.6816,  2.9976,  3.3895,  9.8460])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile(y_test-g_LB.view(args.test),probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81f73a46-e1e1-4a16-b19a-ac530b9d2d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.3065,  0.2309,  0.4437,  0.5909,  0.7160,  0.8175,  0.9082,  0.9951,\n",
       "         1.0839,  1.1792,  1.2819,  1.3926,  1.5221,  1.6734,  1.8560,  2.0665,\n",
       "         2.2786,  2.5544,  2.9004,  3.4798, 13.4798])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile(g_UB.view(args.test)-y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf951b49-6276-4185-aa50-24a746536494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.basic_utils import sample_noise, l1_loss, l2_loss\n",
    "def eva_G_UniY_new(G, x, y, noise_dim, test_size,  J_t_size=50):\n",
    "    \"\"\"\n",
    "    Evaluate the generator model on real data.\n",
    "    Since real-world data may contain outliers that significantly affect the standard deviation,\n",
    "    we evaluate the model on the entire test set at once rather than in batches.\n",
    "    Outliers are then removed, and the final results are reported.\n",
    "\n",
    "    \n",
    "    Parameters:\n",
    "        G (nn.Module): Generator model\n",
    "        x: Covariates used in the testing\n",
    "        y: Response used in the testing\n",
    "        noise_dim (int): Dimension of noise vector eta\n",
    "        test_size (int): The size of testing dataset\n",
    "        J_t_size (int): Number of samples to generate for each input\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Mean L1 loss, mean L2 loss, coverage probability, length of prediction interval, \n",
    "               standard deviation of upper bound error, standard deviation of lower bound error std\n",
    "    \"\"\"\n",
    "    \n",
    "    quantiles = [0.025, 0.975]  # Lower and upper bounds for 95% prediction interval\n",
    "\n",
    "    with torch.no_grad():\n",
    "        LB = torch.zeros(test_size)\n",
    "        UB = torch.zeros(test_size)\n",
    "\n",
    "        output = torch.zeros([J_t_size,test_size])\n",
    "        for i in range(J_t_size):\n",
    "            eta = sample_noise(test_size, noise_dim)\n",
    "            g_input = torch.cat([x,eta],dim=1).float()\n",
    "            output[i] = G(g_input).view(test_size).detach()\n",
    "\n",
    "        test_L1 = l1_loss( output.mean(dim=0), y )\n",
    "        test_L2 = l2_loss( output.mean(dim=0), y  )\n",
    "        CP_test = ( (y  >= output.quantile(quantiles[0],axis=0) ) & (y <= output.quantile(quantiles[1],axis=0) ) ).sum()/test_size\n",
    "        PI_test = torch.mean(torch.abs(output.quantile(quantiles[1],axis=0)  - output.quantile(quantiles[0],axis=0) ))\n",
    "\n",
    "\n",
    "        #compute lower bound error and upper bound error\n",
    "        LB = output.quantile(quantiles[0],axis=0)-y\n",
    "        UB = output.quantile(quantiles[1],axis=0)-y\n",
    "\n",
    "        LB_z_scores = (LB - LB.mean())/LB.std(unbiased=False)\n",
    "        UB_z_scores = (UB - UB.mean())/UB.std(unbiased=False)\n",
    "\n",
    "        filtered_LB = torch.abs(LB_z_scores)<3\n",
    "        filtered_UB = torch.abs(UB_z_scores)<3\n",
    "\n",
    "        LB_std = torch.std(torch.abs(LB[filtered_LB]))\n",
    "        UB_std = torch.std(torch.abs(UB[filtered_UB]))\n",
    "        \n",
    "        #print results\n",
    "        print(f\"L1 Loss: {test_L1}\")\n",
    "        print(f\"L2 Loss: {test_L2}\")\n",
    "        print(f\"CP: {CP_test}\")\n",
    "        print(f\"PI length: {PI_test}\")\n",
    "        print(f\"std of LBE: {LB_std}\")\n",
    "        print(f\"std of UBE: {UB_std}\")\n",
    "\n",
    "        return test_L1.detach().numpy(), test_L2.detach().numpy(),CP_test.detach().numpy(), PI_test.detach().numpy(), LB_std.detach().numpy(), UB_std.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f3beef3-e2a2-429b-b793-6560f6966fe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'torch.nn.modules.container.Sequential'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mG_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLSWG-40000-50m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/twPython/lib/python3.11/site-packages/torch/nn/modules/module.py:2513\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m \n\u001b[1;32m   2478\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2510\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2515\u001b[0m     )\n\u001b[1;32m   2517\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2518\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'torch.nn.modules.container.Sequential'>."
     ]
    }
   ],
   "source": [
    "G_net.load_state_dict(torch.load(\"G_CT_d384_m50_best.pth\", weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49fb56ee-44b8-47ec-880f-8529085ba4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = torch.load(\"LSWG-40000-50m.pth\",weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd534749-6585-40af-a953-941a28ccde4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=434, out_features=128, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7c2dfab-977f-4c44-8206-bb6500d12da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=433, out_features=128, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  (4): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ebf1013-30c4-427b-888f-07f4cf7208ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.validation_utils import val_G, val_G_image\n",
    "from data.SimulationData import generate_multi_responses_multiY\n",
    "from utils.basic_utils import setup_seed, sample_noise, calculate_gradient_penalty, discriminator_loss, generator_loss, l1_loss, l2_loss\n",
    "from utils.plot_utils import plot_kde_2d, convert_generated_to_mnist_range,  visualize_mnist_digits, visualize_digits \n",
    "\n",
    "def train_WGR_fnn(D, G, D_solver, G_solver, loader_train, loader_val, noise_dim, Xdim, Ydim, \n",
    "                  batch_size,  J_size=50, noise_distribution='gaussian', multivariate=False,\n",
    "                  lambda_w=0.9, lambda_l=0.1, save_path='./M1/', model_type=\"M1\", start_eva=1000,  eva_iter = 50,\n",
    "                  num_epochs=10, num_samples=100, device='cuda', lr_decay=None, \n",
    "                  lr_decay_step=5, lr_decay_gamma=0.1, save_last = False, is_plot=False, plot_iter=500):\n",
    "    \"\"\"\n",
    "    Train Wasserstein GAN Regression with Fully-Connected Neural Networks.\n",
    "    \n",
    "    Args:\n",
    "        D: Discriminator model\n",
    "        G: Generator model\n",
    "        D_solver: Discriminator optimizer\n",
    "        G_solver: Generator optimizer\n",
    "        loader_train: Data loader for training set\n",
    "        loader_val: Data loader for validation set\n",
    "        noise_dim: Dimension of noise vector\n",
    "        Xdim: Dimension of covariate X\n",
    "        Ydim: Dimension of response Y\n",
    "        batch_size: Batch size\n",
    "        J_size: Generator projection size (default: 50)\n",
    "        noise_distribution: Distribution for noise sampling (default: 'gaussian')\n",
    "        lambda_w: Weight for Wasserstein loss (default: 0.9)\n",
    "        lambda_l: Weight for L2 regularization (default: 0.1)\n",
    "        save_path: Path to save models (default: './M1/')\n",
    "        start_eva: Iteration to start evaluation (default: 1000)\n",
    "        eva_iter: to conduct the validation per iteration (default: 50)\n",
    "        num_epochs: Number of training epochs (default: 10)\n",
    "        num_samples: Number of noise samples generated for each data point in validation (default: 100)\n",
    "        device: Device to train on (default: 'cuda')\n",
    "        lr_decay: Learning rate decay strategy ('step', 'plateau', 'cosine', or None)\n",
    "        lr_decay_step: Step size for StepLR or patience for ReduceLROnPlateau\n",
    "        lr_decay_gamma: Multiplicative factor for learning rate decay\n",
    "        save_last: Whether to save the last trained network (default: False)\n",
    "        is_plot: Whether to conduct visualization (default: False)\n",
    "        plot_iter: to conduct the visualization per iteration (default: 500)\n",
    "    Returns:\n",
    "        tuple: Best validation scores and final models\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Move models to device\n",
    "    D = D.to(device)\n",
    "    G = G.to(device)\n",
    "    \n",
    "    # Initialize counters and metrics\n",
    "    iter_count = 0\n",
    "    l1_acc, l2_acc = val_G(G=G, loader_data=loader_val, noise_dim=noise_dim, Xdim=Xdim, Ydim=Ydim, num_samples=num_samples, device=device,  multivariate=multivariate )\n",
    "                         \n",
    "    # Save initial model state\n",
    "    best_acc = l2_acc\n",
    "    best_model_g = copy.deepcopy(G.state_dict())\n",
    "    best_model_d = copy.deepcopy(D.state_dict())\n",
    "    \n",
    "    # Initialize learning rate schedulers if requested\n",
    "    D_scheduler, G_scheduler = None, None\n",
    "    if lr_decay == 'step':\n",
    "        D_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            D_solver, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "        G_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            G_solver, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "    elif lr_decay == 'plateau':\n",
    "        D_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            D_solver, mode='min', factor=lr_decay_gamma, patience=lr_decay_step )\n",
    "        G_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            G_solver, mode='min', factor=lr_decay_gamma, patience=lr_decay_step )\n",
    "    elif lr_decay == 'cosine':\n",
    "        D_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            D_solver, T_max=num_epochs, eta_min=0)\n",
    "        G_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            G_solver, T_max=num_epochs, eta_min=0)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        D.train()\n",
    "        G.train()\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(loader_train):\n",
    "            if x.size(0) != batch_size:\n",
    "                continue\n",
    "                \n",
    "            # Move data to the appropriate device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Sample noise\n",
    "            eta = sample_noise(x.size(0), dim=noise_dim, \n",
    "                              distribution=noise_distribution).to(device)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            d_input = torch.cat([x.view(batch_size, Xdim), y.view(batch_size, Ydim)], dim=1)     \n",
    "            g_input = torch.cat([x.view(batch_size, Xdim), eta], dim=1)\n",
    "            \n",
    "            # ==================== Train Discriminator ====================\n",
    "            D_solver.zero_grad()\n",
    "            logits_real = D(d_input)\n",
    "            \n",
    "            fake_y = G(g_input).detach()\n",
    "            fake_images = torch.cat([x.view(batch_size, Xdim), fake_y.view(batch_size, Ydim)], dim=1)\n",
    "                \n",
    "            logits_fake = D(fake_images)\n",
    "            \n",
    "            penalty = calculate_gradient_penalty(D, d_input, fake_images, device)\n",
    "            d_error = discriminator_loss(logits_real, logits_fake) + 10 * penalty\n",
    "            d_error.backward()\n",
    "            D_solver.step()\n",
    "            d_losses.append(d_error.item())\n",
    "             \n",
    "            # ==================== Train Generator ====================\n",
    "            G_solver.zero_grad()\n",
    "            \n",
    "            # First: Standard WGAN loss\n",
    "            fake_y = G(g_input)\n",
    "            fake_images = torch.cat([x.view(batch_size, Xdim), fake_y.view(batch_size, Ydim)], dim=1)\n",
    "            logits_fake = D(fake_images)\n",
    "            g_error_w = generator_loss(logits_fake)\n",
    "\n",
    "            # Second: Generate multiple outputs and compute L2 loss against expected y\n",
    "            if lambda_l>0:  #if lambda_l = 0, then it becomes the standard cWGAN\n",
    "                # Initialize output tensor with dimensions that work for both cases\n",
    "                g_output = torch.zeros([J_size, batch_size, max(1, Ydim)], device=device)\n",
    "                for i in range(J_size):\n",
    "                    eta = sample_noise(x.size(0), noise_dim, distribution=noise_distribution).to(device)\n",
    "                    g_input = torch.cat([x.view(batch_size, Xdim), eta], dim=1)\n",
    "                    output = G(g_input)\n",
    "                    g_output[i] = output.view(batch_size, -1)  # Reshape to [batch_size, Ydim] or [batch_size, 1]\n",
    "                    \n",
    "                # Reshape final result if Ydim=1 to match expected dimensions\n",
    "                if Ydim == 1:\n",
    "                    g_output = g_output.squeeze(-1)  # Remove the last dimension to get [J_size, batch_size]\n",
    "                    # For univariate output, compute mean squared error directly\n",
    "                    g_error_l = torch.mean((g_output.mean(dim=0) - y.view(batch_size))**2)\n",
    "                else:\n",
    "                    # For multivariate output, use MSE loss function\n",
    "                    g_error_l = torch.mean(torch.sum((g_output.mean(dim=0) - y)**2, dim=1))\n",
    "            else: \n",
    "                g_error_l = 0 \n",
    "\n",
    "            \n",
    "            \n",
    "            #y_reshaped = y.view(batch_size, -1)  # Reshape to [batch_size, Ydim] or [batch_size, 1]\n",
    "            #g_error_l = torch.mean((mean_pred - y_reshaped)**2)\n",
    "\n",
    "            # Combined loss with wasserstein and L2 regularization\n",
    "            g_error = lambda_w * g_error_w + lambda_l * g_error_l\n",
    "          \n",
    "            g_error.backward()\n",
    "            G_solver.step()\n",
    "            g_losses.append(g_error.item())\n",
    "            \n",
    "            # Increment iteration counter\n",
    "            iter_count += 1\n",
    "\n",
    "\n",
    "            # Validate and save best model\n",
    "            if (iter_count >= start_eva) and (iter_count % eva_iter == 0):\n",
    "                l1_acc, l2_acc = val_G(G=G, loader_data=loader_val, noise_dim=noise_dim, Xdim=Xdim,  Ydim=Ydim, num_samples=num_samples, device=device,  multivariate=multivariate )\n",
    "                \n",
    "                print(f\"Epoch {epoch}, Iter {iter_count}, \"\n",
    "                      f\"D Loss: {np.mean(d_losses):.4f}, G Loss: {np.mean(g_losses):.4f}, \"\n",
    "                      f\"L1: {l1_acc:.4f}, L2: {l2_acc:.4f}\")\n",
    "\n",
    "                \n",
    "                # Save model if validation improves\n",
    "                if (Ydim==1) and (l2_acc < best_acc):\n",
    "                    best_acc = l2_acc\n",
    "                    best_model_g = copy.deepcopy(G.state_dict())\n",
    "                    best_model_d = copy.deepcopy(D.state_dict())\n",
    "\n",
    "                    # Save models\n",
    "                    torch.save(G.state_dict(), f\"{save_path}/G_\"+model_type+\"_d\"+str(Xdim)+\"_m\"+str(noise_dim)+\"_best.pth\")\n",
    "                    torch.save(D.state_dict(), f\"{save_path}/D_\"+model_type+\"_d\"+str(Xdim)+\"_m\"+str(noise_dim)+\"_best.pth\")\n",
    "                    print(f\"Saved best model with L2: {best_acc:.4f}\")\n",
    "\n",
    "                # for multivariate model, conduct the visulaization\n",
    "                if is_plot:\n",
    "                    if (Ydim>1) and (iter_count % plot_iter == 0): \n",
    "                        generate_Y = torch.zeros([1000,2]) #generate 500 response \n",
    "                        for i in range(1000):\n",
    "                            plot_eta = sample_noise(1, dim = noise_dim, distribution=noise_distribution).to(device)\n",
    "                            plot_input =  torch.cat([torch.tensor([[1]]), plot_eta], dim=1)\n",
    "                            generate_Y[i] = G(plot_input)\n",
    "                        fig, ax = plot_kde_2d(generate_Y.detach(),title=f\"Epoch {epoch} Distribution\")\n",
    "                        plt.show()\n",
    "                        plt.close()\n",
    "                        \n",
    "            \n",
    "        \n",
    "        # Also update best model for multivariate case\n",
    "        if l2_acc < best_acc:\n",
    "            best_acc = l2_acc\n",
    "            best_model_g = copy.deepcopy(G.state_dict())\n",
    "            best_model_d = copy.deepcopy(D.state_dict())\n",
    "            print(f\"New best multivariate model with L2: {best_acc:.4f}\")\n",
    "\n",
    "                        \n",
    "                         \n",
    "        \n",
    "        # Apply learning rate decay at the end of each epoch\n",
    "        epoch_d_loss = np.mean(d_losses)\n",
    "        epoch_g_loss = np.mean(g_losses)\n",
    "        \n",
    "        print(f\"Epoch {epoch} - \"\n",
    "              f\"D Loss: {epoch_d_loss:.4f}, G Loss: {epoch_g_loss:.4f}\")\n",
    "\n",
    "        valid_loss = epoch_d_loss if epoch_d_loss is not None else float('inf')\n",
    "\n",
    "        if lr_decay == 'step' or lr_decay == 'cosine':\n",
    "            if D_scheduler is not None:\n",
    "                D_scheduler.step()\n",
    "            if G_scheduler is not None:\n",
    "                G_scheduler.step()\n",
    "        elif lr_decay == 'plateau':\n",
    "            if D_scheduler is not None:\n",
    "                D_scheduler.step(valid_loss)\n",
    "            if G_scheduler is not None:\n",
    "                G_scheduler.step(l2_acc)  # Use validation L2 for generator\n",
    "        \n",
    "        # Print current learning rates\n",
    "        if lr_decay:\n",
    "            d_lr = D_solver.param_groups[0]['lr']\n",
    "            g_lr = G_solver.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch} - D LR: {d_lr:.6f}, G LR: {g_lr:.6f}\")\n",
    "    \n",
    "    # For multivariate response model, save models at the end of the training\n",
    "    if save_last==True :\n",
    "        best_model_g = copy.deepcopy(G.state_dict())\n",
    "        best_model_d = copy.deepcopy(D.state_dict())\n",
    "        \n",
    "        torch.save(G.state_dict(), f\"{save_path}/G_\"+model_type+\"_d\"+str(Xdim)+\"_m\"+str(noise_dim)+\"_best.pth\")\n",
    "        torch.save(D.state_dict(), f\"{save_path}/D_\"+model_type+\"_d\"+str(Xdim)+\"_m\"+str(noise_dim)+\"_best.pth\")\n",
    "        print(f\"Saved best model with L2: {best_acc:.4f}\")\n",
    "\n",
    "    # Load the best model at the end of training\n",
    "    G.load_state_dict(best_model_g)\n",
    "    D.load_state_dict(best_model_d)\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "def train_WGR_image(D,G, D_solver,G_solver, Xdim, Ydim, noise_dim, loader_data , \n",
    "                    loader_val , batch_size,  eg_x, eg_label, selected_indices, lambda_w=0.9, lambda_l=0.1, \n",
    "                    noise_distribution= 'gaussian', save_path='.', num_epochs=10, start_eva=1000,  eva_iter = 50, data_type ='mnist',\n",
    "                    device='cpu', lr_decay=None, r_decay_step=5, lr_decay_gamma=0.1, is_image=False ):\n",
    "    \"\"\"\n",
    "    Train Wasserstein GAN Regression with Fully-Connected Neural Networks.\n",
    "    \n",
    "    Args:\n",
    "        D: Discriminator model\n",
    "        G: Generator model\n",
    "        D_solver: Discriminator optimizer\n",
    "        G_solver: Generator optimizer\n",
    "        noise_dim: Dimension of noise vector\n",
    "        Xdim: Dimension of covariate X\n",
    "        Ydim: Dimension of response Y\n",
    "        batch_size: Batch size\n",
    "        loader_data: Data loader for training set\n",
    "        loader_val: Data loader for validation set\n",
    "        eg_x: Sample used to show the reconstruction performance\n",
    "        eg_label: label of the eg_x\n",
    "        selected_indices: indices for eg_x to sort it from 0 to 1\n",
    "        noise_distribution: Distribution for noise sampling (default: 'gaussian')\n",
    "        lambda_w: Weight for Wasserstein loss  (default: 0.9)\n",
    "        lambda_l: Weight for L2 regularization  (default: 0.1)\n",
    "        save_path: Path to save models (default: './ ')\n",
    "        start_eva: Iteration to start evaluation (default: 1000)\n",
    "        eva_iter: to conduct the validation per iteration (default: 50)\n",
    "        num_epochs: Number of training epochs (default: 10)\n",
    "        num_samples: Number of noise samples generated for each data point in validation (default: 100)\n",
    "        device: Device to train on (default: 'cpu')\n",
    "        lr_decay: Learning rate decay strategy ('step', 'plateau', 'cosine', or None)\n",
    "        lr_decay_step: Step size for StepLR or patience for ReduceLROnPlateau\n",
    "        lr_decay_gamma: Multiplicative factor for learning rate decay\n",
    "    Returns:\n",
    "        tuple: Best validation scores and final models\n",
    "    \"\"\"\n",
    "    # Initialize learning rate schedulers if requested\n",
    "    D_scheduler, G_scheduler = None, None\n",
    "    if lr_decay == 'step':\n",
    "        D_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            D_solver, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "        G_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            G_solver, step_size=lr_decay_step, gamma=lr_decay_gamma)\n",
    "    elif lr_decay == 'plateau':\n",
    "        D_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            D_solver, mode='min', factor=lr_decay_gamma, patience=lr_decay_step )\n",
    "        G_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            G_solver, mode='min', factor=lr_decay_gamma, patience=lr_decay_step )\n",
    "    elif lr_decay == 'cosine':\n",
    "        D_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            D_solver, T_max=num_epochs, eta_min=0)\n",
    "        G_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            G_solver, T_max=num_epochs, eta_min=0)\n",
    "        \n",
    "    iter_count = 0 \n",
    "    best_acc = 5\n",
    "    \n",
    "    best_model_g = copy.deepcopy(G.state_dict())\n",
    "    best_model_d = copy.deepcopy(D.state_dict())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (x,y, label) in enumerate(loader_data):\n",
    "            if x.size(0) != batch_size:\n",
    "                continue\n",
    "    \n",
    "            eta = sample_noise(x.size(0), noise_dim, distribution=noise_distribution)\n",
    "            x_data = x.view(x.size(0),784)\n",
    "            g_input = torch.cat([x_data,eta],dim=1)\n",
    "            \n",
    "            #train D\n",
    "            D_solver.zero_grad()\n",
    "            real_images = x.clone()\n",
    "            real_images[:,:,7:19,7:19] = y\n",
    "            logits_real = D(real_images)\n",
    "            \n",
    "            fake_y = G(g_input).view(x.size(0),1,12,12).detach()\n",
    "            fake_images = x.clone()\n",
    "            fake_images[:,:,7:19,7:19] = fake_y\n",
    "            logits_fake = D(fake_images)\n",
    "            \n",
    "            penalty = calculate_gradient_penalty(D,real_images,fake_images,device, is_image=True)\n",
    "            d_error = discriminator_loss(logits_real, logits_fake) + 10 * penalty\n",
    "            d_error.backward() \n",
    "            D_solver.step()\n",
    "            \n",
    "            # train G\n",
    "            G_solver.zero_grad()\n",
    "            fake_y = G(g_input).view(x.size(0),1,12,12)\n",
    "            fake_images[:,:,7:19,7:19] = fake_y\n",
    "            \n",
    "            gen_logits_fake = D(fake_images)\n",
    "            g_error = lambda_w * generator_loss(gen_logits_fake) + lambda_l * l2_loss(fake_y,y)\n",
    "            g_error.backward()\n",
    "            G_solver.step()\n",
    "            \n",
    "            if (iter_count % eva_iter == 0):\n",
    "                print('Iter: {}, D: {:.4}, G:{:.4}'.format(iter_count,d_error.item(),g_error.item()))\n",
    "\n",
    "                if (iter_count >= start_eva):\n",
    "                    l1_G_Acc, l2_G_Acc= val_G_image(G, loader_data=loader_val, noise_dim=noise_dim, \n",
    "                                                Xdim=Xdim, Ydim=Ydim, multivariate=True)\n",
    "                    if l2_G_Acc < best_acc:\n",
    "                        print('################## save G model #################')\n",
    "                        best_acc = l2_G_Acc.copy()\n",
    "                        best_model_g = copy.deepcopy(G.state_dict())\n",
    "                        best_model_d = copy.deepcopy(D.state_dict())\n",
    "\n",
    "                        # Save models\n",
    "                        torch.save(G.state_dict(), f\"{save_path}/G_\"+data_type+\"_d\"+str(Xdim)+\"_m\"+str(noise_dim)+\"_best.pth\")\n",
    "                        torch.save(D.state_dict(), f\"{save_path}/D_\"+data_type+\"_d\"+str(Xdim)+\"_m\"+str(noise_dim)+\"_best.pth\")\n",
    "                        print(f\"Saved best model with L2: {best_acc:.4f}\")\n",
    "\n",
    "                        # plot the reconstruction image on the examples\n",
    "                        eg_eta =  sample_noise(batch_size, dim=noise_dim, distribution=noise_distribution ).to(device)\n",
    "                        g_exam_input = torch.cat([eg_x.view(batch_size, Xdim), eg_eta], dim=1)\n",
    "                        recon_y = G(g_exam_input).view(batch_size,1,12,12)\n",
    "                        recover_y = convert_generated_to_mnist_range(recon_y)\n",
    "                        \n",
    "                        recon_x = eg_x.clone()\n",
    "                        recon_x[selected_indices,:,7:19,7:19] = recover_y[selected_indices,:,:,:].detach()\n",
    "                        visualize_digits( images=recon_x[selected_indices] , labels = eg_label[selected_indices], figsize=(3, 13), title='(X,hat(Y)')\n",
    "            iter_count += 1\n",
    "          \n",
    "        if lr_decay == 'step' or lr_decay == 'cosine':\n",
    "            if D_scheduler is not None:\n",
    "                D_scheduler.step()\n",
    "            if G_scheduler is not None:\n",
    "                G_scheduler.step()\n",
    "        elif lr_decay == 'plateau':\n",
    "            if D_scheduler is not None:\n",
    "                D_scheduler.step(valid_loss)\n",
    "            if G_scheduler is not None:\n",
    "                G_scheduler.step(l2_acc)  # Use validation L2 for generator\n",
    "        \n",
    "        # Print current learning rates\n",
    "        if lr_decay:\n",
    "            d_lr = D_solver.param_groups[0]['lr']\n",
    "            g_lr = G_solver.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch} - D LR: {d_lr:.6f}, G LR: {g_lr:.6f}\")      \n",
    "\n",
    "\n",
    "    # Load the best model at the end of training\n",
    "    G.load_state_dict(best_model_g)\n",
    "    D.load_state_dict(best_model_d)\n",
    "    \n",
    "    return G, D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f01ee502-66d2-40fe-88d9-a532d73b40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4788) tensor(0.5235) tensor(0.8666) tensor(2.5542) tensor(21.6355) tensor(22.8846)\n"
     ]
    }
   ],
   "source": [
    "_,_,_,_, LB,UB = eva_G_UniY_old(G=trained_G, loader_data=loader_test,  noise_dim=args.noise_dim, batch_size=args.test_batch, J_t_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "590c4cbe-d2da-4bf7-bfee-81c11d866ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2389)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(LB.view([args.test])-y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ec67eb56-38ba-4abd-b9f2-53d3c1d5f8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9919)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(UB.view([args.test])-y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e1eff2e-55f5-4564-8a3a-9a342589153f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7851)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.mean(LB[0,:]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a302b88c-171f-4a72-9220-0dcc1f27a63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88.1392)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72fa6e73-4f40-4f02-92eb-47f8d29290a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe0240a6-e585-4c54-8d1b-46ed6e16012d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f0e84a62-ddd5-46f7-89c9-a83d399222d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(97.3201)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c2721486-2895-467f-b51c-eab50b38b0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3218)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e9591b1a-18cb-4904-a4d4-887a8612b520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([85.2120, 53.7034, 60.9445,  ..., 54.8546, 26.8212, 28.1751])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LB.view(args.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bbd7b929-c494-4585-92f7-3f82a030df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([88.1392, 56.1797, 64.5704,  ..., 56.4071, 27.0352, 28.5769])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3bd59730-484d-4307-bc77-325466ca5361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2243)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(LB.view(args.test)-y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fb2ec8ce-0d60-40b3-ba35-12573f21b20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.8769)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(LB.view(args.test)-y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e67c76d6-0c5b-4674-b9a7-d231a47a9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.linspace(0, 1, steps=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a40c82ce-8b20-49af-bdb8-07cb0e78a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB_diff = LB.view(args.test)-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "337c0689-2d22-45ed-8ad5-e5e68f975659",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_LB = (LB_diff - LB_diff.mean())/LB_diff.std(unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40f416d2-4254-47f5-8a82-ba080c08a189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(z_scores_LB) < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79a16e1a-c5eb-4b51-97ad-e60d053c6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UB_diff = y_test- UB.view(args.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7694d3e-0635-4ba0-a322-da86d0d7b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_UB = (UB_diff - UB_diff.mean())/UB_diff.std(unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efef93e3-7e3d-456e-bc2a-66bfe5781f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(z_scores_UB) < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43c40604-a48c-46e2-b0d8-97f8a1117277",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_LB_UB = (torch.abs(z_scores_UB) < 3) | (torch.abs(z_scores_LB) < 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86e3ac53-d6b3-482a-b1bb-c55c21cf3363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9972)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_LB_UB.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "764f7507-6345-4693-9837-af0811f3c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = torch.quantile(LB_diff, 0.25)\n",
    "q3 = torch.quantile(LB_diff, 0.75)\n",
    "iqr = q3 - q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6526b820-ee1a-44a9-b709-f0543f69f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7699b13e-516e-4012-9dad-39f4b81fb20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3887)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "780931ab-7033-444c-a607-9e28011716e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9525)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(torch.abs(LB_diff[(LB_diff >= lower ) & (LB_diff <= upper )]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7ba07d3d-de86-4bac-9dd6-5bb5f62e2d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.8769,  -3.4757,  -3.0234,  -2.7150,  -2.4685,  -2.2691,  -2.0676,\n",
       "         -1.8914,  -1.7387,  -1.5790,  -1.4336,  -1.2918,  -1.1646,  -1.0350,\n",
       "         -0.9168,  -0.8060,  -0.6978,  -0.5757,  -0.4384,  -0.2459,   4.2243])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantile(LB.view(args.test)-y_test,probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca842da-1a08-4e4b-9467-9cefe899f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(5678) \n",
    "# =============================================================================\n",
    "NUM = 53490 # total sample size\n",
    "train_num = 40000 # training sample size\n",
    "val_num = 3490 # validation sample size\n",
    "test_num = 10000 # testing sample size\n",
    "\n",
    "all_idx = np.array(range(NUM))\n",
    "\n",
    "random.shuffle(all_idx)\n",
    "train_idx = all_idx[:train_num] # training samples idx\n",
    "val_test_idx = all_idx[train_num:] # validation and testing idx\n",
    "\n",
    "random.shuffle(val_test_idx)\n",
    "val_idx = val_test_idx[:val_num] # validation idx\n",
    "test_idx = val_test_idx[val_num:] #testing idx\n",
    "\n",
    "train_X =  all_X[train_idx].float() \n",
    "train_Y = all_Y[train_idx].float()\n",
    "val_X = all_X[val_idx].float()\n",
    "val_Y = all_Y[val_idx].float()\n",
    "test_X = all_X[test_idx].float() \n",
    "test_Y = all_Y[test_idx].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6964a-dadd-40fd-802f-fdf976b192e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001dd5e-9068-4e98-a55e-dcbfb7062697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
